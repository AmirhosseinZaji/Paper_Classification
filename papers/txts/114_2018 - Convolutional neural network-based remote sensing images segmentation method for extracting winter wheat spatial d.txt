applied sciencesArticleConvolutional Neural Network-Based RemoteSensing Images Segmentation Method for ExtractingWinter Wheat Spatial DistributionChengming Zhang1,2,*, Shuai Gao3,*, Xiaoxia Yang1,2, Feng Li4, Maorui Yue5, Yingjuan Han6,Hui Zhao6, Ya’nan Zhang1and Keqi Fan11College of Information Science and Engineering, Shandong Agricultural University, 61 Daizong Road,Taian 271000, China; yangxx@sdau.edu.cn (X.Y.); zyn980113@hotmail.com (Y.Z.);fkq980810@hotmail.com (K.F.)2Shandong Technology and Engineering Center for Digital Agriculture, 61 Daizong Road,Taian 271000, China3Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Dengzhuangnan Road,Beijing 100094, China4Shandong Climate Center, Mountain Road, Jinan 250001, China; lifengsd@outlook.com5Taian Agriculture Bureau, Naihe Road, Taian 271000, China; yuemaoruita@outlook.com6Key Laboratory for Meteorological Disaster Monitoring and Early Warning and Risk Management ofCharacteristic Agriculture in Arid Regions, CMA, 71 Xinchangxi Road, Yinchuan 750002, China;yjhan_nx@outlook.com (Y.H.); zhaohui_cau@outlook.com (H.Z.)*Correspondence: chming@sdau.edu.cn (C.Z.); gaoshuai@radi.ac.cn (S.G.); Tel.: +86-139-5382-3659 (C.Z.);+86-010-64806258 (S.G.)Received: 26 September 2018; Accepted: 16 October 2018; Published: 19 October 2018/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046Featured Application: In Gaofen-2 images, it is difﬁcult to accurately extract winter wheatspatial distribution using traditional methods. Because our approach can better solve thisproblem, it has played an important role in agricultural surveys and improved the efﬁciency ofagricultural surveys. Our approach has been utilized by the Department of Agriculture and theMeteorological Bureau of Shandong Province, China.Abstract:When extracting winter wheat spatial distribution by using convolutional neural network(CNN) from Gaofen-2 (GF-2) remote sensing images, accurate identiﬁcation of edge pixel is the keyto improving the result accuracy. In this paper, an approach for extracting accurate winter wheatspatial distribution based on CNN is proposed. hybrid structure convolutional neural network(HSCNN) was ﬁrst constructed, which consists of two independent sub-networks of different depths.The deeper sub-network was used to extract the pixels present in the interior of the winter wheatﬁeld, whereas the shallower sub-network extracts the pixels at the edge of the ﬁeld. The model wastrained by classiﬁcation-based learning and used in image segmentation for obtaining the distributionof winter wheat. Experiments were performed on 39 GF-2 images of Shandong province capturedduring 2017–2018, with SegNet and DeepLab as comparison models. As shown by the results,the average accuracy of SegNet, DeepLab, and HSCNN was 0.765, 0.853, and 0.912, respectively.HSCNN was equally as accurate as DeepLab and superior to SegNet for identifying interior pixels,and its identiﬁcation of the edge pixels was signiﬁcantly better than the two comparison models,which showed the superiority of HSCNN in the identiﬁcation of winter wheat spatial distribution.Keywords:remote sensing image segmentation; convolutional neural networks; Gaofen-2;hybrid structure convolutional neural networks; winter wheat spatial distribution;classiﬁcation-based learningAppl. Sci.2018,8, 1981; doi:10.3390/app8101981www.mdpi.com/journal/applsciAppl. Sci. 2018,8, 1981 of 201. IntroductionWinter wheat is the most important food crop in China, comprising 21.38% of the gross croppedarea of the domestic food crops in 2017 according to the data released by the National Bureau ofStatistics, with its output accounting for 21.00% of the total food crop production 1]. For nationalfood security, the Chinese government has assigned minimum area of arable land in each region thatneeds to be safeguarded (the “red line”) 2]. Timely and accurate acquisition of the size and spatialdistribution of winter wheat ﬁelds assists the relevant government departments in guiding the farmingactivities, estimating the yield, and adjusting the agricultural structure for ensuring food security 3].Remote sensing is capable of imaging and large-area monitoring, making it good data sourcefor rapid and accurate extraction of winter wheat planting information. Researchers have successfullyextracted winter wheat spatial distribution information from MODIS (moderate-resolution imagingspectroradiometer) and ETM/TM (enhanced thematic mapper plus/thematic mapper), achievingaccuracies of 85.5% and 89.1%, respectively 4,5]. This exhibits the advantage of remote sensing inthis application. However, owing to limitations in the spatial resolution of the data source, the spatialresolution of the extraction results is also rather coarse and unable to satisfy the requirement of theapplication 6–10]. With the development of high-resolution remote sensing satellites, crop plantingarea can be monitored more accurately using the corresponding images as the data source 11,12].The winter wheat cultivation information is extracted from the remote-sensing images capturedby Gaofen-1 of the Gaofen series of Chinese satellites, yielding satisfactory results, with maximumaccuracy reaching about 89% 13–18]. Most researchers still use traditional methods, such as decisiontrees and textures features. These methods can only take advantage of low-level features, which makeit easy to make mistakes in identifying pixels at the edge of winter wheat planting area.Image segmentation has been successfully used in the processing of camera images and appliedby researchers to high-resolution remote sensing images, achieving signiﬁcantly more accurateclassiﬁcation by pixel-by-pixel segmentation 19–21]. Feature extraction is the key step in remotesensing image segmentation. In high-resolution remote sensing images, as the spectral differencebetween the same type of objects is increased, and between different types of objects is diminished,the former has more probability of exhibiting different spectral properties, whereas the latter tends tobe spectrally similar, which makes feature extraction increasingly difﬁcult 22,23]. Traditional methodsincluding k-nearest neighbors and maximum entropy can only identify low-level image features suchas color, shape, and texture. They are not capable of visually providing semantic description.This hinders the extraction of higher-level features and limits the use of these methods in thesegmentation of high-resolution remote sensing images 24,25].With the development of machine learning, algorithms such as neural networks (NNs) 26]and support vector machine (SVM) 27,28] are being used in the segmentation of high-resolutionimages [29–31].In some studies, when compared with traditional statistical methods andobject-oriented methods, machine learning algorithms yielded better image segmentation results 32,33]. Both SVM and NNs are shallow-learning algorithms 34–36], which do not express complexfunctions well owing to the limitations in their network structure. Therefore, these models cannot adaptto the continuously increasing complexity caused by the increasing sample size and diversity 37,38].Progress in deep learning has facilitated solving these problems by using deep neural networks(DNNs) 39–42]. As an important branch of deep learning, convolutional NN (CNN) is widelyused with visual data because of its excellent feature learning ability 43–45]. CNN is deeplearning network, composed of several layers, capable of nonlinear mapping. Its strength in learningis exempliﬁed by the good image segmentation results achieved 46–52]. Further, the capacity of manylarge CNNs can be scaled according to the size of the training data and complexity and processingability of the model, and their performance in image segmentation has improved signiﬁcantly 53–60].A fully convolutional network (FCN) is deep learning network for image segmentation,which was proposed in 2015. Taking advantage of convolution computation in its featureorganization and extraction abilities, an FCN realizes pixel-by-pixel segmentation of camera imagesAppl. Sci. 2018,8, 1981 of 20by constructing multi-layer convolutional structure and setting appropriate deconvolutionallayers 61–63]. Accordingly, series of convolution-based segmentation models has been developedincluding SegNet 64], UNet 65], DeepLab 66], multi-scale FCN 67], and ReSeg 68]. Of these models,SegNet and UNet are clearly structured, and it is easy to understand the convolution structure of themodel. The processing speed is fast. DeepLab uses method called “Atrous Convolution”, which hasa strong advantage in processing detailed images. multi-scale FCN is designed to address the hugescale gap between different classes of targets, i.e., sea/land and ships. ReSeg exploits the local genericfeatures extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks(RNN) to retrieve distant dependencies. Each model has its own strengths and is adept at dealing withcertain image types.In the work of extracting the spatial distribution of crops with high GF-1 as the data source,in addition to methods such as decision trees, textures features, and maximum entropy, research hasalso been carried out using deep learning. However, most of these studies directly use the existingdeep learning model as tool, and seldom consider the inﬂuence of characteristics difference of edgepixels and inner pixels in the crop planting area are large.On board the Gaofen-2 satellite is panchromatic camera with spatial resolution of m,and multi-spectral camera with spatial resolution of m, which provides ideal data for extractingwinter wheat plantation information. Before the application of CNN to GF-2 remote-sensing imagesfor this purpose, trial extraction is performed with classical network architectures (such as SegNet)where misidentiﬁed pixels are categorized, of which approximately 90% are found at the edge of thecrop ﬁeld. Further analysis indicates the structure of the convolutional layer as the source of thisproblem. The outcome produced by operating the convolution kernel in the pixel block is treated asthe eigenvalue of the central pixel of the pixel block. As such, for the pixels at the edge, 50% of thepixels involved in each convolution are from negative samples, whereas, for the pixels at the corner,this number is 75% or higher. This results in signiﬁcant difference between the eigenvalues of thepixels at these locations and those at the center of the image, and an increase in the probability of therecognition results being placed in wrong category. To avoid these problems, new method is hereinproposed for the extraction of the winter wheat ﬁeld information from the GF-2 remote sensing images.The main procedures are as follows.1. First, CNN consisting of two independent sub-networks of different depths is established.The deep and shallow sub-networks are trained to be sensitive only to the pixels at the interiorand edge of winter wheat planting ﬁeld, respectively, and only these pixels are extracted.This model is named as Hybrid Structure Convolutional Neural Network (HSCNN).2. classiﬁcation algorithm is adopted in the model training. For initial training of the sub-networkused for the edge pixel extraction, edge pixels are considered as positive samples, with thepixels at other locations being treated as negative samples. The inner pixels are then designatedas positive samples, with the pixels at other locations as negative samples, for training thesub-network used for the inner pixel extraction. After the successful completion of the training,the neural network is able to extract the winter wheat ﬁeld from the GF-2 images accurately.3. Finally, GF-2 image is segmented by the trained model. Because SegNet and DeepLab are classicsemantic segmentation models of images, and, the working principles of these two models arevery similar to our work, we choose these two models as the comparison model, and comparisonis performed with them to evaluate the accuracy of the segmentation results.Appl. Sci. 2018,8, 1981 of 202. Data Sources and Methods2.1. Data Sources2.1.1. Study RegionThe whole study region is Shandong province, China. Shandong is located along the eastern coastof China (in the lower stream of the Yellow river), within 34220N–38240N and 11447.50E–122420E.It measures 721.03 km from east to west, and 437.28 km from north to south. The land area of theprovince is 155,800 km2, of which 14.59% is mountainous, 5.56% is water (such as lakes), 15.98%is forest, and 53.82% is cultivated land. The annual total planting area of crops in the province isapproximately 162 million mu. The main food crops of this region are wheat and maize. In 2016,the wheat planting area was 57.45405 million mu, and in 2017 it was 57.6435 million mu 69].In this paper, we used the ground data and remote sensing data of Feicheng county, Ningyangcounty and Zhangqiu county, Shandong province. The three counties are similar in topography, allrelatively ﬂat, which can eliminate the inﬂuence of topographic ﬂuctuations on the experimental results.2.1.2. Ground-Based DataFor manufacturing sample to train our model, we conducted ﬁeld survey in Feicheng county,Ningyang county and Zhangqiu county in 2017 and 2018, and obtained the land use data of 369 samplepoints, among which 257 were winter wheat sample points and 112 were bare land. The survey resultsinclude the time, location and type of land use.2.1.3. Remote Sensing DataWe selected 39 GF-2 remote sensing image, size of each image is 7300 ⇥6900. Of these images,15 were captured on 17 February 2017, 11 were captured on 21 March 2018 and 13 were captured on12 April 2018. We select images from different periods to increase the anti-interference abilities of theHSCNN. These remote sensing data cover Feicheng county, Ningyang county and Zhangqiu county,and are matched with ground investigation time. At the same time, the selected remote sensing datahave fewer clouds and better clarity.The Environment for Visualizing Images (ENVI) software was used for preprocessing the tasks,including fusion of panchromatic spectrum and multispectral band to obtain 1-m spatial resolutionmultispectral data, and the contrast stretch to generate color-enhanced color composite image.2.2. Network Architecture of Our MethodThe HSCNN model is divided into ﬁve functional groups of components, input (a), inner-CNN(b), edge-CNN (f), vote function (j), and output (k), as shown in Figure 1. Both the edge-CNN andinner-CNN have convolution layers, an encoder layer, and classiﬁer layer. In the training stage,the inputs are original images and artiﬁcial classiﬁcation labels. In the classiﬁcation stage, the inputsare original GF-2 images, output is single-band ﬁle, and content of each pixel in the output is thecategory number of the corresponding original image pixel. The HSCNN indicates the winter wheatarea using category number 100, and category number 200 distinguishes other land use. The reasonfor adopting the two numbers is to ﬁt with the coding value table we are working on to obtain detailedland use information.2.2.1. Inner-Layers and Edge-LayersThe operational characteristics of the pixel block-based convolution for image segmentation aredescribed in Section 1, in addition to the effect of the pixel block location on the convolution results.Based on this analysis, two convolution sub-structures of different depths are setup for the featureextraction of the winter wheat ﬁeld. The deep convolution sub-network is used to extract the featuresof the pixels in the interior of the winter wheat plantation, shown as inner-layers (c) in Figure 1.Appl. Sci. 2018,8, 1981 of 20The shallower sub-network is used to extract the features of the pixels at the edge of the winter wheatplantation, shown as edge-layers (g) in Figure 1. The beneﬁts of this design are discussed in Section 4based on the experimental results.In our approach, an inner pixel refers to the pixel that only contains winter wheat pixels in thepixel block when convolution operation is carried out with the pixel as the center pixel. An edge pixelrefers to the pixel that contains winter wheat pixels and other pixels when computing the feature ofthe pixel.Appl. Sci. 2018, 8, FOR PEER REVIEW of 20 Figure 1. Network architecture of the Hybrid Structure Convolutional Neural Network (HSCNN): (a) input; (b) inner-CNN; (c) inner-layers; (d) inner-encoder; (e) inner-classifier; (f) edge-CNN; (g) edge-layers; (h) edge-encoder; (i) edge-classifier; (j) vote function; (k) output. All kernels of the HSCNN take the form, × × c, where is the width, is the height, and is the number of channels of kernel. Two types of kernels are used in the first convolutional layers of inner-layers (c) and edge-layers (g). For one type and are set to 1, and for the other type the values are set to 3. In both cases, is set to because the data in the four multi-spectral bands of GF-2 are used. Kernels of the form × × are used to extract the features of the pixels. The generated feature map is used instantaneously as the input of the encoder, and does not participate in the subsequent convolution. Convolution kernels of the form × × are used to extract the spatial relation between the pixels and generate the spatial semantics by multi-level convolution. After the operation of first convolution layer on the original image, we obtain feature map which has only one channel. Because the input of convolution layer is the feature map calculated by the previous convolution layer, so the and values of the kernels used in all other convolutional layers are set to 3, and is from the second layer. To extract more features from the edge pixels of the crop field, the number of kernels used in each convolutional layer of edge-layers (g) is twice that used in the corresponding layer of inner-layers (c). In the HSCNN, each convolution layer has only one activation layer attached, and there is no pool layer. Accordingly, the convolution result of each pixel block can be used directly as the feature of its central pixel, without the need to determine the position of the pixel that the feature corresponds to through deconvolution. As such, the HSCNN does not utilize deconvolutional layer. This reduces the extent of computation and positioning error of the deconvolution, thereby improving the accuracy of the segmentation. 2.2.2. Inner-Encoder and Edge-Encoder The inner-encoder and edge-encoder are used to encode the eigenvector extracted by the convolution layers on the pixel, ensuring that the classifier can establish the relationship between the eigenvector and pixel type. In the HSCNN model, the inner- and edge-encoders are both × matrices, where is the length of the eigenvector. Let denote the eigenvector of the pixel, denote the encoder matrix, and the encoded vector result. The encoding calculation is displayed in Equation (1). Figure 1. Network architecture of the Hybrid Structure Convolutional Neural Network (HSCNN):(a) input; b) inner-CNN; c) inner-layers; d) inner-encoder; e) inner-classiﬁer; f) edge-CNN;(g) edge-layers; h) edge-encoder; i) edge-classiﬁer; j) vote function; k) output.All kernels of the HSCNN take the form, ⇥h⇥c, where is the width, is the height, and isthe number of channels of kernel. Two types of kernels are used in the ﬁrst convolutional layers ofinner-layers (c) and edge-layers (g). For one type and are set to 1, and for the other type the valuesare set to 3. In both cases, is set to because the data in the four multi-spectral bands of GF-2 areused. Kernels of the form ⇥1⇥4 are used to extract the features of the pixels. The generated featuremap is used instantaneously as the input of the encoder, and does not participate in the subsequentconvolution. Convolution kernels of the form ⇥3⇥4 are used to extract the spatial relation betweenthe pixels and generate the spatial semantics by multi-level convolution.After the operation of ﬁrst convolution layer on the original image, we obtain feature mapwhich has only one channel. Because the input of convolution layer is the feature map calculated bythe previous convolution layer, so the and values of the kernels used in all other convolutionallayers are set to 3, and is from the second layer. To extract more features from the edge pixels of thecrop ﬁeld, the number of kernels used in each convolutional layer of edge-layers (g) is twice that usedin the corresponding layer of inner-layers (c).In the HSCNN, each convolution layer has only one activation layer attached, and there is no pool layer.Accordingly, the convolution result of each pixel block can be used directly as the feature of its central pixel,without the need to determine the position of the pixel that the feature corresponds to through deconvolution.As such, the HSCNN does not utilize deconvolutional layer. This reduces the extent of computation andpositioning error of the deconvolution, thereby improving the accuracy of the segmentation.Appl. Sci. 2018,8, 1981 of 202.2.2. Inner-Encoder and Edge-EncoderThe inner-encoder and edge-encoder are used to encode the eigenvector extracted by theconvolution layers on the pixel, ensuring that the classiﬁer can establish the relationship betweenthe eigenvector and pixel type. In the HSCNN model, the inner- and edge-encoders are both ⇥nmatrices, where is the length of the eigenvector.Let denote the eigenvector of the pixel, denote the encoder matrix, and the encoded vectorresult. The encoding calculation is displayed in Equation (1)."r1r2#="w11w12 ··· w1nw21w22 ··· w2n#⇥hx1x2 ··· xniT+"b1b2#(1)where each row of matrix represents ﬁtting function for particular type of pixel, 1and 2are therespective biases, and the corresponding component of is the encoded value of eigenvector on thatpixel type. The inner- and edge-encoders are trained separately.2.2.3. Inner-Classiﬁer and Edge-ClassiﬁerFor each pixel, the inner-classifier converts its vector of the encoded values given by the inner-encoderinto probability distribution over set of classes, and classifies the pixel as an inner pixel or non-innerpixel of the winter wheat plantation based on the location of the component with the highest probability.Similarly, the edge-classifier distinguishes between the edge and non-edge pixels of the winter wheat fieldusing the vectors of the encoded values generated on the pixels by the edge-encoder.In reference to the classic softmax classiﬁer 60–65], Equation (2) is used here to convert vector rof the encoded values to vector of the class probabilities for each pixel."p1p2#=264er1er1+er2er2er1+er2375 (2)After the transformation, the index of max(p 1and 2) is taken as the predicted category of thepixel. For the inner-classiﬁer, index numbers of and are assigned to the inner pixels of the winterwheat ﬁeld and other pixels, respectively. Accordingly, index numbers of and are assigned to theedge pixels of the winter wheat ﬁeld and other pixels, respectively.2.2.4. Vote-FunctionThe vote-function determines the category number of pixel given by the inner-classiﬁer andedge-classiﬁer and writes it to the output ﬁle. As described in the beginning of Section 2.2, the HSCNNindicates the winter wheat area using category number 100 and other land uses using category number200. The category number of pixel is calculated in Equation (3).o=(100 pinner=1 or pedge=1200 pinner=0 and pedge=0(3)where represents the ﬁnal category number of pixel and innerand edgeare the outputs of theinner-classiﬁer and edge-classiﬁer, respectively.2.3. HSCNN TrainingWe manually labeled all images at the pixel level as ground truth (GT) label data. In other words,for each image, there exists 7300 ⇥6900 label map, having pixel-class (row-col indexed) correspondencewith it. We used 36 images for training, and the remaining images for testing. The GF-2 images and theircorresponding artificial classification labels will be input to the HSCNN as training samples.Appl. Sci. 2018,8, 1981 of 20The training process includes error calculation, error back propagation and weight update.This process is iterated until the difference becomes smaller than the predetermined threshold.We calculated the errors between the predicted classiﬁcation label and manual classiﬁcation labelby the chain rule. The chain rule, the derivative rule in calculus, is used to ﬁnd the derivative ofa complex function, which is common way to do the derivative calculation of calculus. The derivativeof composite function is the product of the derivatives of this ﬁnite number of functions atthe corresponding point, as chain. Then, the errors are back-propagated through the network.The backward propagation algorithm is kind of training and learning method in deep learning,which can spread the error of the output layer backward to realize weight adjustment, adjust theweight between each node in the deep network, and achieve the goal that the sample tag output fromthe network is consistent with the actual tag. We use gradient descent method to update HSCNNparameters. Gradient descent method is the most commonly used optimization method. The ideais to use the negative gradient direction of the current position as the search direction, because thatdirection is the fastest descending direction of the current position.2.3.1. Sample LabelingWe use the ENVI software for labeling and designing preprocessor to build the labels.The process of artiﬁcial labeling is as follows:1. The region-of-interest (RoI) tool in the ENVI software is used to select the winter wheat regionsand other regions in the image. Then, the map locations of the pixels in each region are output todifferent ﬁles based on the category.2. band is added to the image ﬁle by the preprocessor as mask band. The spatial resolution, size,and other parameters of the mask band are the same as the original image. Then, the categorynumber of each pixel is written to the mask band according to the map location of the pixelpreviously output. We manually label all the images at the pixel level. Thus, for each image,there exists 7300 ⇥6900 label map, with row-column-indexed pixel-class correspondence.3. The pixels marked as winter wheat are further categorized as edge pixels and inner pixels. Based onthe parameters given above, the inner-layers comprise of eight convolutional layers, each witha3⇥3(length ⇥width) convolution kernel. Therefore, the feature extraction from pixel involvesa9⇥9p x l block centered at in the calculation. As defined in Section 2.2.1,t ew n e rw e tpixels are divided sequentially into edge pixels and inner pixels. For training class by class, we usetemporary code 160 to denote edge pixels and 170 to denote inner pixels in the mask band.Figure 2shows an example of an image-label pair. Figure 2. Image-label pair example: a) original image; and b) labels.Appl. Sci. 2018,8, 1981 of 202.3.2. Loss FunctionIn our method, new loss functions are deﬁned for the inner-CNN and edge-CNN, which still usethe cross entropy as the basic element for the calculation, as expressed in Equation (4).H(p, q)=Â2i=1qilog(pi) (4)where and are, respectively the predicted and actual probability distribution, and is index ofa component in the probability distribution. On this basis, the loss function of the inner-CNN isdeﬁned asloss=1mÂmÂ2i=1qilog(pi) (5)When computing loss of inner-CNN, is obtained by subtracting the number of edge pixels ofthe winter wheat ﬁeld from the total number of samples, and when computing loss of edge-CNN, isobtained by subtracting the number of inner pixels of the winter wheat ﬁeld from the total numberof samples.2.3.3. Model TrainingImages from two different periods were selected as the training data. We selected imagesfrom different periods for increasing the anti-interference abilities of the HSCNN and mitigatingthe complications such as the change in the seasons, and thus enhancing applicability. The trainingstage proceeded through the following steps:1. Image-label pairs are input into the HSCNN as training samples. Network parametersare initialized.2. Forward propagation is performed on the sample images.3. The [loss]_inner is calculated and back propagated to the inner-CNN, whereas the [loss]_edge iscalculated and back propagated to the edge-CNN.4. The network parameters are updated using the stochastic gradient descent (SGD) 41,48]with momentum.5. Steps (2)–(4) are iterated until both [loss]_inner and [loss]_edge are less than the predeterminedthreshold values.The training yields two sub-networks, an inner-CNN and edge-CNN. The former can accuratelyextract the inner pixels of the winter wheat plantation from the sweet GF-2 remote sensing images,whereas the latter allows the best possible distinction between the edge pixels of the winter wheatplanting region and other pixels.In our training, the SGD method with momentum was used for parameter updates, which isillustrated in the following expression:W(n+1)=W(n)DW(n+1)(6)where W(n)denote the old parameters, W(n+1)denote the new parameters, and DW(n+1)is the incrementin the current iteration, which is combination of the old parameters, gradient, and historicalincrement, i.e.,DW(n+1)=J✓dw·W(n)+∂J(W)∂W(n)◆+m·DW(n)(7)where J(W) is the loss function, #is the learning rate for step length control, wdenotes the weightdecay, and denotes the momentum.Appl. Sci. 2018,8, 1981 of 202.4. Segmentation Using the Trained NetworkAfter successful training, the HSCNN can be used to segment an input imagery pixel-by-pixel.According to our design, the output is written in new band. The beneﬁt of this design avoidsdamaging the original ﬁle.3. Experiments and ResultsThe data used in the experiment are presented in Section 2.1. In this section, the models used forcomparison are described in Section 3.1, and the experimental results and assessment of accuracy aregiven in Section 3.2.3.1. Comparison ModelFeature selection is the basis of remote sensing image segmentation. At present, there aremainly two methods based on artiﬁcial feature selection and machine learning. Haralick et al. (1973)put forward the method of gray level co-occurrence matrix, which is classical artiﬁcial selectionfeature method. This method is mainly used to select image texture features. Since the texture isformed by repeated alternating changes of gray distribution in image space, so there is certaingray-scale relationship between two separate pixels away certain distance, Haralick et al. describedthis correlation through matrix 70]. Based on the artiﬁcial selection feature, only limited, shallowfeatures can generally be selected. The feature selection based on machine learning can fully explorethe deep feature and spatial semantic feature of the image. SegNet and DeepLab are classic semanticsegmentation models of images, which have achieved very good results in the processing of images.Moreover, the working principles of these two models are very similar to our work, so we choosethese two models as the comparison model, which can better reﬂect the advantages of our model infeature extraction. comparative experiment was conducted using the methods established in thepublished literature.3.1.1. SegNetFor the SegNet model, we directly employed the structure proposed by Badrinarayanan et al. 64],which consists of an encoder, decoder, and classiﬁer. The encoder uses the ﬁrst 13 convolutionallayers of the VGG16 network, each having its corresponding decoder layer, totaling 13 decoder layers.The last decoder generates multi-channel feature map as the input to the classiﬁer, which outputsa probability vector of length K, where is the number of classes. The ﬁnal predicted categorycorresponds to the class having maximum probability at each pixel. In terms of training, SegNet canbe trained end-to-end using SGD.3.1.2. DeepLabFor DeepLab, we directly employed the DeepLab v3 model proposed by Liang-Chieh Chen et al. 66].DeepLab was also developed based on the VGG network. To ensure that the output size would not be nottoo small without excessive padding, DeepLab changes the stride of the pool4 and pool5 layers of the VGGnetwork from the original to 1, plus padding. To compensate for the effect of the stride change on thereceptive field, DeepLab uses convolution method called “atrous convolution” to ensure that the receptivefield after pooling remains unchanged and the output is more refined. Finally, DeepLab incorporates fullyconnected conditional random field (CRF) model to refine the segmentation boundary.3.2. Results and Result ComparisonIn the comparative experiment, we applied our trained model to three GF-2 images forsegmentation. These images were only used for testing and not involved in training. Figure 3illustratesthe results obtained from the comparison methods and proposed method. In Figure 3, the ﬁrst columnAppl. Sci. 2018,8, 1981 10 of 20illustrates the results of Experiment 1, the second column illustrates the results of Experiment andthe third column illustrates the results of Experiment 3.Tables 1–3are confusion matrices for the segmentation results of SegNet model, DeepLab model,and HSCNN model, respectively. Each row of the confusion matrix represents the proportion takenby the actual category, and each column represents the proportion taken by the predicted category.As can be seen from the tables, our method achieves better classiﬁcation results. In the example above,the proportion of “winter wheat” wrongly categorized as “background” is on average 0.069, and theproportion of “background” wrongly classiﬁed as “winter wheat” is on average 0.019, resulting in anoverall accuracy of 91.2%.Appl. Sci. 2018, 8, FOR PEER REVIEW 10 of 20 Figure 3. Cont.Appl. Sci. 2018,8, 1981 11 of 20Appl. Sci. 2018, 8, FOR PEER REVIEW 11 of 20 Figure 3. Segmentation results for Gaofen-2 (GF-2) images: (a) original images; (b) ground truth, (c) results of SegNet corresponding to the images in (a); (d) errors of SegNet; (e) results of DeepLab; (f) errors of DeepLab; (g) results of HSCNN; and (h) errors of HSCNN. Figure 3. Segmentation results for Gaofen-2 (GF-2) images: a) original images; b) ground truth,(c) results of SegNet corresponding to the images in a); (d) errors of SegNet; e) results of DeepLab;(f) errors of DeepLab; g) results of HSCNN; and h) errors of HSCNN.Appl. Sci. 2018,8, 1981 12 of 20Table 1. Confusion matrix of the SegNet approach for Figure 4.Experiment GT/Predicted Winter Wheat OthersExperiment-1winter wheat 0.621 0.129Others 0.087 0.163Experiment-2winter wheat 0.387 0.153Others 0.084 0.376Experiment-3winter wheat 0.217 0.123Others 0.129 0.531Table 2. Confusion matrix of the DeepLab approach for Figure 4.Experiment GT/Predicted Winter Wheat OthersExperiment-1winter wheat 0.653 0.107Others 0.055 0.185Experiment-2winter wheat 0.432 0.086Others 0.039 0.443Experiment-3winter wheat 0.301 0.108Others 0.045 0.546Table 3. Confusion matrix of our HSCNN approach for Figure 4.Experiment GT/Predicted Winter Wheat OthersExperiment-1winter wheat 0.681 0.075Others 0.027 0.217Experiment-2winter wheat 0.458 0.062Others 0.013 0.467Experiment-3winter wheat 0.329 0.071Others 0.017 0.583Accuracy, precision, recall, and the Kappa coefﬁcient were used to evaluate the models.These indices are calculated via mixing matrix c.Accuracy is the ratio of the number of correctly classiﬁed samples to the total number of samples,and is given in this case by the following equation:Accuracy =Â2i=1CiiÂ2i=1Â2j=1Cij(8)Here, Ciidenotes the number of correctly classiﬁed samples, and Cijdenotes the number ofsamples of class misidentiﬁed as class j.Precision denotes the average proportion of pixels correctly classiﬁed to one class from the totalretrieved pixels. Precision is calculated as:Precision =12ÂiCii/ÂjCij (9)Recall represents the average proportion of pixels that are correctly classiﬁed in relation to theactual total pixels of given class. Recall is computed as:Recall =12ÂiCii/ÂiCij (10)Appl. Sci. 2018,8, 1981 13 of 20The Kappa coefﬁcient measures the consistency of the predicted classes with artiﬁcial labels.The Kappa coefﬁcient is computed as:Kappa =Â2i=1CiiÂ2i=1Â2j=1CijÂ2i=1CiiÂ2j=1Cij(Â2i=1Â2j=1Cij)21Â2i=1CiiÂ2j=1Cij(Â2i=1Â2j=1Cij)2(11)Equations (8)–(11) use the deﬁnitions given in Reference 18] and are modiﬁed according to ouractual situation. The minimum accepted precision is 89% according to practical application.The indicator values are listed in Table 4.Table 4. Comparison of the approaches using SegNet, DeepLab, and HSCNN.Approach Index Experiment-1 Experiment-2 Experiment-3 AverageSegNetAccuracy 0.784 0.763 0.748 0.765Precision 0.740 0.767 0.721 0.743Recall 0.718 0.766 0.720 0.734Kappa 0.579 0.617 0.564 0.586DeepLabAccuracy 0.838 0.875 0.847 0.853Precision 0.815 0.877 0.830 0.840Recall 0.778 0.877 0.852 0.836Kappa 0.665 0.778 0.716 0.720HSCNNAccuracy 0.898 0.925 0.912 0.912Precision 0.895 0.927 0.897 0.906Recall 0.853 0.927 0.921 0.900Kappa 0.776 0.860 0.826 0.8214. Analysis and DiscussionFrom the experimental results in Section 3, it is clear that our method signiﬁcantly improves theaccuracy of winter wheat extraction. In this section, the advantages of our model are discussed interms of the differences between the remote sensing images and camera images. This is followed bymore speciﬁc comparisons with SegNet and DeepLab. Finally, the role of our model in the classiﬁcationof land uses by remote sensing is discussed brieﬂy.4.1. Advantages of the HSCNN ModelCNNs have achieved signiﬁcant success in camera image segmentation, which has motivatedresearchers to apply them to remote sensing images. The HSCNN model proposed here is developedbased on previous work followed by further in-depth analysis of the fundamental differencebetween camera images and remote sensing images. Thus, it possesses clear advantages comparedwith the traditional practice of the straightforward application of camera image segmentation modelto remote sensing images.Camera images and remote sensing images essentially differ in information representation.Owing to their advantages in shooting distance and the pixel quality of the camera, camera imagesare superior in terms of the rich details they contain, such that one object is formed by multiplepixels. Thus, the color of pixel reﬂects the information at certain point on an object but not thespatial relation between the pixels, which is found and expressed only by convolution. The natureof convolution is to represent the spatial correlation between the pixels by constructing complexﬁtting function by operating on the pixel value of pixel block. Particularly because it makes good useof the essential characteristics of camera images, deep convolution is extremely successful in cameraimage processing.Appl. Sci. 2018,8, 1981 14 of 20In remote sensing images, particularly for crop ﬁelds, pixel generally contains multipleobjects. For example, generally in GF-2 images m2of ground is covered by pixel, which contains600–700 winter wheat plants. pixel embodies the color information of the plants and the spatialinformation between them. However, at the edge of winter wheat ﬁeld, the region covered by pixelis often mixture of the winter wheat and bare land or winter wheat and other geographical objects,with varying percentages of winter wheat in the space. In this view, the information contained ina pixel at the edge region is signiﬁcantly different from that at the interior. These two types of pixelscan even be regarded as two different types of objects.Based on the above analyses, the HSCNN network architecture is designed with completeconsideration of the properties of the remote sensing image and extraction target, and it makesgood use of the characteristics of the winter wheat ﬁeld captured in the GF-2 remote sensing images.The strengths of this model are exhibited in the following three aspects:1. Considering the signiﬁcant difference between the pixels of the interior and edge region of thewinter wheat plantation (during extraction), these two regions are treated as two subclasses.Accordingly, the features of the inner pixels are more focused, which facilitates the model training.Two sub-networks with different depths are then designed with respect to the characteristics ofthe two subclasses. The deep sub-network extracts the pixels at the interior, whereas the shallowersub-network extracts those at the edge. This scheme reduces the effect of the non-winter wheatpixels on the features and improves the stability of the model for edge pixel extraction.2. Two types of kernels are used in the ﬁrst convolutional layer. The ⇥1⇥4 kernels are usedto extract the feature of the pixels, and the ⇥3⇥4 kernels are used to extract the spatialrelation between pixels. This design takes advantage of the ability of convolution for extractinghigher-level spatial semantics and for obtaining the rich pixel information contained in the remotesensing images.3. Our model does not utilize pooling, instead the convolution result is taken as the eigenvalueof the central pixel of the pixel block. In the application of the convolutional network to imageclassiﬁcation, the basic target (sample) for the classiﬁcation is the entire image. Thus, pooling canproduce the main features of the feature map and reduce the amount of subsequent computation.Although the information on the accurate position of the features is lost during this process,their relative positions are nevertheless retained, which ensures the normal operation of thesubsequent computational steps. However, in image segmentation, the basic target (sample) forthe classiﬁcation is an individual pixel, whose exact location must be mapped by the eigenvalue.Therefore, the major advantage of our model is its ability to preserve the spatial location of theeigenvalue, which makes it possible to remove the deconvolution adopted by the traditionalFCN. Accordingly, the amount of computation is reduced. Further, the loss in precision due topositioning error is reduced, as the accurate position of the eigenvalue is kept.4.2. Comparison with SegNet and AnalysisSegNet is founded on the FCN model. Its main strength lies in the search and extraction of therich details of an image by deep convolution, and it is very distinct when extracting target objectswith relatively few pixels. If the target objects contain only few pixels or even one pixel, the deepconvolution does not generate more details and may introduce more noise owing to the expandedﬁeld of view, affecting the determination of the pixel type.In the remote sensing images of GF-2, the edge and interior of the winter wheat plantation arevery different in composition, which makes it more difﬁcult for SegNet to locate the common features,because of its structure containing single convolutional network. In comparison, the HSCNN isequipped with two sub-networks of different depths and is adaptable to the characteristics of the edgeand interior. It also uses two different sizes of kernel, which are capable of uncovering the spatialrelation between the pixels, and the information embedded in the pixels.Appl. Sci. 2018,8, 1981 15 of 20As shown in Figure 3, the segmentation results of HSCNN and SegNet are nearly identical for theinterior of the winter wheat ﬁeld. SegNet, however, produces prominent errors at the edge of the ﬁeld,while HSCNN does not.Both HSCNN and SegNet use classiﬁers to generate the probability distribution of the classes,and consider the class with the maximum probability (max) in the distribution as the type to whichthe pixel belongs. Clearly, larger difference between the max and background implies higherseparability of the pixels and more reliable results. The probability differences given by the HSCNNand SegNet model for the inner wheat and edge wheat classes are presented in Figures 4and5,respectively. It is clear in Figure 4that HSCNN and SegNet lead to signiﬁcant probability differencesfor many pixels in the interior, which demonstrates the high separability of this region and the strengthof CNN. In the probability distribution in Figure 5, fewer pixels are noted as having large probabilitydifferences in both the HSCNN and SegNet; nevertheless, the number is maintained at quite highlevel for the HSCNN, whereas SegNet exhibits reduced performance.Figure 4. Distribution of the probability differences for the inner wheat pixels.Figure 5. Distribution of the probability differences for the edge wheat pixels.4.3. Comparison with DeepLab and AnalysisCompared with the FCN and SegNet, DeepLab has signiﬁcant improvements in two aspects:(1) the deconvolution; and (2) the reﬁnement of the boundary of the segmentation result by fullyconnected CRFs. These two improvements are beneﬁcial for the segmentation of individual objectscovering numerous pixels. Based on the published literature, DeepLab displays higher segmentationAppl. Sci. 2018,8, 1981 16 of 20accuracy at the boundary than the FCN and SegNet, because it better utilizes the detailed informationcontained in the image and the large-scale spatial correlation between the pixels. However, in itsapplication to winter wheat identiﬁcation, the strength of DeepLab is not fully realized, because thedetails within pixel block of the winter wheat plantation do not change signiﬁcantly. Therefore, lessinformation is available to the model, and the spatial correlation within the farmlands and woods isnot strong over large regions.As mentioned in Section 4.2, the HSCNN completely utilizes the characteristics of the pixels andthe spatial relation between them. Therefore, it is well adapted to the data characteristics of the winterwheat plantation. Further, it effectively avoids the deﬁciencies of DeepLab and ensures the accuracyof segmentation.As in Section 4.2, the probability differences between the HSCNN and DeepLab models in the innerwheat and edge wheat class are displayed in Figures 6and7, respectively. It is clear in Figure 6thatboth the models produce large probability differences for many pixels in the interior. In the probabilitydistribution of Figure 7, considerable number of pixels still display large probability differencesafter the HSCNN processing, whereas DeepLab shows much poorer performance (even lower thanSegNet), proving again the notion that the atrous convolution is not suitable for farmlands. Figure 6. Distribution of the probability differences for the inner wheat pixels.Figure 7. Distribution of the probability differences for the edge wheat pixels.Appl. Sci. 2018,8, 1981 17 of 204.4. Beneﬁts of Using the Proposed Approach to Classify Land UseAccurate land use classiﬁcation is of tremendous importance in scientiﬁc research and agriculturewith the use of remote sensing data as an increasingly common practice for this purpose. Based ona CNN and taking complete advantage of the convolution in feature extraction, the design of the CNNarchitecture adapting to the features of the remote-sensing images is the key in land use classiﬁcationby this method.We have taken the feature difference between the edge pixel and the inner pixel in the white wheatplanting area into full consideration, this signiﬁcantly improve the extraction accuracy of the edgepixel. Compared with earlier research, the model presented in this paper has the following advantages.Firstly, two types of kernels were used in the convolution of the model, which allowed the fullutilization of the strength of the convolution in the extraction of spatial semantics and made appropriateuse of the rich information contained in the pixels of the remote sensing images, thus achieving moreaccurate segmentation.Secondly, pooling layers were not used in the model. Although the speed of the featureaggregation was consequently reduced, the information of the exact location to which an eigenvaluecorresponds was retained, thereby effectively mitigating the loss in the accuracy due to the positioningerror of the deconvolution and improving the overall effect of the segmentation.The model presented in this paper provides solution for the edge extraction problem or thesegmentation of the winter wheat plantation using GF-2 images. It has an important role to play andenhances the efﬁciency of the agricultural survey. This model has been utilized by the Department ofAgriculture and the Meteorological Bureau of Shandong Province, China.5. ConclusionsThis paper presents novel approach for the extraction of the winter wheat distribution from GF-2remote-sensing images. Compared with the two typical deep learning-based approaches, the extractionaccuracy is obviously improved. Our approach combines the segmentation and classiﬁcation stages,taking the accuracy as the only constraint, and achieves high quality classiﬁcation in an end-to-endway. The GT classes of ground objects are taken as the supervised information that guides both thefeature extraction and the region generation. Taking into account the signiﬁcant differences betweenpixel and edge pixel in the planting area, different convolution structures were used to extract thefeature of edge and interior pixels, focusing on the common features in the two subclasses for moreeffective model training, and obtained high resolution class prediction.Our model is still limited in many aspects, and further improvements could be made in thefollowing two areas: (1) The current encoder uses relatively simple regression algorithm for encoding;thus, regression that can express the complex relationship between the eigenvalues needs to beexplored. (2) new pooling method, which allows for expedited feature aggregation without the lossof the spatial information of the eigenvalues, should be established. We will continue our work in thefuture to improve the current model and obtain better classiﬁcation performance.Author Contributions: C.Z. wrote the manuscript; C.Z. and S.G. presented the direction of this study anddesigned the experiments; X.Y. and F.L. carried out the experiments and analyzed the results; M.Y. and Y.Z. carriedout ground investigation and preprocessing; and Y.H., H.Z. and K.F. carried out sample labeling.Funding: This work was funded by National Key R&D Program of China, grant number 2017YFA0603004;Science Foundation of Shandong, grant numbers ZR2017MD018 and ZR2016DP01; the National ScienceFoundation of China, grant number 41471299; and Open research project of Key Laboratory on meteorologicaldisaster monitoring, early warning and risk management in characteristic agricultural areas of arid area, grantnumbers CAMF-201701 and CAMF-201803.Conﬂicts of Interest: The authors declare no conﬂict of interest.Appl. Sci. 2018,8, 1981 18 of 20References1. Announcement of the National Statistics Bureau on Grain Output in 2017. Available online: http://www.gov.cn/xinwen/2017-12/08/content_5245284.htm (accessed on December 2017).2. Wang, L.M.; Liu, J.; Yao, B.M.; Ji, F.H.; Yang, F.G. Area change monitoring of winter wheat based onrelationship analysis of GF-1 NDVI among different years. Trans. CSAE 2018,34, 184–191. CrossRef ]3. He, H.; Zhu, X.F.; Pan, Y.Z.; Zhu, W.Q.; Zhang, J.S.; Jia, B. Study on scale issues in measurement of winterwheat plant area by remote sensing. J. Remote Sens. 2008,1, 168–176.4. Zhang, J.H.; Feng, L.L.; Yao, F.M. Improved maize cultivated area estimation over large scale combiningMODIS-EVI time series data and crop phonological information. ISPRS J. Photogramm. Remote Sens. 2014,94,102–113. CrossRef ]5. Wu, M.Q.; Wang, C.Y.; Niu, Z. Mapping paddy ﬁeld in large areas, based on time series multi-sensors data.Trans. CSAE 2010,26, 240–244.6. Xu, Q.Y.; Yang, G.J.; Long, H.L.; Wang, C.C.; Li, X.C.; Huang, D.C. Crop information identiﬁcation based onMODIS NDVI time-series data. Trans. CSAE 2014,30, 134–144.7. Becker-Reshef, I.; Vermote, E.; Lindeman, M.; Justice, C. generalized regression-based model for forecastingwinter wheat yields in Kansas and Ukraine using MODIS data. Remote Sens. Environ. 2010,114, 1312–1323.[CrossRef ]8. Zhang, J.G.; Li, X.W.; Wu, Y.L. Object oriented estimation of winter wheat planting area using remote sensingdata. Trans. CSAE 2008,24.[CrossRef ]9. Zhu, C.M.; Luo, J.C.; Shen, Z.F.; Chen, X. Winter wheat planting area extraction using multi-temporal remotesensing data based on ﬁled parcel characteristic. Trans. CSAE 2011,27, 94–99.10. Lu, L.L.; Guo, H.D. Extraction method of winter wheat phenology from time series of SPOT/VEGETATIONdata. Trans. CSAE 2009,25, 174–179.11. Jha, A.; Nain, A.S.; Ranjan, R. Wheat acreage estimation using remote sensing in tarai region of Uttarakhand.Vegetos 2013,26, 105–111. CrossRef ]12. Wu, M.Q.; Yang, L.C.; Yu, B.; Wang, Y.; Zhao, X.; Niu, Z.; Wang, C.Y. Mapping crops acreages based onremote sensing and sampling investigation by multivariate probability proportional to size. Trans. CSAE2014,30, 146–152.13. You, J.; Pei, Z.Y.; Wang, F.; Wu, Q.; Guo, L. Area extraction of winter wheat at county scale based on modiﬁedmultivariate texture and GF-1 satellite images. Trans. CSAE 2016,32, 131–139. CrossRef ]14. Wang, L.M.; Liu, J.; Yang, F.G.; Fu, C.H.; Teng, F.; Gao, J. Early recognition of winter wheat area based onGF-1 satellite. Trans. CSAE 2015,31, 194–201. CrossRef ]15. Ma, S.J.; Yi, X.S.; You, J.; Guo, L.; Lou, J. Winter wheat cultivated area estimation and implementationevaluation of grain direct subsidy policy based on GF-1 imagery. Trans. CSAE 2016,32, 169–174. CrossRef ]16. Wang, L.M.; Liu, J.; Yang, L.B.; Yang, F.G.; Teng, F.; Wang, X.L. Remote sensing monitoring winter wheat areabased on weighted NDVI index. Trans. CSAE 2016,32, 127–135. CrossRef ]17. Wu, M.Q.; Huang, W.J.; Niu, Z.; Wang, Y.; Wang, C.Y.; Li, W.; Hao, P.Y.; Yu, B. Fine crop mapping bycombining high spectral and high spatial resolution remote sensing data in complex heterogeneous areas.Comput. Electron. Agric. 2017,139, 1–9. CrossRef ]18. Fu, G.; Liu, C.J.; Zhou, R.; Sun, T.; Zhang, Q.J. Classiﬁcation for high resolution remote sensing imageryusing fully convolutional network. Remote Sens. 2017,9, 498. CrossRef ]19. Liu, Y.D.; Cui, R.X. Segmentation of Winter Wheat Canopy Image Based on Visual Spectral and RandomForest Algorithm. Spectrosc. Spect. Anal. 2015,35, 3480–3485.20. Dong, Z.P.; Wang, M.; Li, D.R. High Resolution Remote Sensing Image Segmentation Method byCombining Superpixels with Minimun Spanning Tree. Acta Geod. Cartogr. Sin. 2017,46, 734–742. CrossRef ]21. Basaeed, E.; Bhaskar, H.; Al-Mualla, M. Supervised remote sensing image segmentation using boostedconvolutional neural networks. Knowl.-Based Syst. 2016,99, 19–27. CrossRef ]22. Liu, D.W.; Han, L.; Han, X.Y. High Spatial Resolution Remote Sensing Image Classiﬁcation Based on DeepLearning. Acta Opt. Sin. 2016,36, 0428001. CrossRef ]23. Luo, B.; Zhang, L.P. Robust autodual morphological proﬁles for the classiﬁcation of high-resolution satelliteimages. IEEE Trans. Geosci. Remote 2014,52, 1451–1462. CrossRef ]Appl. Sci. 2018,8, 1981 19 of 2024. Li, D.R.; Zhang, L.P.; Xia, G.S. Automatic Analysis and Mining of Remote Sensing Big Data. Acta Geod.Cartogr. Sin. 2014,43, 1211–1216. CrossRef ]25. Chan, T.H.; Jia, K.; Guo, S.H.; Lu, J.; Zeng, Z.; Ma, Y. PCANet: Simple Deep Learniing Baseline for ImageClassiﬁcation. IEEE Trans. Image Process. 2015,24, 5017–5032. CrossRef ][PubMed ]26. Mas, J.F.; Flores, J.J. The application of artiﬁcial neural networks to the analysis of remotely sensed data.Int. J. Remote Sens. 2008,29, 617–663. CrossRef ]27. Gustavo, C.V.; Bruzzone, L. Kernel-based methods for hyperspectral image classiﬁcation. IEEE Trans. Geosci.Remote Sens. 2005,43, 1351–1362.28. Mountrakis, G.; Im, J.; Ogole, C. Support vector machines in remote sensing: review. ISPRS J. Photogramm.Remote Sens. 2011,66, 247–259. CrossRef ]29. Paciﬁci, F.; Chini, M.; Emery, W.J. neural network approach using multi-scale textural metrics from veryhigh-resolution panchromatic imagery for urban land-use classiﬁcation. Remote Sens. Environ. 2009,113,1276–1292. CrossRef ]30. Huang, X.; Zhang, L. An SVM ensemble approach combining spectral, structural, and semantic featuresfor the classiﬁcation of high resolution remotely sensed imagery. IEEE Trans. Geosci. Remote Sens. 2013,51,257–272. CrossRef ]31. Liu, C.; Hong, L.; Chen, J.; Chun, S.S.; Deng, M. Fusion of pixel-based and multi-scale region-based featuresfor the classiﬁcation of high-resolution remote sensing image. J. Remote Sens. 2015,19, 228–239. CrossRef ]32. Yuan, Y.; Lin, J.; Wang, Q. Hyperspectral Image Classiﬁcation via Multitask Joint Sparse Representation andStepwise MRF Optimization. IEEE Trans. Cybern. 2016,46, 2966–2977. CrossRef ][PubMed ]33. Xie, F.D.; Li, F.F.; Lei, C.K.; Ke, L.N. Representative Band Selection for Hyperspectral Image Classiﬁcation.ISPRS Int. J. Geo-Inf. 2018,7, 338. CrossRef ]34. Bengio, Y. Learning deep architectures for AI. Found. Trends Mach. Learn. 2009,2, 1–127. CrossRef ]35. Larochelle, H.; Bengio, Y.; Louradour, J.; Lamblin, P. Exploring strategies for training deep neural networks.J. Mach. Learn. Res. 2009,10, 1–40.36. Jones, N. The learning machines. Nature 2014,505, 146–148. CrossRef ][PubMed ]37. Gao, Q.S.; Lim, S.S.; Jia, X.P. Hyperspectral Image Classiﬁcation Using Convolutional Neural Networks andMultiple Feature Learning. Remote Sens. 2018,10, 299. CrossRef ]38. Dong, Y.; Liu, Y.N.; Lian, S.G. Automatic age estimation based on deep learning algorithm. Neurocomputing2016,187, 4–10. CrossRef ]39. Taormina, R.; Chau, K.W. Data-driven input variable selection for rainfall–runoff modeling usingbinary-coded particle swarm optimization and Extreme Learning Machines. J. Hydrol. 2015,529, 1617–1632.[CrossRef ]40. Liang, Z.; Shan, S.; Liu, X.; Wen, Y. Fuzzy prediction of AWJ turbulence characteristics by using typicalmulti-phase ﬂow models. Eng. Appl. Comput. Fluid Mech. 2017,11, 225–257. CrossRef ]41. Bellary, S.A.I.; Adhav, R.; Siddique, M.H.; Chon, B.H.; Kenyery, F.; Samad, A. Application of computationalﬂuid dynamics and surrogate-coupled evolutionary computing to enhance centrifugal-pump performance.Eng. Appl. Comput. Fluid Mech. 2016,10, 171–181. CrossRef ]42. Zhang, J.; Chau, K.W. Multilayer Ensemble Pruning via Novel Multi-sub-swarm Particle SwarmOptimization. J. Univ. Comput. Sci. 2009,15, 840–858.43. Wang, W.C.; Chau, K.W.; Xu, D.M.; Chen, X.Y. Improving forecasting accuracy of annual runoff time seriesusing ARIMA based on EEMD decomposition. Water Resour. Manag. 2015,29, 2655–2675. CrossRef ]44. Zhang, S.; Chau, K.W. Dimension reduction using semi-supervised locally linear embedding for vegetationleaf classiﬁcation. Emerg. Intell. Comput. Technol. Appl. 2009,5754, 948–955.45. Wu, C.; Chau, K.; Fan, C. Prediction of rainfall time series using modular artiﬁcial neural networks coupledwith data-preprocessing techniques. J. Hydrol. 2010,389, 146–167. CrossRef ]46. Castelluccio, M.; Poggi, G.; Sansone, C.; Verdoliva, L. Land Use Classiﬁcation in Remote Sensing Images byConvolutional Neural Networks. arXiv 2015; arXiv:1508.00092.47. Hu, F.; Xia, G.S.; Hu, J.; Zhang, L. Transferring deep convolutional neural networks for the scene classiﬁcationof high-resolution remote sensing imagery. Remote Sens. 2015,7, 14680–14707. CrossRef ]48. Zhu, C.; Cheng, T. Research on geological hazard identiﬁcation based on deep learning. In Proceedings ofthe 6th International Conference on Computer-Aided Design, Manufacturing, Modeling and Simulation,Busan, Korea, 14–15 April 2018. CrossRef ]Appl. Sci. 2018,8, 1981 20 of 2049. Wu, Z.; Zhang, Q. On combining spectral, textural and shape features for remote sensing image segmentation.Acta Geod. Cartogr. Sin. 2013,42, 44–50.50. Noh, H.; Hong, S.; Han, B. Learning deconvolution network for semantic segmentation. In Proceedings ofthe IEEE International Conference on Computer Vision, Santiago, Chile, 7–13 December 2015; pp. 1520–1528.51. Paisitkriangkrai, S.; Sherrah, J.; Janney, P.; Hengel, V.D. Effective semantic pixel labelling with convolutionalnetworks and conditional random ﬁelds. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition Workshops, Boston, MA, USA, 7–12 June 2015; pp. 36–43.52. Papandreou, G.; Kokkinos, I.; Savalle, P.A. Untangling local and global deformations in deep convolutionalnetworks for image classiﬁcation and sliding window detection. arXiv 2014; arXiv:1412.0296.53. Badrinarayanan, V.; Handa, A.; Cipolla, R. SegNet: deep convolutional encoder-decoder architecture forrobust semantic pixel-wise labelling. arXiv 2015; arXiv:1505.07293.54. Ioffe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariateshift. arXiv 2015; arXiv:1502.03167.55. Liu, J.; Liu, B.; Lu, H. Detection guided deconvolutional network for hierarchical feature learning.Pattern Recognit. 2015,48, 2645–2655. CrossRef ]56. Volpi, M.; Ferrari, V. Semantic segmentation of urban scenes by learning local class interactions.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Boston,MA, USA, 7–12 June 2015; pp. 1–9.57. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv ,2014; arXiv:1409.1556.58. Mittal, A.; Hooda, R.; Sofat, S. LF-SegNet: Fully Convolutional Encoder–Decoder Network for SegmentingLung Fields from Chest Radiographs. Wirel. Pers. Commun. 2018,101, 511–529. CrossRef ]59. Kendall, A.; Badrinarayanan, V.; Cipolla, R. Bayesian SegNet: Model uncertainty in deep convolutionalencoder-decoder architectures for scene understanding. arXiv 2015; arXiv:1511.02680.60. Chen, L.C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; Yuille, A.L. Semantic Image Segmentation with DeepConvolutional Nets and Fully Connected Crfs. arXiv 2014; arXiv:1412.7062.61. Long, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks for semantic segmentation. In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7–12 June 2015;pp. 3431–3440.62. Längkvist, M.; Kiselev, A.; Alirezaie, M.; Loutﬁ, A. Classiﬁcation and segmentation of satellite orthoimageryusing convolutional neural networks. Remote Sens. 2016,8, 239. CrossRef ]63. Dolz, J.; Desrosiers, C.; Ben, A. 3D fully convolutional networks for subcortical segmentation in MRI:A large-scale study. NeuroImage 2018,170, 456–470. CrossRef ][PubMed ]64. Badrinarayanan, V.; Kendall, A.; Cipolla, R. SegNet: deep convolutional encoderdecoderarchitecture forimage segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017,39, 2481–2495. CrossRef ][PubMed ]65. Ronneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation.arXiv 2015; arXiv:1505.04597.66. Chen, L.C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; Yuille, A.L. Deeplab: Semantic image segmentationwith deep convolutional nets, atrous convolution, and fully connected crfs. arXiv 2016; arXiv:1606.00915.67. Lin, H.N.; Shi, Z.W.; Zou, Z.X. Maritime Semantic Labeling of Optical Remote Sensing Images withMulti-Scale Fully Convolutional Network. Remote Sens. 2017,9, 480–501. CrossRef ]68. Visin, F.; Ciccone, M.; Romero, A.; Kastner, K.; Cho, K.; Bengio, Y.; Matteucci, M.; Courville, A. Reseg:A recurrent neural network-based model for semantic segmentation. arXiv 2016; arXiv:1511.07053.69. Statistical Yearbook of Shandong Province. Available online: http://www.stats-sd.gov.cn/col/col6279/index.html (accessed on 27 October 2017).70. Haralick, R.M.; Shanmugam, K. Texture features for image classiﬁcation. IEEE Trans. Syst. Man Cybern. 1973,SMC-3 610–621. CrossRef ]©2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open accessarticle distributed under the terms and conditions of the Creative Commons Attribution(CC BY) license http://creativecommons.org/licenses/by/4.0/ ).