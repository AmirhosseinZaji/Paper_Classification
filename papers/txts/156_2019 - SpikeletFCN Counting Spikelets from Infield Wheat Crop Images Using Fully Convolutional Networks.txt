SpikeletFCN: Counting Spikeletsfrom Inﬁeld Wheat Crop ImagesUsing Fully Convolutional NetworksTahani Alkhudaydi1,2(B), Ji Zhou1,3,4,a dB a r zD eL al l s a11University of East Anglia, Norwich Research Park, Norwich NR4 7TJ, UK{t.alkhudaydi,b.iglesia }@uea.ac.uk2Faculty of Computers and IT, University of Tabuk, Tabuk 71491, Saudi Arabiatalkhudaydi@ut.edu.sa3Earlham Institute, Norwich Research Park, Norwich NR4 7UZ, UKJi.Zhou@earlham.ac.uk4Plant Phenomics Research Center, China-UK Plant Phenomics Research Centre,Nanjing Agricultural University, Nanjing 210095, ChinaJi.Zhou@njau.edu.cnAbstract. Currently, crop management through automatic monitoringis growing momentum, but presents various challenges. One key challengeis to quantify yield traits from images captured automatically. Wheat isone of the three major crops in the world with total demand expectedto exceed 850 million tons by 2050. In this paper we attempt estima-tion of wheat spikelets from high-deﬁnition RGB inﬁeld images usinga fully convolutional model. We propose also the use of transfer learn-ing and segmentation to improve the model. We report cross validatedMean Absolute Error (MAE) and Mean Square Error (MSE) of 53.0,71.2 respectively on 15 real ﬁeld images. We produce visualisations whichshow the good ﬁt of our model to the task. We also concluded that bothtransfer learning and segmentation lead to very positive impact forCNN-based models, reducing error by up to 89%, when extracting keytraits such as wheat spikelet counts.Keywords: Wheat ·Spikelet counting ·Plant phenotyping ·Image analysis ·CNN·Density estimation1I t o u t o nThe application of the internet of things (IoT) in agriculture has enabled themonitoring of crop growth through networked remote sensors and non-invasiveimaging devices 7,27]. Analysis of the output of such systems with machinelearning and image processing techniques can help to extract meaningful infor-mation to assist crop management. For example, yield quantiﬁcation can be tiedto other features measured (e.g. temperature, humidity, variety of seed, etc.)to ultimately develop fully automated monitoring systems capable of deliveringreal-time information to farmers.c/circlecopyrtSpringer Nature Switzerland AG 2019L. Rutkowski et al. (Eds.): ICAISC 2019, LNAI 11508, pp. 3–13, 2019.https://doi.org/10.1007/978-3-030-20912-4 _14 T. Alkhudaydi et al.Wheat is one of the three major crops in the world with total demand expectedto exceed 850 million tons by 2050 1]. One of the key challenges for wheat is tostabilise the yield and quality in wheat production 22]. However, climate changeand related environmental issues have aﬀected yield production 11].In this paper, we focus on the task of counting spikelets in wheat images asa form of yield quantiﬁcation for wheat crops. In particular, we use densityestimation method which has been applied in the context of crowd counting 14],to count spikelets.The tasks of counting wheat spikelets from inﬁeld images (Fig. 1)( so p s dto images obtained in some constrained lab environment) presents some real chal-lenges because of their self-similarity, high volume per image, and severe occlu-sion as well as the challenges posed by lighting and other variations in the imagescaptured. Image processing or machine-learning approaches for object countingrequire manual identiﬁcation of features. Deep learning can automatically extractuseful features, and can also lead to high accuracy in image classiﬁcation tasks[13]. Convolutional Neural Networks (CNNs), particular type of deep learningmodel, learn their own features representations and have shown real promise inmany areas in computer vision and plant phenotyping 24]. For that reason andbecause density estimation is considered as structural problem (requiring pre-diction for each pixel in the image), we employ Fully Convolutional Network(FCN) 15]t os l et et s .(a) (b)SpikeletsSpikesFig. 1. Example of images from (a) ACID dataset and (b) CropQuant dataset whichshows spikes, and spikelets.Furthermore, because the data annotation required to extract ‘ground truth’from images is expensive in term of time and resources, we utilised transferlearning in the task of density estimation 18,25]. Transfer learning enhances theimage training set with further labelled images from other context and those canbe used to pre-train some of the parameters improving the model ﬁt.Our overall approach is as follows. We employ fully convolutional model(SpikeletFCN) to perform density estimation from dot annotated images. Weutilise additional labelled data for the density estimation by means of trans-fer learning. In addition, we investigate training SpikeletFCN with and with-out prior segmentation and compare the performance of each. Section 2presentsresearch that is relevant to our method. Section 3discusses the datasets we used,Counting Spikelets with CNNs 5the architecture of SpikeletFCN, model optimisation and training proceduredetails. Section 4describes the performance results of testing SpikeletFCN andthe their interpretation. Finally, Sect. 5presents our conclusions.2R l t d o kObject counting from images is diﬃcult problem that emerges in many dif-ferent scenarios, for example, monitoring crowds 26], performing wildlife census[2], counting blood cells in images 8] and others. Supervised counting methodsrequired labelled images with ground truth. Methods for supervised countinginclude counting by detection or by segmentation, regression based methodssuch as global regression 3,6,12], local regression 4] and density estimation[14].Many works 2,19,21] have used detection or segmentation in various ways,but they may require intensive labelling. However, when the only task requiredis to determine the total number of certain object in an image rather thandetecting them or their position, then counting by regression can be more naturaland suitable, specially when the number of objects per image is high. It can bedivided into three sub-methods: global regression, density estimation and localregression.Global Regression often maps global image features to real number 3,6,12].However, as stated by Lempitsky and Zisserman 14] extracting these featuresglobally discards information about the location of the objects which may beimportant in some contexts. Also, suﬃcient labelled images would be requiredto represent diﬀerent counts for training purposes.Learning to count objects through density estimation regression 14]t k sinto account the spatial information of objects. Density estimation regressionlearns mapping from local features into pixel level densities. This gives theadvantage of integral density estimation over any image regions. Lempitsky andZisserman 14] used dot annotations to infer density maps and utilised them astraining ground truths by applying normalised 2D Gaussian kernel. Then, theydesigned counting cost function that minimises the distance between the targetdensity map and the inferred ground truth one. Subsequently, Fiaschi et al. 5]used random forest regression, which optimised the training process to predictthe density map.On the other hand, Local Regression 4] predicts the local count of smallregion in the image directly without the need to predict density map. How-ever, it uses the density map in the training stage to infer object counts. Also,it employs the concept of redundant counting to ensure maximum countingprecision.Although it captures the local features of objects, it can be expensive andineﬃcient in term of time and computational resources.6 T. Alkhudaydi et al.2.1 Counting in Plant PhenotypingCounting organs or constituent parts of plants is an essential and important taskto be tackled in plant phenotyping. For example, TasselNet 16]w sd v l p dt ocount maize tassels from inﬁeld maize crop. TasselNet performs counting by localregression using deep convolutional neural network-based approach. Poundet al. 18]d v l p dam l i t s kd e pl a n n gm d lt oc u ta dl c l s ew e tspikes and spikelets, achieving good accuracy. They tested the model on wheatcrop images captured in controlled environment inside glasshouse. Theirproblem is therefore similar to ours but simpler given the reduced variation inthe controlled laboratory environment as opposed to real ﬁeld image. Figure 1shows both type of images.Also, Madec et al. 17] investigated counting spikes from inﬁeld wheat cropimages captured by UAV platform using two CNN-based models. The ﬁrst wasFaster-RCNN 20], CNN based object detection model. The second was anadaptation of TasselNet 16]f rt i st s .T e yc n l d dt a tb t hm d l sachieved similar results when tested on images containing crops that have asimilar distribution of spikes as the images both models trained on. However,they found that Faster-RCNN outperformed other models when tested on imagescontaining more mature crops.(a) (b) (c)Fig. 2. An example of spikelets density generation where: (a) represents sub-image of awheat crop, (b) represents corresponding dot annotation and (c) the generated densitymap from the dot annotation.conv1 conv2 conv3 conv4 conv5FC6FC7P1 P4 P3 P2 P5Upsample(FCN-32) Upsample (FCN-16) Upsample(FCN-8) Score Score 4ScoreP 3Fig. 3. SpikeletFCN architectureCounting Spikelets with CNNs 7Also, Hasan et al. 9] tackled the problem of counting spikes by using anR-CNN object detector. They trained four versions of R-CNN on four diﬀerentgrowth stages of inﬁeld wheat images that vary in growth stage and variety andreported good results.3S i e e s o n i g s n S i e e F N3.1 Problem StatementWe propose to model the problem of counting spikelets as density estimationproblem. Given Ninput images I1,I2,...,I Nwith size of H×W×Dthatrepresent inﬁeld wheat crop plots, for each image, Ii, there is correspondingdot map Pithat can be represented as set of 2D points Pi={P1,...,P SPC i},where |Pi|is the number of spikelets in image Ii. Each point is placed at thecentre of each spikelet as shown in Fig. 2(b). To generate the ground truth mapGTi(shown in Fig. 2(c)), 2D Gaussian kernel N(p;P,σ212×2) is applied to thedot map Piwhich generates density for each pixel pof image Ii. Therefore,the size of GTiis the same as the input image: GTi={DP1,...,DPH×W}whereDPjis the generated density for the jthpixel in image Ii.The eﬀect of applying the Gaussian kernel is that it can reﬂect the crowdingaround spikelet by taking into account the information of the pixel’s neigh-bourhood when updating its density value. In other words, the more spikeletocclusion in certain region, the high density values will be assigned to pixelsin the region.The total number of spikelets in certain image Iiis the sum of all pixelsdensities in GTi:|Pi|=SPC i=/summationdisplayp∈IiDp(1)3.2 DatasetsCropQuant Dataset [27]. We used 15 high-dimensional RGB image series of6×1.5 wheat plots collected at Norwich Research Park (NRP) between Mayand July 2016. The image series covers one growing stage: ﬂowering. The res-olution of images is 2592 by 1944 pixels, which were captured hourly by R-picamera modules integrated in the CropQuant workstation. Image data were syn-chronised with HPC data storage infrastructure at NRP. We have dot annotatedeach image by placing dot in the centre of each spikelet. The total number ofspikelets in all images is 63,006 and the average spikelet number per scene is4200.4 with standard deviation of 197.ACID Dataset. The Annotated Crop Image Dataset (ACID) has 520 imagesof wheat plants captured from 21 pots in glasshouse with resolution of1956 ×1530. The imaging is done by 12 MP cameras and all images have ablack background. The images show diﬀerent spike arrangements and leaves and8 T. Alkhudaydi et al.were obtained in consistent lighting. Also, the images were dot annotated byplacing dot in the centre of each spikelet. The total number of spikelets inall images is 48,000 and the average spikelet number per scene is 92.3 with astandard deviation of 28.52.Figure 1shows examples of both CropQuant and ACID images which exem-plify their similarities and diﬀerences.3.3 SpikeletFCN ArchitectureIn our approach, we apply fully convolutional network to tackle the problem ofspikelet counting. Figure 3represents our architecture. The last fully connectedlayers attached in any CNN-based classiﬁers are converted to convolutions. Thisensures that the semantics of target objects are preserved which are essentialfor tasks that require structural predictions (predictions for each pixel) becauseconverting those layers to convolutions provide localisation and shape informa-tion about target objects. Our model, SpikeletFCN, is composed of Very DeepConvolutional Network (VGG16) 23]( i . 3: conv1-P5), formed by two fullyconvolutional (Fig. 3: FC6 and FC7) layers and three upsampling layers. Theﬁlter size selected for all convolutional layers is ×3w t has r d eo f1a dthe max-pool layers have pooling size of ×2 with stride of 2. We employthe concept of feature fusion by adding two skip connections (Fig. 3: after P3and P4) to fuse the local features related to spikelets from lower layers to othershape and semantic features related to the wheat crops from higher layers. Weadded upsampling layers to ensure we recover the original image size aﬀectedby the application of repetitive convolutions and subsampling which reduces theinput size.We found that using pixel-wise L2l s sf n t o n( q 2)a st ec s tf n t o nfor model optimisation gave the best results to regress the per pixel density:L=/summationdisplayp∈Ii(DpGTi−Dppredicted)2(2)where DpGTiis the density ground truth and Dppredictedis the predicted densityfor certain pixel pin image Ii.The weights were updated for every learning iteration using mini-batchRMSprop optimising algorithm 10]w t hal a n n gr t eo f0 .001 and mini-batchof 20.3.4 Experimental Set UpWe ﬁrst formed the training and validation set from the ACID dataset accordingto the 80:20 split rule. Then, we randomly sampled sub-images with size of512×512 for each set. After that, we manually selected 1241 sub-images fromthe training set and 303 sub-images from the validation set that contain spikeregions. With those images we trained the model for 100 epochs for the transferlearning experiments described below.Counting Spikelets with CNNs 9On the other hand, for the CQ 2016 dataset, the limitations imposed by thetask of dot annotating, which is time consuming and could therefore only beaccomplish for very reduced number of images, meant we only had 15 imagesdot annotated for our experiments. We therefore decided to divide them into3-folds for cross validation, with images per fold. Then, we randomly subsam-pled 512 ×512 sub-images from each fold individually.To investigate whether segmenting spike regions could enhance the spikeletscounting task, we manually remove the background using ground truth masks.In future research, we intend to also use CNN to tackle the segmentation,instead of manual approach.We trained the model on each fold of the CQ 2016 dataset, validated andtested on the other two folds in four steps:1. We ﬁrst trained the model from scratch on the original images (no segmen-tation) and the model converged after an average of 155 epochs.2. We then trained the model from scratch on the images with the spike regionsisolated so after this manual segmentation the model converged after an aver-age of 75 epochs.3. We then loaded parameters learned from training the model on the ACIDdataset, as described earlier, and continued ﬁne tuning the model using theoriginal images. The model converged after an average of 36 epochs. Thisrepresents transfer learning, using the ACID dataset in the initial stage ofparameter initialisation and the CQ 2016 dataset to train the ﬁnal model.4. We repeated the previous transfer learning model building step, but thencombined it with continued ﬁne tuning on the CQ 2016 dataset images withthe spike regions isolated and the model converged after an average of 30epochs.For the testing phase, SpikeletFCN predicts the density of each pixel in certainimage. Then, the number of spikelets in the image is calculated by summing allthe predicted densities over the whole image according to Eq. 1in Sect. 3.1.4R s l sObject counting methods use two evaluation metrics to measure the model per-formance when applied on testing images: mean absolute error (MAE) and meanssquare error (MSE).We have calculated the cross-validated performance of the SpikeletFCNmodel for the diﬀerent experimental steps described in Sect. 3.4using the MAEand MSE measures. Table 1shows our results. Table 1shows that applying seg-mentation before counting has decreased the spikelet counting error to 82.2 and102.0 for MAE and MSE respectively when training SpikeletFCN from scratch.This represents reduction of 83.5% and 81.2% respectively for MAE and MSEwith respect to error measures without segmentation.In terms of transfer learning, loading ACID pre-trained parameters has pos-itive impact on the model performance by decreasing MAE and MSE to 53.0 and10 T. Alkhudaydi et al.Table 1. The MAE and MSE of estimating the number of spikelets for two experi-mental setups (as columns): training SpikeletFCN from scratch and by loading ACIDdataset learned parameters and for pre-segmenting images (in rows) on CQ 2016images.Scratch ACID 18]MAE MSE MAE MSEWith segmentation 82.2 102.0 53.0 71.2Without segmentation 498.0 543.5 77.12 107.171.2 respectively when segmentation is also applied. This represents decreaseof 35.5% and 30.2% respectively when segmentation is applied with respect tothe error from the scratch model. When no segmentation is applied, the trans-fer learning reduces error by 84.5% and 80.3% respectively for MAE and MSE.Hence both segmentation and transfer learning have very signiﬁcant eﬀect onerror rates. It is worth noting that the pre-trained ACID model has minimisedthe gap between the SpikeletFCN performance with and without segmentation.The diﬀerence in missed spikelets when training from scratch is 415.8 for MAEand 441.5 for MSE. On the other hand, the comparative diﬀerence when loadingpre-trained ACID parameters is 24.12 for MAE and 35.9 for MSE. Overall, thediﬀerence between the best model (with segmentation and transfer learning) andthe worse (the scratch model without segmentation) is over 89% for MAE andover 86% for MSE.In term of model training time, we can infer from Sect. 3that loading ACIDpre-trained parameters and training the model in images with segmentation haveresulted on faster training of the model.Also, we analysed the results in more detail through visualisation. Figure 4shows some images with their density maps and respective spikelet counts. Theyshow that visually the density maps obtained appear to be reasonably accuratewith respect to the original images and seem to improve with the segmentation,though in some cases the prediction represents under or over-counting.More detailed visual analysis, in this case for the ACID images, is shown inFig.5. By comparing the density maps generated from our models (column (b)of Fig. 5with the ‘ground truth’ density maps derived from the dot annotation(column (c)) we can note that in some images, SpikeletFCN may be consideredas over-counting because it is able to detect spikelets that were miss-annotated(missed) by accident in the dot annotation. For example, in Fig. 5, SpikeletFCNpredicted spikelet number for the second and third images as 53.47 and 49.93while the ground truth for both images was 46.0 and 46.61. However, in theseimages, spikes that appear to contain single row of spikelets in the dot anno-tation are recognised as having more spikelets by the SpikeletFCN model andthis seems to correlate to the images in column (a). We can assume that asthe dot annotation gets much more complex in the very crowded inﬁeld images,dot annotation may also be more inaccurate, so some of our errors may reﬂectthe inaccuracies of our ground truth.Counting Spikelets with CNNs 11(a) (b) (c) (d) (e)N=1325.18 N=1080.39 N=1202.68N=288.60 N=273.83 N=253.39N=888.22 N=792.46 N=858.77Fig. 4. Visualisation of density map results of testing SpikeletFCN on some CQ 2016sub-images where (a) represents image patch, (b) image patch without background(c) ground truth for spikelet density map and counts, and (d) and (e) are predictedspikelet density map and count for the original image patch and image patch withoutbackground respectively.(a) (b) (c)N=108.00 N=102.17N=46.00 N=53.47N=46.61 N=49.93Fig. 5. Visualisation of density maps resulting from testing for the Adapted Spikelet-FCN on ACID dataset. (a) is image patch, (b) is ‘ground truth’ spikelet density mapand count obtained after dot annotation, and (c) is the predicted spikelet density mapand count.5C n l s o nCounting spikelets from inﬁeld wheat crop images is vital step in quantify-ing yield traits but is very challenging given the variability, density and occlu-sion associated with spikelets in real wheat images. In this paper, we trained12 T. Alkhudaydi et al.and tested SpikeletFCN to count spikelets using density estimation approach.We also attempted to improve our learning by applying transfer learning andsegmentation.Our experimental results were very promising and resulted in good errorrates, much improved by using both manual segmentation and transfer learning.In particular transfer learning did help to improve the performance of the modelstrained on inﬁeld crops images. Error rates decreased by over 81% when usingmanual segmentation and over 86% when combining segmentation with transferlearning. Also, it led to faster training of the model.Visualisation helped us to discover that the process of obtaining ground truthby dot annotation is imperfect and models may actually uncover spikelets whichhave not been dot annotated. This is encouraging as it means the model is ableto learn features of the spikelets, even in the context of imperfect training data.In the future, we plan to test our model on more inﬁeld wheat crops thatvary in year growth, growth stages and other factors. We will also develop CNN-based models to tackle the task of spike segmentation as we have shown that itcan play an important role in improving the task of spikelets counting.References1. Alexandratos, N., Bruinsma, J.: World agriculture towards 2030/2050. Land UsePolicy 20(4), 275 (2012)2. Arteta, C., Lempitsky, V., Zisserman, A.: Counting in the wild. In: Leibe, B.,Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9911, pp. 483–498.Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46478-7 303. Cho, S.Y., Chow, T.W., Leung, C.T.: neural-based crowd estimation by hybridglobal learning algorithm. IEEE Trans. Syst. Man Cybern. Part Cybern. 29(4),535–541 (1999)4. Cohen, J.P., Boucher, G., Glastonbury, C.A., Lo, H.Z., Bengio, Y.: Count-ception:counting by fully convolutional redundant counting. In: 2017 IEEE InternationalConference on Computer Vision Workshop (ICCVW), pp. 18–26. IEEE (2017)5. Fiaschi, L., Koethe, U., Nair, R., Hamprecht, F.A.: Learning to count with regres-sion forest and structured labels. In: 2012 21st International Conference on PatternRecognition (ICPR), pp. 2685–2688. IEEE (2012)6. Giuﬀrida, M.V., Minervini, M., Tsaftaris, S.A.: Learning to count leaves in rosetteplants. In: Proceedings of the Computer Vision Problems in Plant Phenotyping(CVPPP), pp. 7–10 (2016)7. Gubbi, J., Buyya, R., Marusic, S., Palaniswami, M.: Internet of Things (IoT): avision, architectural elements, and future directions. Future Gener. Comput. Syst.29(7), 1645–1660 (2013). https://doi.org/10.1016/j.future.2013.01.0108. Habibzadeh, M., Krzy˙ zak, A., Fevens, T.: White blood cell diﬀerential countsusing convolutional neural networks for low resolution images. In: Rutkowski, L.,Korytkowski, M., Scherer, R., Tadeusiewicz, R., Zadeh, L.A., Zurada, J.M. (eds.)ICAISC 2013. LNCS (LNAI), vol. 7895, pp. 263–274. Springer, Heidelberg (2013).https://doi.org/10.1007/978-3-642-38610-7 259. Hasan, M.M., Chopin, J.P., Laga, H., Miklavcic, S.J.: Detection and analysisof wheat spikes using convolutional neural networks. Plant Methods 14(1), 100(2018). https://doi.org/10.1186/s13007-018-0366-8Counting Spikelets with CNNs 1310. Hinton, G., Srivastava, N., Swersky, K.: Neural networks for machine learning. In:Lecture 6a: Overview of Mini-batch Gradient Descent, p. 14 (2012)11. Howden, S.M., Soussana, J., Tubiello, F.N., Chhetri, N., Dunlop, M., Meinke, H.:Adapting agriculture to climate change. Proc. Nat. Acad. Sci. U.S.A. 104(50),19691–19696 (2007). https://doi.org/10.1073/pnas.070189010412. Kong, D., Gray, D., Tao, H.: viewpoint invariant approach for crowd counting.In: 18th International Conference on Pattern Recognition, ICPR 2006, vol. 3, pp.1187–1190. IEEE (2006)13. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436 (2015)14. Lempitsky, V., Zisserman, A.: Learning to count objects in images. In: Advancesin Neural Information Processing Systems, pp. 1324–1332 (2010)15. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semanticsegmentation (2015)16. Lu, H., Cao, Z., Xiao, Y., Zhuang, B., Shen, C.: TasselNet: counting maize tasselsin the wild via local counts regression network. Plant Methods 13(1), 79 (2017)17. Madec, S., Jin, X., Lu, H., De Solan, B., Liu, S., Duyme, F., Heritier, E., Baret,F.: Ear density estimation from high resolution RGB imagery using deep learningtechnique. Agric. For. Meteorol. 264, 225–234 (2019)18. Pound, M.P., Atkinson, J.A., Wells, D.M., Pridmore, T.P., French, A.P.: Deeplearning for multi-task plant phenotyping. In: 2017 IEEE International Conferenceon Computer Vision Workshop (ICCVW), pp. 2055–2063. IEEE (2017)19. Ren, M., Zemel, R.S.: End-to-end instance segmentation with recurrent attention.In: CVPR (2017)20. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time objectdetection with region proposal networks. In: Advances in Neural Information Pro-cessing Systems, pp. 91–99 (2015)21. Ryan, D., Denman, S., Fookes, C., Sridharan, S.: Crowd counting using multiplelocal features. In: Digital Image Computing: Techniques and Applications, 2009,DICTA 2009, pp. 81–88. IEEE (2009)22. Shewry, P.R.: Wheat. J. Exp. Bot. 60(6), 1537–1553 (2009)23. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scaleimage recognition. CoRR24. Tardieu, F., Cabrera-Bosquet, L., Pridmore, T., Bennett, M.: Plant phenomics,from sensors to knowledge. Curr. Biol. 27(15), R770–R783 (2017)25. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features indeep neural networks? In: Advances in Neural Information Processing Systems,pp. 3320–3328 (2014)26. Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via deep con-volutional neural networks. In: Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 833–841 (2015)27. Zhou, J., et al.: CropQuant: an automated and scalable ﬁeld phenotyping plat-form for crop monitoring and trait measurements to facilitate breeding and digitalagriculture. bioRxiv. https://doi.org/10.1101/161547