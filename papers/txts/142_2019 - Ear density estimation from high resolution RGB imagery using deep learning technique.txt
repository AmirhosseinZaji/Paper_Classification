Contents lists available at ScienceDirectAgricultural and Forest Meteorologyjournal homepage: www.elsevier.com/locate/agrformetEar density estimation from high resolution RGB imagery using deeplearning techniqueSimon Madeca,/uni204E, Xiuliang Jina, Hao Lub, Benoit De Solanc, Shouyang Liua, Florent Duymec,Emmanuelle Heritierc, Frédéric BaretaaINRA, UMR EMMAH, UMT-CAPTE, Avignon, FrancebNational Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology,Wuhan, 430074, ChinacARVALIS, Institut du végétal, Avignon, FranceARTICLE INFOKeywords:Wheat ear densityObject detectionObject countingConvolutional neural networksPhenotypingBroad-sense heritabilityABSTRACTWheat ear density estimation is an appealing trait for plant breeders. Current manual counting is tedious andine/uniFB03cient. In this study we investigated the potential of convolutional neural networks (CNNs) to provide ac-curate ear density using nadir high spatial resolution RGB images. Two di /uniFB00erent approaches were investigated,either using the Faster-RCNN state-of-the-art object detector or with the TasselNet local count regression net-work. Both approaches performed very well (rRMSE ≈6%) when applied over the same conditions as thoseprevailing for the calibration of the models. However, Faster-RCNN was more robust when applied to datasetacquired at later stage with ears and background showing di /uniFB00erent aspect because of the higher maturity ofthe plants. Optimal spatial resolution for Faster-RCNN was around 0.3 mm allowing to acquire RGB images froma UAV platform for high-throughput phenotyping of large experiments. Comparison of the estimated ear densitywith in-situ manual counting shows reasonable agreement considering the relatively small sampling area usedfor both methods. Faster-RCNN and in-situ counting had high and similar heritability (H/uni00B2 ≈85%), demonstratingthat ear density derived from high resolution RGB imagery could replace the traditional counting method.1. IntroductionWheat ear density in wheat crops is associated with components ofcrop yield related to plant population and tiller number per plant, but isa di/uniFB03cult and tedious trait for breeders to /uniFB03ciently measure. Further,it is prone to sampling errors when the sampling area is small due tolimited human resources. Computer vision approaches provide po-tential solution to increase the throughput as well as the spatial re-presentativeness, leading potentially to an improved accuracy. Anumber of studies based on high spatial resolution imaging systemsapplied to plant phenotyping under /uniFB01eld conditions have receivedmuch attention in recent years Li et al., 2014 ). Both ground-based andaerial platform Araus and Cairns, 2014 ;Deery et al., 2014 ;Tardieuet al., 2015 have been exploited to image the microplots with spatialresolution spanning within few centimeters to fraction of millimeter.Because of the typical size of wheat ears and the possible occlusionsbetween them, spatial resolution of few millimeters is required toidentify non-ambiguously the ears. Therefore, most studies focused onhigh-resolution RGB images on which high pass /uniFB01lter andmorphological operators were applied Journaux et al., 2010 )(Fernandez-Gallego et al., 2018 ). Those methods provide promisingresults on small datasets. However, these types of algorithms may failwhen applied to images acquired under di /uniFB00erent conditions and fordi/uniFB00erent development stages: the change in illumination conditions, theocclusions, the variability of ear aspect due to genotype including thepresence or absence of awns, the /uniFB02owering status, the variability of thebackground and the image quality make the scalability of this pheno-typing task challenging.The advances in computation capacity along with the availability ofvery large collections of labelled images have fostered enhanced ma-chine learning methods based on convolutional neural networks (CNNs)in the /uniFB01eld of computer vision Hinton and Salakhutdinov, 2006 ;LeCunet al., 2015 ). CNNs are currently achieving impressive performances forimage classi /uniFB01cation Singh et al., 2016 ;Krizhevsky et al., 2012 ). Be-cause the number of label images required to train CNN models fromscratch is important, pre-trained are often used as starting point.Further, pretrained models generally improved the resulting accuracy(Mohanty et al., 2016 and limit over /uniFB01tting issues Yosinski et al.,https://doi.org/10.1016/j.agrformet.2018.10.013Received 18 May 2018; Received in revised form 15 October 2018; Accepted 22 October 2018/uni204ECorresponding author.E-mail address: simon.madec@inra.fr (S. Madec).$JULFXOWXUDODQG)RUHVW0HWHRURORJ\² $YDLODEOHRQOLQH2FWREHU(OVHYLHU%9$OOULJKWVUHVHUYHG72014 ). Several network architectures have proven their /uniFB00ectivenessover benchmark computer vision database like Alexnet Singh et al.,2016 ), VGG Simonyan and Zisserman, 2014 and more recently re-sidual network with inception layers like Inception-ResNet Szegedyet al., 2016 ). These models are then /uniFB01ne-tuned Dauphin et al., 2012 )on small training dataset speci /uniFB01c to particular classi /uniFB01cation task(Yosinski et al., 2014 ;Donahue et al., 2014 ;Sharif Razavian et al.,2014 ).Some of these methods have already been applied to plant pheno-typing. CNN models have demonstrated to be /uniFB00ective for dis-criminating features for wheat plants including highly accurate iden-ti/uniFB01cation of ears Pound et al., 2017 in glasshouse condition. Similarstudies have shown that CNNs outperform classic hand-crafted featuredescriptors and /uniFB00er an alternative approach for classi /uniFB01cation problems(Pound et al., 2017 ;Allen et al., 2005 ;Madec et al., 2017 ). The de-tection algorithm needs to identify and localize each ear in the image.When the overlap between identi /uniFB01ed objects is common pattern as forcrowded scenes, counting by regression networks was recently de-monstrated to be relevant alternative Huang et al., 2016 ;Hosanget al., 2017 ;The Open Images dataset, 2018 ): the TasselNet model wasproposed for counting maize tassels Lu et al., 2017 ). Tasselnet is basedon CNN with regression output layer. The local counts regressedfrom individual sub-images are merged to provide count map for thewhole image. Likewise, deep residual model with regression outputwas used to count the number of wheat plants at emergence Salton andMcGill, 1983 ). The emergence counting is achieved through two-stageprocess: segmenting wheat plants and regressing the counts from smallimage patches. This presents an alternative way to tackle occludedwheat plants, while the precision of the system is also /uniFB00ected by thesegmentation algorithm used.The main objective of this study is to evaluate deep learning ap-proaches for high-throughput wheat ear counting under /uniFB01eld condi-tions. For this purpose, two types of CNN architectures will be in-vestigated: (i) local object detection and (ii) counting by regression. Thein/uniFB02uence of the spatial resolution of the RGB on the model performancewill be analyzed to select the optimal resolution. Finally, the ear densityestimated from the RGB images will be compared with the in-situ visualear counting and the broad sense heritability is then quanti /uniFB01ed toevaluate the suitability of the proposed method for /uniFB01eld phenotyping.2. Material and methods2.1. Data collection and labelling2.1.1. Experimental siteThe study area is wheat /uniFB01eldphenotyping platform located inGréoux les Bains (France, 43.7° latitude North, 5.8° longitude East).Wheat was sown on November 3rd 2016 with row spacing of 17.5 cmand density of 300 seeds·m−2. trial of 120 microplots of 2.0 widthby 10 long was considered. Half of the microplots was irrigated(called WW) while the other part was subjected to water stress (calledWS). The 20 contrasting genotypes were replicated three times both inthe WW and WS modalities and organized as an alpha-design.A crop water balance model Allen et al., 2005 has been used toestimate the water stress during the whole growing season. It used themeasured soil water holding capacity of 143 mm to compute the actualevapotranspiration at the daily time from the rainfall and the potentialevapotranspiration. The di /uniFB00erence between the actual and maximumevapotranspiration values corresponds to the daily crop water de /uniFB01citthat was cumulated from emergence up to maturity Fig. ). This in-dicated that the water de /uniFB01cit for the WS modality started after the earemergence stage (stage Z59). The irrigation on the WW modality werestarting after this date.2.1.2. Ground measurements of ear densityIn each microplot, the ear density was measured on June the 7th2017 after the /uniFB02owering stage over three segments of m length by twoadjacent rows which represent sampling area of 1.05 m/uni00B2. The /uniFB01rst tworows located at the border of the microplots were not considered in thesampling to minimize border /uniFB00ects.2.1.3. Canopy heightThe height is required to de /uniFB01ne the footprint of the image and tocompute the ear density by dividing the number of ears by the size of itsfootprint. The height was measured with LiDAR /uniFB01xed on fully au-tomated robot called “phénomobile ”(Madec et al., 2017 ). The un-certainties associated to the height estimated by the LiDAR were fewcentimeters. More details can be found in Madec et al., 2017 ).2.1.4. Image acquisition and labellingA Sony ILCE-6000 digital camera with 6000 4000 pixels was/uniFB01xed on boom. The RGB images were taken from the nadir view di-rection at 2.9 distance to the ground. For each microplot two imageswere recorded. The measurements were completed on June 2ndand the16th2017. 60 mm and 50 mm focal lengths were used respectively onJune the 2ndand the 16th. This resulted in ground sampling distancebetween 0.010 0.016 cm/pixel and footprint area of individualimages between 0.25m/uni00B2 and 0.56m/uni00B2 depending on the height of thewheat and the focal length used.The ears were interactively labelled in all the images of the /uniFB01rstexperiment (June 2nd) resulting into 240 images (20 genotypes 3replicates 2 modalities 2 images). Between 80 and 170 ears werecontained in each image. The LABELIMG darrenl, 2017 graphicalimage annotation tool was used to draw the bounding boxes aroundeach identi /uniFB01ed ear in the images Fig. ). The bounding boxes containall the pixels of the ears, except when the bounding box would have tobe made too large to include the awns. If possible, the boxes alsocontain small portion of the stem. When comparing the results of theidenti /uniFB01cation by one of the model developed, we discovered that fewears were forgotten by the operator in interactive label process. Theimages were thus reprocessed interactively with greater care. Finally, atotal of 30,729 ears were identi /uniFB01ed after the second ear label round.The second experiment (June the 16th) was only used to evaluatethe scalability of the models when applied to another stage with dif-ferent illumination conditions and camera focal length: no interactivelabelling was made for this experiment.2.1.5. Data preparationIt was not possible to train the model with the original 6000 4000pixels images because of GPU memory limitation. The maximum imagesize acceptable for the available computer con /uniFB01guration () wasFig. 1. Cumulated Dh for di /uniFB00erent zadocks development stages. Dh is the dif-ference between the actual and maximum evapostranspiration values.S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²500 500 pixels. The original images where therefore split into mul-tiple sub-images while keeping 50% overlap between the sub-images.The overlap allows to minimize problems observed on the borders whenears are only partially contained in the sub-image. Note that usingsmaller sub-images increases the size of the training dataset Poundet al., 2016 ). To investigate the in /uniFB02uence of the spatial resolution,several training datasets were generated by resampling the originalimages by factor of 2, 3, 4, and ( Table ) using bi-linear ag-gregation function. Note that the larger resampling factors corre-sponding to degraded spatial resolution will correspond to largerfootprints of the 500 500 pixels sub-images and therefore limitednumber of training data set. To investigate this trade-o /uniFB00, sub-image sizeof 250 250 were also considered, which increased by factor thetraining dataset at the expense of more border /uniFB00ects. Note that nospeci /uniFB01c data-augmentation was applied to the training data base. Re-garding the TasselNet method sub-image of size of 256 were used,these sub-images were further down sample by factor of 8.2.2. Data processing2.2.1. Object detection using Faster-RCNNObject detection techniques searches /uniFB01rst in the sub-image potentialcandidates. An object proposal method is thus required. number ofobject proposal methods have been reviewed and compared Hosanget al., 2015 for general object detection using the convolutional fea-tures of the full sub-image network. The Region Proposal Network(RPN) generates /uniFB01rst dense grid of anchor regions (candidatebounding boxes) with speci /uniFB01ed sizes and aspect ratios over the inputsub-image. An anchor is assigned as positive/negative if its intersectionover union (IoU) ratio with the ground truth object is greater/lowerthan relatively large/small overlap threshold. The RPN made of ashallow CNN predicts score for each anchor, which measures itsprobability to contain an ear. One of the advantages of this approach isthat the model learns features of the background, thus removing ne-gative location to the classi /uniFB01cation step.The TensorFlow implementation of Faster-RCNN by the object de-tection API Huang et al., 2016 was used. The RPN branch is insertedbetween the conv4 and conv5 blocks. The Inception-Resnet-V2 modelwas used here because it achieves the best accuracy among currentobject detectors Huang et al., 2016 ). An anchor is set at each locationconsidered by the convolution maps of the RPN layer. set of 12 an-chors with di /uniFB00erent sizes and aspect ratios were assigned at each lo-cation, following the default setting. Anchors were considered con-taining an ear if the IOU between their bounding boxes and those of thelabelled ears were between 0.6 and 1.0. Conversely, it was consideredas background if the IOU with label ears was lower than 0.175. Whenthe IOU was in between 0.175 and 0.6, the anchors were no moreconsidered. These hyper-parameters were corresponding to standardvalues. The number of proposed anchors per sub-image was /uniFB01xed to300 which is consistent with the maximum expected number of ears ina sub-image. The batch size was /uniFB01xed to because it saves the com-putation time and memory requirement while marginally impacting theperformances (results not shown for the sake of brevity). Each boundingbox was associated with score value. score threshold of 0.5 was usedto decide whether bounding box will be considered as an ear or not.To limit overlap between bounding boxes containing the same ear, anIOU threshold of 0.6 was used to select only one of the two boundingboxes Hosang et al., 2017 ). The model was pretrained on the COCOdataset The Open Images dataset, 2018 ). It contains 0.33 millionimages with 1.5 million of object instances belonging to 80 object ca-tegories. The model was /uniFB01nally /uniFB01ne trained with learning rate of0.0003 and momentum of 0.9.The results on the sub-images were then merged to count the earsover the full original image. Because of the 50% overlap between thesub-images an ear was generally detected in more than one sub-image.An overlap ratio was computed for each bounding box. It was computedasthe intersection area between the two bounding boxes divided by thearea of the smaller bounding box. If this ratio was larger than 0.85, thesmaller bounding box was deleted.2.2.2. Counting by regression using TasselNetTasselNet is recent regression-based counting approach. TasselNetlearns mapping from local visual characteristics to local image counts.The image is processed using sliding window. The global image countis computed by summing the counts over the set of local windows.Compared to Faster R-CNN, learning TasselNet only needs dotted an-notations (the center of each bounding box). Following the suggestionsfrom Lu et al., 2017 ), the Alex-like CNN model with local counts as theregression target based on L1 loss function were used here. Further,since TasselNet allows to work on relatively low-resolution images, theoriginal image was down sampled to 1/8 of its original size and32 32 pixels sub-images were considered, corresponding to256 256 pixels sub-images in the original spatial resolution Table ).We refer readers to Lu et al., 2017 for further details.2.2.3. Evaluation metricsThe training and validation datasets were populated with di /uniFB00erentgenotypes: 14 genotypes (168 images) out of the 20 were randomlyselected for training the models. The six remaining genotypes (72images) were used for the validation. This will allow identi /uniFB01cation ofpossible over /uniFB01tting in the training process.A predicted bounding box is considered correct (true positive, TP) ifit overlaps more than the IOU threshold with labelled bounding box.Fig. 2. Example of bounding boxes interactively drawn using the labelimgsoftware.Table 1Characteristics of the several models considered in this study.Model Approach ResolutionfactorResolution(mm)Size sub-image(pixels)Number of sub-images usedfor training/validation#1 Faster-RCNN 0.13 500 56994/24426#2 Faster-RCNN 0.26 500 12270/5258#3 Faster-RCNN 0.39 500 5782/2478#4 Faster-RCNN 0.52 250 12270/5258#5 Faster-RCNN 0.52 500 2478/1062#6 Faster-RCNN 0.78 250 5782/2478#7 Faster-RCNN 0.78 500 990/424#8 Faster-RCNN 1.04 250 2478/1062#9 TasselNet 1.04 256 SlidingwindowS. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²Otherwise the predicted bounding box is considered as false positive(FP). When the labelled bounding box have an IOU with predictedbounding box lower than the threshold value, it is considered as falsenegative (FN). The standard IOU threshold value of 0.5 was used. Theprecision and recall are then computed (Eq. (1)):=+=+PrecisionTPFP TPRecallTPFN TP,(1)The score associated to each bounding box allows evaluating thetrade-o /uniFB00between false positive and false negative. The average preci-sion (AP@0.5IOU) Salton and McGill, 1983 was used to quantify thedetection performances. The standard average precision metrics, AP@0.5IOU, is the area under the precision-recall curve obtained for dif-ferent bounding box scores. The AP@0.5IOU balances the precision andrecall performances terms that may be strongly correlated. AP@0.5IOUvaries between (TP 0) to (FN 0)The ear counting performances were quanti /uniFB01ed using several me-trics: root mean squared error (RMSE), the relative RMSE (rRMSE), themean absolute error (MAE), the Bias (BIAS) and the coe /uniFB03cient of de-termination (R/uni00B2):/uni2211 =/uni2212=RMSENtc1()knkk12(2)/uni2211/uni239C/uni239F /uni239B/uni239D/uni2212/uni239E/uni23A0=rRMSENtct1knkkk12(3)/uni2211=/uni2212=BIASNtc1()knkk1 (4)/uni2211=/uni2212=MAENtc1||knkk1 (5)/uni2211=/uni2212/uni2211/uni2212/uni2212==Rtctt²1() ²() ²knkkknkk11¯(6)Where denotes the number of test images,tkandckare respectivelythe reference and estimated counts for image k, and tk¯is the meanreference count.3. Results and discussion3.1. resolution around 0.3 mm is needed for best performance with faster-rcnnThe time required to train each model was around h (4000iterations run on NVIDIA GTX 1080Ti). The model performance (AP@0.5IOU computed on the validation dataset) was computed at severalstages during the training process for the several combinations of spa-tial resolution and sub-window size Table ). This allows for evalua-tion of the quality of the training process. Results showed that AP@0.5IOU was generally converging rapidly towards maximum value(Fig. ): after 1000 iterations, most of the models reached an AP@0.5IOU close to the maximum one. This is explained by the fact that themodel was pretrained and initialized using the COCO dataset. Noover/uniFB01tting characterized by decrease of the AP@0.5IOU afterreaching maximum value was observed Fig. ), except for the model#8 that was trained on to small number of sub-images. The maximumvalues of AP@0.5IOU found here Table ) were higher than thosereported in other studies based also on the COCO dataset Huang et al.,2016 ). This improvement in the performances observed in our studywas mostly explained by the larger size of the training dataset used andthe relatively lower complexity of the ear detection problem as com-pared to the categories considered in Huang et al., 2016 ).When using the original spatial resolution of the images (model #1),borders with ears partially in the sub-image were often observed(Fig. ). This may explain why the performances degraded as comparedto slightly coarser resolution Fig. ,Table ). Note that the sub-imagesize was limited by the GPU memory. Further, the object size Table )observed for model #1 with the highest spatial resolution is muchlarger than the typical size of objects considered in standard convolu-tional networks (in between 100 and 250 pixels) Huang et al., 2016 ).This may pose di /uniFB03culties to handle these large objects for the /uniFB01rst stepof the algorithm where regions are proposed (RPN).The spatial resolution showed marginal impact on the AP@0.5IOUvalues that kept around AP@0.5IOU 0.9 for most models havingmore than 3000 sub-images used for the training, with the exception ofthe original resolution (model #1) wich showed strong border /uniFB00ectsand too large bounding boxes as already discussed Table ). The AP@0.5IOU was therefore mostly in /uniFB02uenced by the number of sub-imagesused in the training process. For the same spatial resolution (models #4and #5 with 0.52 mm resolution, and models #6 and #7 with 0.78 mmresolution) the AP@0.5IOU was always higher when the number ofTable 2Performances of the Faster-RCNN models considered evaluated over the 72 validation images.Dataset Resolution (mm) Average object size (pixels) Size of sub-image (pixels) Number of sub-images AP@0.5IOU after 4000 iterations ear count per imageR/uni00B2 rRMSE#1 0.13 221.6 500 56994 0.70 0.73 11%#2 0.26 110.8 500 12270 0.85 0.91 5.3%#3 0.39 73.9 500 5782 0.83 0.85 5.4%#4 0.52 55.4 250 12270 0.83 0.83 11.2%#5 0.52 55.4 500 2478 0.67 0.87 24.7%#6 0.78 36.9 250 5782 0.82 0.75 11.2%#7 0.78 36.9 500 990 0.54 0.33 38.5%#8 1.04 27.7 250 2478 0.49 0.62 30.3%Fig. 3. AP@0.5IOU as function of the number of iterations during the trainingprocess. The several curves correspond to the models presented in Table .S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²images used for training was larger Table ). Even with spatial re-solution degraded down to 0.78 mm (model #6), AP@0.5IOU wereonly marginally decreasing as compared to 0.39 mm resolution (model#3) when the training data set is large enough (5782 sub-images). Forthe 0.78 mm resolution, the average size of the ear bounding box is 37pixels, which is consistent with other studies A closer look, 2018 andwith the size considered for the proposed objects. For coarser spatialresolution, the number of available sub-images used for the trainingwill be too small to provide robust performances. Further, even by in-creasing the size of the training dataset with additional labelled images,performances are expected to decrease because Faster-RCNN is knownto have di /uniFB03culties with small objects A closer look, 2018 ).For more detailed evaluation of the performances of the severalmodels presented in Table , the R/uni00B2 and rRMSE of the ear countingestimation for each sub-image were also computed. Those two metricswere generally in agreement with the AP@0.5IOU Table ). However,relatively high rRMSE and small R/uni00B2 were observed for models #4 and#6. Visual inspection of the resulting estimated bounding boxes showedthat too many boxes were assigned for the same ear which is notproperly considered by the AP@0.5IOU metrics. This problem corre-sponds to poorer /uniFB03ciency of the RPN step when the size of the sub-images (250 250) was too small. It was thus recommended to usesub-image size larger than 250 250 pixels. However, it is possiblethat manipulating concurrently other hyper-parameters such as thenumber of maximum proposed bounding boxes will partly solve thislimitation.Best performances were observed for dataset #2 with sub-imagesize of 500 500 pixels and spatial resolution around 0.26 mm(Fig. ). This dataset was used in the validation part of this project(Fig. )A total of 8097 ears were detected with model #2 applied to thevalidation dataset, with 1.5% false positive (commission) and 2.9%false negative (omission). Closer inspection of the false positive casesshowed that signi /uniFB01cant part (around 40%) corresponded to actualears that were not identi /uniFB01ed during the interactive labelling process.The Faster-RCNN model achieved thus better ear detection than hu-mans when properly trained. However, the model failed to detect mostof occluded ears with poor lighting conditions that were also largelymissed by the human labelling: the model was obviously not trained forthese situations. The false positive cases were also generally associatedwith lower con /uniFB01dence score Fig. ). The in /uniFB02uence of this score willbe further discussed in later in this paper.When part of stem was visible, model #2 was more easily de-tecting the ears: the stem carried therefore useful information for earrecognition. However, this situation is not the dominant one for most ofthe genotypes at the early stages since ears are mostly vertical andobserved from nadir Fig. ). The model also failed to detect very largeears Fig. , left). Lower AP@0.5IOU was computed when the modelwas facing ears with no awn. Further, the model had more di /uniFB03cultieswith bounding boxes that had an aspect ratio di /uniFB00erent from one. TheIOU ratio with the reference labelled boxes was generally smaller forears with no awn or for bounding boxes with aspect ratio di /uniFB00erent fromone. This later problem can be improved by adding anchors with alarger range of size and aspect ratio.3.2. Faster-RCNN is more robust than TasselNetThe ear counting based on TasselNet required very few hyper-parameters. The standard values proposed by Lu et al., 2017 wereused here. The comparison between TasselNet and Faster-RCNN isbased on the ear density estimated from the images belonging to thevalidation dataset. In fact, TasselNet did not identify and locate the earsand so it is not possible to compute confusion matrix from which theAP@0.5IOU could be derived. Further, the lack of localization step inTasselNet prevents the opportunity for exploration of other potentialtraits exploration at the ear level such as detecting the presence ofawns, measuring the size and shapes of the ears and quantifying the/uniFB02owering status. Nervertheless, TasselNet presented the advantage toidentify each ear using single point as compared to the more complexlabel using bounding boxes as in the Faster-RCNN approach.Results showed good performances for both methods as evaluatedover the validation dataset, with very small bias (< ears) and betterrRMSE for Faster-RCNN ≈5%) Fig. ). This result was expected incases of non-crowded scenes with little overlap between objects, whichwas the case for ears in this study Fig. ): less than 1% of the inter-actively label bounding boxes had an IOU 0.5. TasselNet was moree/uniFB03cient for relatively low spatial resolution images to evaluate thedensity of small object instances (< 30 pixels). TasselNet seemedtherefore not exploiting all the detailed texture information required forFaster-RCNN to identify individual ears: the degraded spatial resolution(1.04 mm) of the images used for TasselNet provided better perfor-mances as compared to Faster-RCNN applied to the same spatialFig. 4. Example of the sub-windows used for model-#1 (left: original resolution (0.13 mm) and subimage size 500 500), model #2 (middle: resolution degradedby factor of (0.26 mm) and subimage size 500 500), and model #5 (right: resolution degraded by factor of (0.52 mm) and subimage size 500 500).Fig. 5. Distribution of the associated scores for the true positive (blue) and thefalse positive (red) bounding boxes. (For interpretation of the references tocolour in this /uniFB01gure legend, the reader is referred to the web version of thisarticle.)S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²resolution (model #8 in Table ).Since the number of ears is not expected to change after the /uniFB02ow-ering stage, estimations for the /uniFB01rst date were compared with those ofthe second date for which no ears were labelled. This allowed evalua-tion of the scalability for the second date of the models calibrated onthe/uniFB01rst date. Because the images taken over each microplot were notlocated exactly at the same place between the two dates, performanceswere based on the average ear density of the microplot. It was com-puted from the ear count of the two images taken over each microplot,divided by their footprint area de /uniFB01ned by plant height and camera /uniFB01eldof view. Results showed that the application to the second date of theFaster-RCNN previously trained over the /uniFB01rst date agreed very wellwith ear density estimates for the /uniFB01rst date Fig. ) with however aslight underestimation of the ear density that will be further in-vestigated in the next section.Conversely, TasselNet showed large discrepancies between the eardensity estimation of the two dates with strong underestimation(Fig. ). This appeared mostly related to the senescence state of themicroplot, TasselNet having di /uniFB03culties to detect senescent ears oversenescent leaves Fig. 10 ). Further, ears from the second date presenteddi/uniFB00erent visual aspects as compared to the /uniFB01rst date, with in addition aslight change in the spatial resolution due to the use of 60 mm and50 mm focal lengths camera respectively for the /uniFB01rst and second dates(Fig. 10 ). TasselNet failed to capture ears from the second date and thusgeneralized poorly in new scenes. The better scalability of the Faster-RCNN model may be due the fact that it was already pretrained todetect millions of object instances, exploiting more the gray-scale imagepattern than the TasselNet model that seemed to be much more sensi-tive to the color of the objects. More investigations should be carriedout to improve TasselNet scalability based on larger training datasetwith signi /uniFB01cant fraction of images over senescent crops, or to simplytransform the RGB images into gray-scale images.Because of the limits of the TasselNet model highlighted previously,focus will be put on the Faster-RCNN model#2 trained over the /uniFB01rstdate. The slight degradation of performances of the Faster-RCNN modelfor the second date was further investigated. The RMSE between the eardensity estimated from the RGB images and the ground measurementswere computed for range of score threshold values used to decide if abounding box is considered containing an ear or not Fig. 11 ). Resultsshowed that the RMSE decreases with the score threshold values downFig. 6. Examples of sub-images (500 500 pixels) with ears detected by model #2.Fig. 7. Illustration of problems encountered with model #2. On the left, example of False negative (omission). On the right, example of false positive (commission).All the images have the same resolution.S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²to minimum around 0.7 for the /uniFB01rst date: increasing the scorethreshold value limits the fraction of false positives. After this minimumvalue, the RMSE increased with the score since the fraction of falsenegative will increase. The situation is slightly di /uniFB00erent for the seconddate: the RMSE increased continuously with the score threshold value.Ears for the second date were slightly di /uniFB00erent from the /uniFB01rst date usedto train the model. The score threshold should therefore be relaxed toprevent rejecting too many candidate ears that were slightly di /uniFB00erentfrom the /uniFB01rst date. The score threshold value initially used (0.5) ap-peared thus to be optimal when considering the two measurementdates: both curves were crossing for score threshold value of 0.5(Fig. 11 ).3.3. Ear density estimates are highly heritableThe broad sense heritability (H/uni00B2) quantifying the repeatability of theear density estimation was computed as the ratio between the genotypicvariances to the total one Holland et al., 2002 ). linear mixed-e /uniFB00ectsstatistical model was applied on each date to quantify the geneticvariance. The ‘lm4′R package was applied to our alpha plan experi-mental design Bates et al., 2014 ). The soil water holding capacity (S)that was carefully documented was used as /uniFB01xed /uniFB00ect in the modelthat writes (random terms are underlined) as:=++ ++ +YµSGLCL C/uni0190 _ :_ (7)WhereYis the ear density, G_is the random /uniFB00ect of the genotypes,L_andC_are respectively, the random /uniFB00ects for lines and column in thealpha plan, and LC:_ is the random sub-block /uniFB00ect.µis the /uniFB01xed in-tercept term andεthe random residual error. Since the genotypes mayexpress di /uniFB00erently depending on the environmental conditions, theheritability was computed independently for the two modalities.The high heritability values ≈85%) for the estimated ear densitywas observed for both measurement dates and the two modalities(Table ). This was partly explained by the fact that contrasted geno-types were used with signi /uniFB01cant di /uniFB00erences in the tillering capacity.The heritability is higher for the /uniFB01rst date as compared to the secondone. This can be attributed by the fact that the models were trainedwith images from the /uniFB01rst date. The heritability of the WS modality isslightly higher than that observed on the WW modality. The heritabilityassociated to the ground measurements of the ear density were in thesame order of magnitude as those estimated from the RGB imagery andthe faster-RCNN model. The heritability of the WW modality(H/uni00B2 80%) is lower than that of the WS modality (H/uni00B2 91%) inagreement with RGB imagery estimates. However, the heritability ofthe WW modality is lower than that provided by the Faster-RCNNmodel. This point will be investigated in the next section.3.4. Faster-RCNN was more reliable than ground measured ear densityThe ear density of the WS and WW modalities were expected to bevery similar since the water stress was mostly appearing after the earemergence stage Fig. ) when all ears have already emerged from thestems. The estimated ear density averaged over the three replicateswere thus compared between the two modalities. The same was donefor the ground measured ear densities. The best coe /uniFB03cient of de-termination (R/uni00B2) was observed with the images from the /uniFB01rst date andthe output from Faster –RCNN model #2 Fig. 12 ): the ear densitybetween the two modalities were very similar as expected with almostno bias (Bias 0.6 ears/m/uni00B2). The same was observed over the seconddate of RGB image acquisition (R/uni00B2 0.78; Bias 20.6 ears/m/uni00B2). Con-versely, ground measured ear densities were higher in the WS modalityas compared to those of the WW modality. This was not expected andshould result from larger uncertainties in the ground measurements.This may also explain the low heritability of ground measured eardensity found for the WW modality Table .The ear density estimated with Faster-RCNN was /uniFB01nally comparedwith the ground measurements. The ear density estimated with model#2 was in relatively good agreement with the ground measurements forthe WS modality and the /uniFB01rst date (June 2nd) of RGB images acquisi-tion. Table and Fig. 13 ). The scatter of points observed might bepartly attributed to the relatively small sampling size used for theground observations (1.05 m/uni00B2) and for the RGB images (about 0.6 m/uni00B2for the /uniFB01rst date and 1.0 m/uni00B2 for the second date). The spatial re-presentativeness was therefore limited to get an accurate comparisonbetween the two types of ear density that were not measured at thesame place over each microplot. Increasing the number of RGB imagestaken on each plot would improve this aspect which should not be amajor issue considering the high-throughput associated both to theimage acquisition and data processing. The ear density over the WWFig. 8. Comparison between the number of ears in each image visually labellingand that estimated using either the Faster-RCNN (model #2 black dots) or theTasselnet (model #9 red dots). (For interpretation of the references to colour inthis/uniFB01gure legend, the reader is referred to the web version of this article.)Fig. 9. Comparison between the average ear density estimated for the /uniFB01rst(June 2nd) and the second (June 16th) dates using Faster-RCNN (blue dots) andTasselNet (red dots) models trained over the /uniFB01rst date only. (For interpretationof the references to colour in this /uniFB01gure legend, the reader is referred to the webversion of this article.)S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²modality showed signi /uniFB01cant degradation of the agreement betweenestimated and measured values. systematic under estimation of theear density from the ground measurement in the WW modality wasobserved, probably due to the uncertainties attached to the groundmeasurements already outlined.4. ConclusionsThe main objective of this study was to evaluate the /uniFB03ciency ofdeep learning approaches to estimate the ear density in wheat pheno-typing trials using high resolution RGB images acquired at nadir.Considering the challenge of managing the large diversity in the ear andbackground aspects due to genotypic speci /uniFB01cities as well as dates ofobservations, our results were promising (rRMSE 5.3%) for theFig. 10. Example of two images selected fromthe same microplot on the /uniFB01rst and second dateover which Faster-RCNN and TasselNet modelswere applied. The models were trained on the/uniFB01rst date and the genotype was in the valida-tion dataset. The top images corresponded tomicroplot showing little senescence, while thebottom images corresponded to microplotshowing almost complete senescence. From leftto right: original RGB image, detection byFaster-RCNN model #2, density maps esti-mated using the TasselNet model.Fig. 11. RMSE of the estimated ear density as function of the score thresholdvalue.Table 3Broad sense heritability (H/uni00B2) computed for the ear density for the groundmeasurements and the estimates for the two dates considered. The H/uni00B2 values arecomputed for the two modalities.H/uni00B2 (%) Date WW WS ALLGround Measurements June 7th79.8 91.4 66.3Estimates from /uniFB01rst date June 2nd86.9 88.5 86.5Estimates from second date June 16th82.2 82.8 76.3Fig. 12. Comparison between ear density estimated in the WW modality and inthe WS modality (The size of the circles represented the variability of themeasurements for the di /uniFB00erent replicates).Table 4Performances of ear density estimated from the Faster-RCNN trained over the/uniFB01rst date (June 2nd) and applied to the two dates for both WW (irrigated) andWS (water stress) modalities.June 2ndJune 16thWW WS All WW WS AllRMSE (ear/m/uni00B2) 82.0 53.0 68.7 62.1 77.4 69.4rRMSE (%) 16.4 12.1 14.4 15.1 18.9 16.9Bias (ear/m/uni00B2) 55.2 2.73 25.8 24.62 52.49 10.05R/uni00B2 0.52 0.70 0.53 0.46 0.62 0.37S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²Faster-RCNN model. The Faster-RCNN model were much better thanthe/uniFB01rst round of interactive labeling: many ears were missed by theoperator when label the ears on the screen. This forced us to improvethe interactive label by reanalyzing the training and validation datasets.Faster-RCNN model was demonstrated to be more robust than countingby regression using the TasselNet model. Counting by regression such aswith the TasselNet model should be more /uniFB03cient for crowded sceneswith objects that overlap frequently, which was not the case for ears asobserved from nadir. Faster-RCNN bene /uniFB01ciated from the transferlearning approach based on model pretrained on the COCO dataset.However, the gain in robustness for the Faster-RCNN model comes atthe expense of larger computation requirements using GPU resources.Nevertheless, models based on object detection present the advantageof providing the basis for estimating additional traits for the ears in-cluding spatial distribution between rows, presence of awns, size, in-clination or color that could be useful for breeders.Results showed that the broad sense heritability of ear density es-timates from RGB images was high and close to that computed from thedirect in situ measurements. However, the ear density computed fromthe RGB images based on Faster-RCNN model showed only fairagreement with the ear density measured in the /uniFB01eld (rRMSE ≈15%),particularly for the WW modality that was suspected to had largeruncertainties attached to the ground counting. An improved matchbetween the RGB estimates and direct counting in the /uniFB01eld is expectedmostly either (i) by working on the same samples which is notstraightforward to achieve, or better by (ii) increasing the sampling sizeof both the ground sampling as well as the sampling area covered by theimages. This is easy to achieve with the RGB imagery by capturing moreimages over each microplot, while it is expensive for the in-situ mea-surements.Faster-RCNN model was demonstrated to have optimal perfor-mances for images with spatial resolution between 0.26 and 0.39 mm.Higher spatial resolution corresponded to too large bounding boxes aswell as increased border /uniFB00ects di /uniFB03cult to manage. For broader re-solution, the loss of textural information degraded the identi /uniFB01cationperformances. Therefore, an optimal resolution around 0.3 mm wouldallow to use UAV observations for covering large phenotyping experi-ments as already demonstrated by Jin et al., 2017 and get very high-throughput method. Further, UAV observations cover the whole mi-croplot, allowing large sampling area, thus increasing the precisionand heritability. Further, uncertainties attached to the knowledge of thearea used to compute the density will be negligible. This was not thecase in this study where the relatively small footprint of the imagesforced to get accurate estimation of the distance between the cameraand the ear layer, with possible representativeness of the row /uniFB00ectsince the width of the images was not necessarily multiple of thedistances between rows.The size and the diversity of the training dataset is critical to obtaingood estimation performances. Faster-RCNN model trained over onedate was demonstrated to apply well on another date with only smalldegradation of performances even if the ears and the background as-pects were quite di /uniFB00erent. However, improved performances are ex-pected by applying data augmentation to arti /uniFB01cially increase the size ofthe training dataset that was demonstrated to be critical, as well as thediversity of measurement conditions including orientation, adjustmentof the white balance, of the spatial resolution and of the sharpness. Theconcept of domain adaptation should also help to tackle the domain anddataset discrepancy problem Chen et al., 2018 ). Nevertheless, perfor-mance can be further improved with availability of large datasets ofcarefully labelled images. For this reason, we /uniFB00er to the communitythe labelled dataset used in this study that is freely accessible at:https://github.com/simonMadec where 30,729 ears were identi /uniFB01ed in240 images over 20 contrasting genotypes grown in two environmentalconditions.FundingThis study was supported by “Programme ’investissementd’Avenir ”PHENOME (ANR-11-INBS-012) and Breedwheat (ANR-10-BTR-03) with participation of France Agrimer and “Fonds de Soutien àl’Obtention Végétale ”. The work was completed within the UMT-CAPTEfunded by the French ministry of Agriculture.AcknowledgementWe thank very much Olivier Moulin, Guillaume Meloux and MagaliCamous from the Arvalis experimental station in Gréoux for their kindsupport during the measurements.ReferencesA closer look, 2018. closer look. Small Object Detection in Faster R-CNN IEEEConference Publication [Internet]. [cited 2018 Jan 11]. Available from: http://ieeexplore.ieee.org/abstract/document/8019550/ .Allen, R.G., Clemmens, A.J., Burt, C.M., Solomon, K., ’Halloran, T., 2005. Predictionaccuracy for projectwide evapotranspiration using crop coe /uniFB03cients and referenceevapotranspiration. J. Irrig. Drain. Eng. 131, 24 –36.Fig. 13. Comparison between ear density estimated from the RGB images and the ground-measurements for the two modalities (red WS; blue WW). (Forinterpretation of the references to colour in this /uniFB01gure legend, the reader is referred to the web version of this article.)S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²Araus, J.L., Cairns, J.E., 2014. Field high-throughput phenotyping: the new crop breedingfrontier. Trends Plant. Sci. 19, 52 –61.Bates D., Mächler M., Bolker B., Walker S. Fitting linear mixed-e /uniFB00ects models using lme4.ArXiv Prepr ArXiv14065823 [Internet]. 2014; Available from: https://arxiv.org/abs/1406.5823 .Chen Y., Li W., Sakaridis C., Dai D., Van Gool L. Domain adaptive faster r-cnn for objectdetection in the wild. ArXiv Prepr ArXiv180303243. 2018.darrenl. labelImg: :metal: labelImg is graphical image annotation tool and label objectbounding boxes in images [Internet]. 2017. Available from: https://github.com/tzutalin/labelImg .Dauphin, G.M.Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., et al., 2012.Unsupervised and transfer learning challenge: deep learning approach. PMLR[Internet] 97 –110. [cited 2017 May 30] Available from: http://proceedings.mlr.press/v27/mesnil12a.html .Deery, D., Jimenez-Berni, J., Jones, H., Sirault, X., Furbank, R., 2014. Proximal remotesensing buggies and potential applications for Field-based phenotyping. Agronomy 4,349–379.Donahue, J., Jia, Y., Vinyals, O., Ho /uniFB00man, J., Zhang, N., Tzeng, E., et al., 2014. DeCAF: Adeep convolutional activation feature for generic visual recognition. Icml [Internet]647–655. Available from: http://www.jmlr.org/proceedings/papers/v32/donahue14.pdf .Fernandez-Gallego, J.A., Kefauver, S.C., Gutiérrez, N.A., Nieto-Taladriz, M.T., Araus, J.L.,2018. Wheat ear counting in- /uniFB01eld conditions: high throughput and low-cost approachusing RGB images. Plant. Methods. 14, 22 .Hinton, G.E., Salakhutdinov, R.R., 2006. Reducing the dimensionality of data with neuralnetworks. Science. 313, 504 –507.Holland, J.B., Nyquist, W.E., Cervantes-Martínez, C.T., 2002. Estimating and interpretingheritability for plant breeding: an update. In: Janick, J. (Ed.), Plant Breed Rev[Internet]. John Wiley Sons, Inc, pp. –112. Available from: http://onlinelibrary.wiley.com/doi/10.1002/9780470650202.ch2/summary .Hosang, J., Benenson, R., Dollár, P., Schiele, B., 2015. What makes for /uniFB00ective detectionproposals? IEEE Trans Pattern Anal. Mach. Intell. PAMI 2015 [Internet] Availablefrom: http://ieeexplore.ieee.org/abstract/document/7182356/ .Hosang J., Benenson R., Schiele B. Learning non-maximum suppression. ArXiv PreprArXiv170502950 [Internet]. 2017; Available from: https://arxiv.org/abs/1705.02950 .Huang J., Rathod V., Sun C., Zhu M., Korattikara A., Fathi A., et al. Speed/accuracy trade-o/uniFB00s for modern convolutional object detectors. ArXiv161110012 Cs [Internet]. 2016;Available from: http://arxiv.org/abs/1611.10012https://github.com/tensor /uniFB02ow/models/tree/master/research/object_detection .Jin, X., Liu, S., Baret, F., Hemerlé, M., Comar, A., 2017. Estimates of plant density ofwheat crops at emergence from very low altitude UAV imagery. Remote. Sens.Environ. 198, 105 –114.Journaux, L., Marin, A., Cointault, F., Miteran, J., 2010. Fourier Filtering for WheatDetection in Context of Yield Prediction. [cited 2017 May 29]. Available from:.CIGR. http://www.csbe-scgab.ca/docs/meetings/2010/CSBE101090.pdf .Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classi /uniFB01cation with deep con-volutional neural networks. Adv. Neural Inf. Process. Syst. [Internet] 1097 –1105.Available from: http://papers.nips.cc/paper/4824-imagenet-classi /uniFB01cation-with-deep-convolutional-neural-networks .LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature. 521, 436 –444.Li, L., Zhang, Q., Huang, D., 2014. review of imaging techniques for plant phenotyping.Sensors 14, 20078 –20111 .Lu, H., Cao, Z., Xiao, Y., Zhuang, B., Shen, C., 2017. TasselNet: counting maize tassels inthe wild via local counts regression network. Plant. Methods. 13, 79 .Madec, S., Baret, F., de Solan, B., Thomas, S., Dutartre, D., Jezequel, S., et al., 2017. High-throughput phenotyping of plant height: comparing unmanned aerial vehicles andGround LiDAR estimates. Front. Plant. Sci. [Internet] [cited 2018 Jan 22]. Availablefrom: https://www.frontiersin.org/articles/10.3389/fpls.2017.02002/full .Mohanty, S.P., Hughes, D.P., Salathé, M., 2016. Using deep learning for image-basedplant disease detection. Front. Plant. Sci. [Internet] Available from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5032846/ .Pound, M.P., Burgess, A.J., Wilson, M.H., Atkinson, J.A., Gri /uniFB03ths, M., Jackson, A.S.,et al., 2016. Deep machine learning provides state-of-the-art performance in image-based plant phenotyping. bioRxiv., 053033 .Pound, M.P., Atkinson, J.A., Wells, D.M., Pridmore, T.P., French, A.P., 2017. Deeplearning for multi-task plant phenotyping. bioRxiv. 204552 .Salton, G., McGill, M.J., 1983. Introduction to Modern Information Retrieval [Internet].Available from:. McGraw-Hill, New York. https://trove.nla.gov.au/work/19430022 .Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson, S., 2014. CNN features /uniFB00-the-shelf: an astounding baseline for recognition. Proc. IEEE Conf. Comput. Vis. PatternRecognit. Workshop [Internet] 806 –813. Available from: http://www.cv-foundation.org/openaccess/content_cvpr_workshops_2014/W15/html/Razavian_CNN_Features_O/uniFB00-the-Shelf_2014_CVPR_paper.html .Simonyan K., Zisserman A. Very Deep Convolutional Networks for Large-Scale ImageRecognition. ArXiv14091556 Cs [Internet]. 2014; Available from: http://arxiv.org/abs/1409.1556 .Singh, A., Ganapathysubramanian, B., Singh, A.K., Sarkar, S., 2016. Machine learning forhigh-throughput stress phenotyping in plants. Trends Plant. Sci. 21, 110 –124.Szegedy C., Io /uniFB00e S., Vanhoucke V., Alemi A. Inception-v4, Inception-ResNet and theImpact of Residual Connections on Learning. ArXiv160207261 Cs [Internet]. 2016;Available from: http://arxiv.org/abs/1602.07261 .Tardieu, F., Le Gouis, J., Lucas, P., Baret, F., Neveu, P., Pommier, C., et al., 2015.PHENOME: French plant phenomic center. EPPN Plant. Phenotyping Symp. np .The Open Images dataset, 2018. The Open Images Dataset [Internet]. Openimages.Available from:. https://github.com/openimages/dataset .Yosinski, J., Clune, J., Bengio, Y., Lipson, H., 2014. How transferable are features in deepneural networks? Adv. Neural Inf. Process. Syst. [Internet] 3320 –3328. Availablefrom: http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks .S. Madec et al. $JULFXOWXUDODQG)RUHVW0HWHRURORJ\²