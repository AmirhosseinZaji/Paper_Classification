Contents lists available at ScienceDirectEuropean Journal of Agronomyjournal homepage: www.elsevier.com/locate/ejaEstimating above ground biomass of winter wheat at early growth stagesusing digital images and deep convolutional neural networkJuncheng Maa, Yunxia Lib, Yunqiang Chenb, Keming Dua,/uni204E, Feixiang Zhenga, Lingxian Zhangb,Zhongfu SunaaInstitute of Environment and Sustainable Development in Agriculture, Chinese Academy of Agricultural Sciences, Beijing, 100081, ChinabCollege of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, ChinaARTICLE INFOKeywords:Winter wheatAbove ground biomassRGB imagesDeep convolutional neural networkABSTRACTAbove ground biomass (AGB) is critical trait indicating the growth of winter wheat. Currently, non-destructivemethods for measuring AGB heavily depend on tools such as Remote Sensing and LiDAR, which is subject tospecialized knowledge and high-cost. Low-cost solutions appear therefore to be necessary supplement. In thisstudy, an easy-to-use AGB estimation method for winter wheat at early growth stages was proposed by usingdigital images captured under /uniFB01eld conditions and Deep Convolutional Neural Network (DCNN). Using canopyimages as input, the DCNN was trained to learn the relationship between the canopy and the corresponding AGB.To compare the results of the DCNN, conventionally adopted methods for estimating AGB in conjunction withsome color and texture feature extraction techniques were used. Results showed strong correlations could beobserved between the actual measurements of AGB to those estimated by the DCNN, with high coe /uniFB03cient ofdetermination (R2= 0.808) and low Root-Mean-Square-Error (RMSE 0.8913 kg/plot, NRMSE 24.95%).Factors may in /uniFB02uence the accuracy of the DCNN were evaluated. Results showed selecting suitable values ofthese factors for the DCNN was the guarantee to accurate estimation results. Plant density was proved to be anin/uniFB02uence of factor to all the estimation methods based on digital images. The performances of all the methodswere in /uniFB02uenced to varying degrees while the DCNN achieved the best robustness, indicating the DCNN with RGBimages could be an /uniFB03cient and robust tool for estimating AGB of winter wheat at early growth stages.1. IntroductionAbove ground biomass (AGB) is one of the most commonly usedtraits indicating the growth of winter wheat Eitel et al., 2014 ;Schirrmann et al., 2016b ;Walter et al., 2018 ;Zhang et al., 2018 ). It is ofgreat practical signi /uniFB01cance to monitor the growth and to estimate theyield. The conventional methods involving destructive sampling andmanual calculation of the dry weight to measure AGB is prohibitivelytime consuming and laborious Casadesús and Villegas, 2014 ;Walteret al., 2018 ;Zhang et al., 2018 ). Due to the limitations, the conven-tional methods can only process limited amount of data, making itimpossible to be used in high-throughput plant phenotyping tasks.With the development of /uniFB01eld-based plant phenotyping, many non-destructive methods for measuring AGB were proposed. common wayto estimate AGB of winter wheat was through the estimation of vege-tation indices (VIs) and regression analysis. VIs can be calculated fromlight re /uniFB02ected at di /uniFB00erent wavelengths from images captured byRemote Sensing (RS) or ground-based image sensors Pölönen et al.,2013 ;Jannoura et al., 2015 ;Liebisch et al., 2015 ;Rasmussen et al.,2016 ;Zhang et al., 2018 ). The normalized di /uniFB00erence vegetation index(NDVI) was the most widely used VIs on estimating AGB, which wasde/uniFB01ned as the ratio of the re /uniFB02ectance in the near-infrared and redchannel of the spectrum Tucker, 1979 ). Strong correlations betweenNDVI and ground measured biomass of crop can be observed Pölönenet al., 2013 ;Rasmussen et al., 2016 ;Schirrmann et al., 2016b ). Exceptfor NDVI, some other VIs were also successfully adopted to estimateAGB, such as Modi /uniFB01ed Chlorophyll Absorption in Re /uniFB02ectance Index(MCARI) Daughtry et al., 2000 ;Pölönen et al., 2013 ), the enhancedNDVI index (ENDVI) Rasmussen et al., 2016 ), and GnyLi Gnyp et al.,2014 ). With the increased use of low-cost Unmanned Aerial Vehicles(UAVs), VIs derived from low-cost UAV imagery were able to achievegood estimation of AGB, such as Normalized Green-Red Di /uniFB00erenceIndex (NGRDI) Jannoura et al., 2015 ;Rasmussen et al., 2016 ), ExcessGreen (ExG) Rasmussen et al., 2016 ;Schirrmann et al., 2016a ;https://doi.org/10.1016/j.eja.2018.12.004Received 12 June 2018; Received in revised form December 2018; Accepted December 2018/uni204ECorresponding author at: Institute of Environment and Sustainable Development in Agriculture, Chinese Academy of Agricultural Sciences, 12, ZhongguancunSouth Street, Haidian District, Beijing, 100081, China.E-mail address: dukeming@caas.cn (K. Du).(XURSHDQ-RXUQDORI$JURQRP\²(OVHYLHU%9$OOULJKWVUHVHUYHG7Woebbecke et al., 1995 ), RGB vegetation index (RGBVI) Possoch et al.,2016 ). While the VIs showed excellent ability to estimate AGB, therewere still limitations that the VIs can be /uniFB00ected by saturation of theindex Eitel et al., 2014 ;Tucker, 1977 ;Walter et al., 2018 ). Canopyheight is another widely used parameter to estimate AGB. Schirrmannet al. (2016) showed that the inclusion of plant height into the model,established to estimate AGB by plant coverage, could /uniFB03ciently in-crease the estimation accuracy. They also revealed plant height alonewas strong predictor of AGB. The same conclusions can be observed inBendig et al. (2015) ;Pittman et al. (2015) and Walter et al. (2018) .Many methods have been used to obtain plant height data, such as LightDetection And Ranging (LiDAR) Eitel et al., 2014 ;Pittman et al.,2015 ), ultrasonic sensor Chang et al., 2017 ;Moeckel et al., 2017 ;Pittman et al., 2015 ), UAVs Bendig et al., 2015 ). However, thesesensors were costly and required specialized knowledge, leading to low/uniFB02exibility.Ground-based images captured by low-cost devices can be an al-ternative to the expensive sensors and have received great interest.Many researchers have used ground-based images to estimate growth-related traits, such as Leaf Area Index (LAI) Casadesús and Villegas,2014 ;Fan et al., 2017 ), AGB Casadesús and Villegas, 2014 ;Chunget al., 2017 ;Walter et al., 2018 ), plant height Jiang et al., 2016 ;Walteret al., 2018 ). While ground-based images is promising way to estimategrowth-related traits, more feasible and robust approach should beexplored.The objective of this study was to apply the Deep ConvolutionalNeural Network (DCNN) to the estimation of AGB for winter wheat atearly growth stages using ground-based digital images. The DCNN wasused to model the relationship between an RGB image of winter wheatcanopy in speci /uniFB01c area and the corresponding AGB. By following theproposed framework including canopy images processing, image aug-mentation, construction of the DCNN, this paper investigated the po-tential of the DCNN using ground-based digital images in estimatingAGB of winter wheat at early growth stages.2. Materials and methods2.1. Experiment setupThe experiment took place in the /uniFB01eld station of Shangqiu Academyof Agriculture and Forestry Sciences in Shuangba, Henan, China (Lat:34°32 ′21.1884 ″: N, Long: 115°43'8.0868": E), which was located at thecenter of winter wheat producing areas of Huang-Huai Plain of China.The yearly average precipitation in this region was approximately750 mm with an average temperature of 13 ◦C. The winter wheatcultivar was Guomai 301 which was one of the major wheat cultivars inChina. Therefore, the winter wheat cultivar was adapted to the climateconditions. The plots were m ×2.4 m. Plots (12) of winter wheatwere sown on 14th October 2017, with three target plant densities(120, 270 and 420 plants/m2)(Fig.1 ). Each plant density was applied tofour replicates. total of 210 kg ha−1was applied, of which 60%(126 kg ha−1) was used before sowing and 40% (84 kg ha−1) wasused at early jointing stage. total of 90 kg ha−1and 45 kg ha-1wasapplied before sowing.2.2. Images collection and processingIt was important to maintain uniformity in the area under ob-servation so that the amount of information used in each image wasidentical. rectangle was laid down on the /uniFB02oor as physical marker toensure that all images contained to the same fraction of canopy(Fig.1 a). The color of the marker was white, and the size was1 ×1 m. With the growth of the winter wheat, the marker can beraised by putting sticks under the four corners. Three markers wereplaced within each plot, and certain distance was kept between everytwo markers. The images were captured by Canon EOS 600D digitalcamera which was mounted on tripod Fig.1 b). The camera orientedvertically downwards over the canopy at distance of 1.5 m, resultingin a/uniFB01eld of view of approximately seven rows of each plot. Images weretaken at focal length of 18 mm with an aperture of f/4. All imageswere obtained in 5184 ×3456 pixel spatial resolution with /uniFB02ash al-ways /uniFB00and without optical or digital zoom. The images were capturedbetween 10:00 am and 11:00 am and stored in JPG /uniFB01le format.The image collection was performed 17 times from days aftersowing (DAS) 44 to 153. dataset containing 612 images (4 plots 3densities 3 markers 17 times) was constructed, which was thendivided into three subsets, i.e., training, validation, and test datasets.The training dataset (4 plots 3 densities 2 markers 17 times)contained two of the three marker images of each plot and the testdataset (4 plots 3 densities 1 markers 17 times) contained theleft one marker image of each plot. Of the images in training dataset,20% was randomly chosen to construct validation dataset. Prior toimage analysis, all the images were manually cropped to eliminate theparts outside the markers, which resulted in images in 2763 ×2757pixel spatial resolution. The image dataset for the DCNN was con-structed by reshaping the cropped images into pixel size 64 ×64(height ×width). To enlarge the image dataset and decrease the chanceof over /uniFB01tting, data augmentation scheme was adopted Fig. ).The following augmentations were conducted: 90° rotation, 180°rotation, 270° rotation, horizontal /uniFB02ip, and vertical /uniFB02ip. In /uniFB01eld con-ditions, illumination was the major in /uniFB02uence of factors /uniFB00ecting imagecollection. Therefore, to make the DCNN robust to the illuminationchange of /uniFB01eld environment, the datasets were further augmented bysimulating the illumination change. The augmentation can be achievedby transferring the images to HSV color space and adjusting the Vcomponent Xiong et al., 2017 ). The augmentation scheme wasFig. 1. Experiment setup, (a) the markers within each plot; (b) the camera mounted on tripod.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²performed to the three subsets respectively, resulting in dataset (thewhole dataset) containing 15,912 images. The number of images in thethree datasets was 8486, 2122, and 5304 respectively. Each image inthe datasets was associated with an AGB value obtained by /uniFB01eld mea-surement.2.3. Above ground biomass measurementField measurements of AGB were performed simultaneously withimage collection. These measurements were conducted at intervalsranging from three to eight days, speci /uniFB01cally on DAS44, 48, 51, 55, 59,65, 80, 93, 100, 109, 115, 133, 138, 143, 146, 150, and 153. For eachmeasurement, /uniFB01ve plants of winter wheat were randomly sampled ineach plot outside of the markers. The measurement can be obtained byaveraging the /uniFB01ve dry weights and multiplying the corresponding plantdensity.2.4. Deep convolutional neural networkThe architecture of the DCNN used in this paper was depicted inFig.3 The DCNN consisted of four convolutional layers, four batchnormalization layers, three pooling layers, and one fully connectedlayer. The input to the DCNN was an RGB image of winter wheatcanopy with size 64 ×64 ×3, which would be mapped to scalar AGBin the deeper layer of the DCNN. The convolutional layers used /uniFB01ltersthat had size ×5 to extract features. The number of /uniFB01lters in eachconvolutional layer was 32, 64, 128, and 256 respectively, which wasinspired by the VGGNet Simonyan and Zisserman, 2014 ). Zero paddingwas not used by default except for the second convolutional layer sothat the size of the feature maps in the deeper layer was as an integer.Following the convolutional layers were four batch normalizationlayers which can speed up network training and reduce the sensitivityto network initialization. The /uniFB01lters in pooling layers used in the DCNNhad size ×2 and stride 2, which had the /uniFB00ect of shrinking the fea-ture maps by factor of two. Instead of max pooling, the function usedin the pooling layers was average pooling. Average pooling can beachieved by computing the average value of the kernel region. fullyconnected layer with one hidden unit and dropout layer with adropout rate of 50% were used to reduce the dimensions of the featuremaps down to scalar. The output of the DCNN was the estimated AGBand mean-squared-error with the /uniFB01eld measured value. The Recti /uniFB01edLinear Unit (ReLU) function was used as the activation function for allconvolutional and fully connected layers Krizhevsky et al., 2012 ;Dingand Taylor, 2016 ;Ghosal et al., 2018 ).The DCNN was trained on NVIDIA Quadro P4000 (8 GB memory)with CUDA 9.0. The stochastic gradient descent with momentumFig. 2. Image processing scheme.Fig. 3. Architecture of DCNN.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²(SGDM) was used to optimize the network weights. The learning ratewas initialized as 0.001 and dropped every 20 epochs by drop factorof 0.1. The momentum was set to 0.9 and remained constant for thetraining process. mini-batch of 64 was used. The maximum number ofepochs used for training was set to 300.2.5. Performance evaluationSome in /uniFB02uences of factors that may /uniFB00ect the performance of theDCNN on AGB estimation were evaluated so that the initial networkarchitecture can be updated for better estimation results. These factorsincluded the color information of canopy image, the amount of dataaugmentation, learning rate and mini-batch size. The other networkparameters were kept /uniFB01xed for the training process. In order to comparethe performance of the DCNN, tests were performed with other esti-mating methods that have been adopted successfully to estimate AGB. Itwas reported that Canopy Cover (CC) had strong correlation with AGB(Casadesús and Villegas, 2014 ;Bai et al., 2016 ;Baresel et al., 2017 ;Chung et al., 2017 ). Therefore, linear regression model using CC asthe predictor variable (LR-CC) was used as one of the comparedmethods. Besides, conventional classi /uniFB01ers in conjunction with somecolor and texture feature extraction techniques, including RandomForest (RF) Breiman, 2001 ), Support Vector Machine Regression (SVR)(Clevers et al., 2007 and Back propagation Neural Network (BPNN)(Wang et al., 2016 ), were also used as the compared methods.The canopy images of winter wheat were captured at early growthstages, resulting in the existence of pixels representing non-vegetationelements in these images, such as soil. Therefore, it was necessary toperform image segmentation of vegetation for the compared methodsprior to feature extraction. The segmentation was achieved by Canopeo,as well as the calculation of CC value Patrignani and Ochsner, 2015 ;Chung et al., 2017 ). Some examples were shown in Fig.4 Canopeo wasdeveloped based on color ratios of red to green (R/G) and blue to green(B/G) and an excess green index (2G-R-B), which was capable of de-tecting CC at high speed. The green canopy can be segmented by thefollowing criteria:<< /uni2212 /uni2212 >RGP n B GP n GRBP// 12 (1)where P1,P2, and P3were parameters for the color ratios that weretypically set to 0.95, 0.95, 20 by default. In this paper, the defaultvalues for the parameters were adopted to segment the canopy images.For the conventional classi /uniFB01ers that were compared to the DCNN,feature extraction from the segmented vegetation images was per-formed prior to model construction. The color information of winterwheat canopy was obviously di /uniFB00erent from that of the background.Therefore, color features were adopted as the distinguishing features.The color features used in this paper consisted of the mean and varianceof the 15 channels from /uniFB01ve color spaces, including R, G, from RGBcolor space, H, S, V, from HSV color space, H, S, I, from HSI color space,L, a*, b*, from CIEL*a*b* color space and Y, Cb, Cr from YCbCr colorspace. Texture features derived from the Gray-level co-occurrencematrix of each channel were used as well. The texture features includedcontrast, correlation, energy and homogeneity of the 15 channels.Based on the feature designs, feature set containing 90 features wasconducted.Linear regression analysis was utilized to compare model accuracy.Root-Mean-Squared Error (RMSE), Normalized Root-Mean-SquaredError (NRMSE) Rischbeck et al., 2016 ), and coe /uniFB03cient of determina-tion (R2) were used as the criterion for model evaluation. The NRMSEwas obtained by dividing the RMSE by the mean estimated AGB.3. Results of AGB estimation3.1. Results on the whole datasetThe performance of the DCNN evaluated over the test dataset wasshown in Fig.5 Regression analysis suggested that the AGB values es-timated from the canopy images of winter wheat in the test dataset hada good agreement with the corresponding AGB values derived from/uniFB01eld measurements (R2= 0.808, RMSE 0.8913 kg/plot, NRMSE =Fig. 4. Calculating CC using Canopeo, (a) Canopy images of di /uniFB00erent plant densities on DAS 44, (b) the segmentation results.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²24.95%).The contributions of the color information of canopy images to AGBestimation were examined by constructing the gray whole dataset. Thecolor information of images in the gray whole dataset was removed bytransferring the color images to grayscale images. The DCNN wastrained on the gray whole dataset, whose dimension of the images inthe input layers was changed to one. The performance of the DCNNevaluated over the gray test dataset was shown in Fig.6 .Compared to the performance on the whole dataset, the DCNN wasobviously in /uniFB02uenced by the absence of the color information. de-gradation on the performance can be observed (R2= 0.5857,RMSE 1.1872 kg/plot, NRMSE 33.54%). The results indicated thatthe color information of canopy images made contributions to AGBestimation. The relationship between the measured and estimated AGBvalues was much clearer when the estimation results were demon-strated individually on each measured date Fig. ). The estimated AGBvalues by the DCNN on the whole dataset had strong relation to themeasured values. It can be seen in Table that the performance of theDCNN over the whole dataset had larger R2values compared to thatover the gray whole dataset, as well as smaller errors, on each measureddate.The in /uniFB02uence of the data augmentation scheme to the performanceof the DCNN was evaluated by conducting experiments on the DCNNwith datasets augmented by di /uniFB00erent augmentation methods. Five da-tasets were constructed, i.e., no augmentation, rotation /uniFB02ipaugmentation, rotation illumination adjustment augmentation,/uniFB02ip illumination adjustment augmentation, and rotation /uniFB02ip +illumination adjustment augmentation. R2, RMSE and NRMSE wereadopted as the evaluation metrics. The estimation results were shown inTable . An obvious improvement on the performance of the DCNN bythe data augmentation scheme can be observed. The DCNN over the/uniFB01ve datasets had R2values equal to 0.2284, 0.4251, 0.5414, 0.7062 and0.808, respectively, RMSE values equal to 1.9646, 1.5060, 1.3169,1.1081 and 0.8913 kg/plot, respectively and NRMSE values equal to56.38%, 52.31%, 42.45%, 32.15% and 24.95%.The in /uniFB02uence of learning rate to the performance of the DCNN wasexamined on the whole dataset with learning rate varying from 10−3to10-8. The results were demonstrated in Table . It can be observed thatthe learning rate had second-order /uniFB00ect on the performance of theDCNN. Although the DCNN reached the top (R2= 0.7616, RMSE =0.8695 kg/plot, NRMSE 25.64%) with the learning rate of 10-5, thebest performance was achieved by the DCNN with dynamic learningrate Fig. a). The same method was utilized to evaluate the in /uniFB02uenceof mini-batch size to the performance of the DCNN. The range of themini-batch size was varying from to 256. The results were demon-strated in Table . Similar second-order /uniFB00ect on the performance of theDCNN can be observed for the in /uniFB02uence of mini-batch size, indicatingthat the DCNN reached the top with the mini-batch size of 64 Fig. b).The capability of the DCNN to estimate AGB at late growth stageswas evaluated by using images at jointing stage and booting stage. Thelate stage dataset was constructed by integrating the images of the twolate stages to the whole dataset, resulting in 828 images, i.e. 612 for theearly stages and 216 for the two late stages (4 plots 3 densities 3markers 6 times). By following the same protocols for image pro-cessing as in the AGB estimation of early stages, the dataset was dividedand augmented. The number of images in the three datasets was 11481,2871, and 7176 respectively. The performance of the DCNN evaluatedover the test dataset was shown in Fig.9 Compared to the performanceon the whole dataset, the DCNN evaluated over the late stage datasetgave worse results Fig. a, R2= 0.726, RMSE 1.557 Kg/plot,NRMSE 26.10%). It can be seen that the R2value at the two latestages was much worse than that at early stages Fig. b, R2= 0.1328,RMSE 0.6241 Kg/plot, NRMSE 6.06%), indicating that the DCNNin this study was not capable of estimating AGB at late growth stages.According to the experiment setup, the CC values for each plot canbe calculated by averaging the three CC values obtained from the threemarker images within the plot. The CC dataset containing 204 ob-servations (4 plots 3 densities 17 times) was constructed for theestimation of AGB, which was then divided into two subsets, i.e., cali-bration and test. 12 observations on DAS 115 were removed as outliersfrom the dataset due to strong illumination. The number of observa-tions in the calibration and test datasets was 144 and 48 respectively.Linear regression analysis was then used to calibrate the model. Theestimation results were shown in Fig. 10 The correlation (R2= 0.7246)can be observed between the /uniFB01eld measurements and the predictedAGB. The RMSE was 0.9409 kg/plot and the NRMSE was 37.22%.Compared to the DCNN, LR-CC indicated worse R2and RMSE results onthe whole dataset.Prior to the development of the conventional classi /uniFB01ers, the corre-lation analysis was performed between pairs of parameters that in-cluded the 90 features and AGB measured from the /uniFB01eld measurements(Table ).The features that were positively correlated to the AGB values wereused to build the classi /uniFB01ers. It can be seen that the strongest correla-tions were observed between the texture features and the AGB mea-surements. Speci /uniFB01cally, it was revealed that the energy of the sixchannels, i.e., of HSV, L* of CIEL*a*b*, a* of CIEL*a*b*, b* ofCIEL*a*b*, Cb of YCbCr and Cr of YCbCr r= 0.659**, 0.644**,0.669**, 0.602**, 0.594** and 0.667**, respectively), and theHomogeneity of a* of CIEL*a*b* channel r= 0.656**) had positivecorrelations with the AGB values. Therefore, these seven features wereFig. 5. Performance of the DCNN on the whole dataset.Fig. 6. Performance of the DCNN on the gray whole dataset.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²Fig. 7. Performance of the DCNN at each measured date.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²used to build the conventional classi /uniFB01ers. The number of observationsfor the two datasets, i.e., train and test, was 144 and 48 respectively.The estimation results by the conventional classi /uniFB01ers were shown inFig. 11 The R2values of the conventional classi /uniFB01ers were 0.7796,0.7445, and 0.736, respectively, the RMSE values were 0.9169, 0.9848,and 0.9379 kg/plot, respectively, and the NRMSE values were 35.19%,51.85% and 34.36%, respectively. It was shown that the best perfor-mance was achieved by the RF classi /uniFB01er. Although there was slightdecrease in the R2values, the three conventional classi /uniFB01ers showedsimilar ability to LR-CC on estimating AGB, since these methods werebased on the low-level features extracted from the digital images whichwere prone to be in /uniFB02uenced by illumination and clutter background.Table presented the evaluation results of all the estimationmethods on the whole dataset. It was revealed in Table that the DCNNdemonstrated superior results to the compared methods in the threemetrics.3.2. Results on the density datasetsAccording to the experiment setup, three datasets (Density 120,Density 270 and Density 450) corresponding to the three target plantdensities were constructed. Each dataset consisted of 1768 test images.The performance of the DCNN evaluated over the three density datasetswere shown in Fig.12 .Table presented the statistics that assessed theperformance.It can be seen the DCNN for AGB estimation over the three densitydatasets had R2values equal to 0.8693, 0.8062, and 0.7433, respec-tively, RMSE values equal to 0.4796, 0.8495, and 1.1656 kg/plot, andNRMSE values equal to 18.31%, 23.34%, and 26.15%. The in /uniFB02uence ofplant density on the performance of LR-CC was also evaluated by thethree density datasets. Each density dataset containing 16 observationswas used for test. Linear regression was used as statistical predictionmodel between the CC values and AGB Fig.13 ). The R2values of themodel over three density datasets were 0.8129, 0.7775 and 0.7183,respectively, the RMSE values were 0.6243, 0.8517, and 1.2212 kg/plot, respectively, and the NRMSE values were 28.48%, 33.03%, and43.41%, respectively. Table ).For the three conventional classi /uniFB01ers on the density datasets, theperformances were shown in Fig. 14 Similar results to LR-CC can beobserved Table ).Table presented the evaluation results of all the estimationmethods on the density datasets. The DCNN still outperformed the othermethods on the density datasets.3.3. Results on the temporal datasetThe in /uniFB02uence of temporal information of the /uniFB01eld measured AGB ondi/uniFB00erent DAS on the estimated AGB was evaluated. Canopy imagescaptured on /uniFB01ve DAS (DAS093, DAS115, DAS138, DAS146, andDAS153) were deducted for training. Therefore, the training of theDCNN was not using the information presented by canopy imagescaptured on these /uniFB01ve DAS. Canopy images captured on the other 12DAS were used to construct the datasets for training and validation.Data augmentation was performed to all the datasets. According to thisexperiment setup, temporal dataset was constructed and the numberof images in the three datasets was 8985, 2247, 4680 respectively. Theestimates of AGB for each plot were computed by averaging the threeestimated values from the marker images within the plot. The perfor-mance of the DCNN evaluated over the test dataset was shown inFig.15 .Although the performance slightly decreased (R2= 0.7629,RMSE 0.9409 kg/plot, NRMSE 23.87%), the DCNN was still cap-able of giving accurate AGB estimations. The in /uniFB02uence of temporalinformation on the performances of the compared methods was alsoevaluated. To obtain the results, the compared methods were trained onTable 1Estimation results of DCNN at each measured date.Date The whole dataset Gray whole datasetR2RMSE(kg/plot)NRMSE(%)R2RMSE(kg/plot)NRMSE(%)All dates 0.8080 0.8913 24.95% 0.5875 1.1872 33.54%DAS044 0.7491 0.4908 29.34% 0.5857 0.9078 43.23%DAS048 0.8211 0.4629 21.52% 0.6327 0.7646 31.25%DAS051 0.8131 0.4623 18.77% 0.3914 0.8337 32.95%DAS055 0.6777 0.7735 28.15% 0.0359 0.9863 30.98%DAS059 0.7447 0.7603 25.95% 0.5443 0.7944 33.15%DAS065 0.7950 0.8445 27.18% 0.0005 1.2800 39.02%DAS080 0.8580 0.5199 17.76% 0.8639 0.4718 17.17%DAS093 0.5972 0.7070 23.91% 0.2412 0.7922 30.69%DAS100 0.8854 0.4527 14.02% 0.5588 0.7003 24.75%DAS109 0.7501 0.7818 26.48% 0.4797 0.8566 23.91%DAS115 0.8839 0.4989 15.10% 0.4126 0.9939 31.30%DAS133 0.8073 0.6740 18.49% 0.6484 0.8968 24.76%DAS138 0.7571 0.9178 23.93% 0.7026 0.8008 20.18%DAS143 0.6726 0.6424 14.29% 0.3457 1.1920 27.11%DAS146 0.7346 1.0070 18.19% 0.5501 1.2430 23.81%DAS150 0.7937 0.9820 16.15% 0.3828 1.8460 29.01%DAS153 0.6603 1.2370 18.49% 0.2650 1.3380 23.24%Table 2Estimation results of the DCNN under the in /uniFB02uences of di /uniFB00erent factors.Factors RangeNumber of input imagesMethods No augmentation Rotation /uniFB02ip Flip illuminationadjustmentRotation illuminationadjustmentRotation /uniFB02ip illuminationadjustment//Number oftrainingimages408 2448 4488 6528 10608 //R20.2284 0.4251 0.5414 0.7062 0.808 //RMSE (Kg/plot) 1.9646 1.506 1.3169 1.1081 0.8913 //NRMSE (%) 56.38% 52.31% 42.45% 32.15% 24.95% //Learning rateRange 10−310−410−510−610−710−8DynamicR20.707 0.7292 0.7616 0.7002 0.6739 0.671 0.808RMSE (Kg/plot) 0.8049 0.9206 0.8695 0.9891 1.0109 0.7648 0.8913NRMSE (%) 21.08% 26.96% 25.64% 28.46% 30.12% 79.43% 24.95%Mini-batch sizeRange 16 32 64 128 256 /R20.3667 0.523 0.749 0.808 0.7177 0.6763 /RMSE (Kg/plot) 0.5252 0.5360 0.8619 0.8913 1.0534 1.1991 /NRMSE (%) 14.48% 15.59% 24.41% 24.95% 30.87% 34.98% /J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²Fig. 8. Performance of the DCNN under the in /uniFB02uence of di /uniFB00erent factors, (a) learning rate, (b) mini-batch size.Fig. 9. Performance of the DCNN at late growth stages, (a) early stage and the two late stages, (b) the two late stages, (c) jointing stage, (d) booting stage.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²144 observations (4 plots 3 densities 12 times) and tested on 48observations (4 plots 3 densities 4 times) of the left-out DAS. 12observations on DAS 115 were removed as outliers. The estimationresults were shown in Fig. 16 .Table presented the evaluation results of all the methods under thein/uniFB02uence of temporal information. It can be seen the DCNN consistentlyoutperformed the compared methods over the temporal dataset.4. DiscussionConvolutional neural network (CNN) is currently the state-of-the-arttechnique for image processing Ghosal et al., 2018 ;LeCun et al.,2015 ). As opposed to the conventional shallow machine learningmethod, CNN can automatically learn appropriate features fromtraining datasets instead of manual feature extraction Ferreira et al.,2017 ;LeCun et al., 2015 ), achieving promising results in wide rangeof plant phenotyping tasks, such as plant stress phenotyping Ghosalet al., 2018 ), weed detection Ferreira et al., 2017 ), plant disease re-cognition Ferentinos, 2018 ;Ma et al., 2018 ;Mohanty et al., 2016 ), andplant species identi /uniFB01cation Krause et al., 2018 ;Grinblat et al., 2016 ).The results obtained from this research indicated the potentials of theDCNN in estimating AGB. Compare to the four methods, the DCNN wasa more direct method for AGB estimation. The image segmentation ofvegetation was not necessary because the DCNN was able to use theimportant features to estimate AGB and ignore the non-important fea-tures, which not only reduced the computation cost but also increasedthe /uniFB03ciency of the estimation Ferentinos, 2018 ). In contrast, theperformances of the compared estimating methods greatly depended onthe results of image segmentation. Accurate segmentation resultsguaranteed accurate data sources to feature extraction. However, ca-nopy images captured under real /uniFB01eld conditions were su /uniFB00ering fromuneven illumination and complicated background, which was bigchallenge to achieve robust image segmentation of vegetation. Maet al., 2017 ).Fig. 10. Performance of LR-CC on the whole dataset.Table 3Correlations of the AGB measurements with the features.Above ground biomassR_avg .470**h_avg .478**l_avg .530**y_avg .523**H_avg .155*R_std −.053 h_std −.681**l_std .016 y_std .054 H_std −.657**R_Contrast .209**h_Contrast −.650**l_Contrast −.543**y_Contrast .181**H_Contrast −.576**R_Correlation −.200**h_Correlation −.555**l_Correlation −.678**y_Correlation −.170*H_Correlation −.240**R_Energy −.554**h_Energy .659**l_Energy .644**y_Energy −.572**H_Energy .556**R_Homogeneity −.616**h_Homogeneity .541**l_Homogeneity .532**y_Homogeneity −.620**H_Homogeneity .531**G_avg .539**s_avg .181**a_avg −.568**cb_avg −.382**S_avg −.599**G_std .094 s_std −.584**a_std .386**cb_std .185**S_std −.504**G_Contrast .242**s_Contrast −.430**a_Contrast −.541**cb_Contrast −.349**S_Contrast −.284**G_Correlation −.077 s_Correlation −.093 a_Correlation −.571**cb_Correlation −.604**S_Correlation −.188**G_Energy −.572**s_Energy −.461**a_Energy .669**cb_Energy .594**S_Energy −.487**G_Homogeneity −.625**s_Homogeneity −.431**a_Homogeneity .656**cb_Homogeneity .349**S_Homogeneity −.416**B_avg .499**v_avg .538**b_avg .423**cr_avg −.588**I_avg .515**B_std .249**v_std .102 b_std .187**cr_std .459**I_std .083B_Contrast .317**v_Contrast .242**b_Contrast −.456**cr_Contrast −.528**I_Contrast .265**B_Correlation −.062 v_Correlation −.077 b_Correlation −.734**cr_Correlation −.627**I_Correlation −.142*B_Energy −.553**v_Energy −.572**b_Energy .602**cr_Energy .667**I_Energy −.563**B_Homogeneity −.624**v_Homogeneity −.625**b_Homogeneity .434**cr_Homogeneity .528**I_Homogeneity −.623*** Correlation is signi /uniFB01cant at the 0.05 level.** Correlation is signi /uniFB01cant at the 0.01 level.Fig. 11. Performance of conventional classi /uniFB01ers on the whole dataset.Table 4AGB estimation results on the whole dataset.Evaluation metrics MethodsDCNN LR-CC RF SVR BPNNR20.808 0.7246 0.7796 0.7445 0.736RMSE (kg/plot) 0.8913 0.9409 0.9169 0.9848 0.9379NRMSE (%) 24.95% 37.22% 35.19% 51.85% 34.36%J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²The number of images in the input dataset was proved to be anin/uniFB02uence of factor to the DCNN, which agreed with the conclusion thatthe DCNN can achieve satisfactory results with large number of inputimages Ferreira et al., 2017 ;Ma et al., 2018 ). It can be seen in Table 2that the DCNN demonstrated an improving trend with the increase ofthe number of training images. Compared to no augmentation, all theaugmentation schemes improved the performance of the DCNN. Thebest performance was achieved by the DCNN on dataset rotation +/uniFB02ip illumination adjustment augmentation. The results indicated thatdata augmentation was necessary to the DCNN on AGB estimation,which can enlarge the number of input dataset, as well as enable theinput dataset to cover as much variability as in the /uniFB01eld conditions.As it was shown in Table , the performances of the DCNN over thewhole dataset were consistently superior to that over the gray wholedataset on each measured date, indicating that the color information ofcanopy images was also an in /uniFB02uence of factor to the DCNN. Color in-formation was essential to discriminate canopy and soil, therefore, theremoval of the color information from the gray whole dataset disabledthe DCNN to discriminate canopy and soil, leading to an obviousdecrease on the performance of the DCNN over the gray whole dataset.The results indicated that the color information was necessary to AGBestimation by using digital images and DCNN. Learning rate and mini-batch size were two factors that in /uniFB02uenced the accuracy of DCNN forAGB estimation. Both two factors had second-order /uniFB00ects on the per-formance of the DCNN Fig. a, b). According to the experiments, dy-namic learning rate and mini-batch size of 64 was suitable for theDCNN in this study to estimate AGB. It can be seen in Fig. that theDCNN in this study was not able to estimate AGB at late growth stages.The estimation results for the two late growth stages were much worsethan those for the early stages. Especially for the booting stage, theestimation results were seemed to reach the upper limit, staying around10 kg/plot even though the winter wheat was gaining dry matters. Thisupper limit may be rooted in the input layer, whose input images withsize 64 ×64 ×3 were not informative enough for the DCNN to capturethe canopy details at late growth stages. Therefore, it was suggested touse input images of higher resolution for the DCNN to estimate AGB atlate growth stages.It can be seen from Table that noticeable drop in the perfor-mances of the compared methods can be observed when the plantdensity increased from 120 to 420 plants/m2while the DCNN onlyslightly dropped from 0.8693 to 0.7433, indicating that the plantdensity in /uniFB02uenced the performance of the conventional classi /uniFB01ers whilethe DCNN made robust tool to estimate AGB of winter wheat at earlygrowth stages. Linear regression model using CC as the predictor vari-able could achieve good estimations of AGB in relatively low-densityplot, which agreed with several previous studies on using CC to esti-mate the growth-related traits Casadesús and Villegas, 2014 ;Rahamanet al., 2015 ;Gizaw et al., 2016 ;Naito et al., 2017 ). In the relatively low-density plot, the adjacent leaves barely overlapped. The informationpresented by the vegetation can be fully captured by digital images. Inthe relatively high-density plot, the overlapping adjacent leaves causedmissing information of the green structure in the digital images. Thereason why the DCNN consistently outperformed the comparedFig. 12. Performance of the DCNN on the density datasets, (a) Density 120, (b) Density 270, (c) Density 420.Table 5AGB estimation results under in /uniFB02uence of plant density.EvaluationmetricsDatasets MethodsDCNN LR-CC RF SVR BPNNR2Density 120 0.8693 0.8129 0.8314 0.7952 0.8261Density 270 0.8062 0.7775 0.7628 0.7146 0.6995Density 420 0.7433 0.7183 0.6887 0.6575 0.6124RMSE(Kg/plot)Density 120 0.4796 0.6243 0.5597 0.7061 0.6655Density 270 0.8495 0.8517 0.8526 0.8949 0.8949Density 420 1.1656 1.2212 1.2972 1.2876 1.1797NRMSE (%) Density 120 18.31% 28.48% 26.78% 58.38% 39.24%Density 270 23.34% 33.03% 28.66% 36.66% 24.73%Density 420 26.15% 43.41% 49.35% 62.87% 41.04%Fig. 13. Performance of LR-CC on the density datasets, (a) Density 120, (b) Density 270, (c) Density 420.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²methods over the temporal dataset was that the quality of the imagefeatures was determinant factor in the performance of the comparedmethods. For these methods, low-level features were manually de-signed, resulting in the weak generalization ability to AGB estimation.In contrast, the DCNN can automatically learn appropriate featuresfrom canopy images of winter wheat instead of manual feature design,which turned out to be reliable tool for AGB estimation.In this study, an easy-to-use AGB estimation method for winterwheat at early growth stages was proposed by using digital imagescaptured under /uniFB01eld conditions and deep convolutional neural network.The method was performed on digital images of three density datasets(120, 270 and 420 plants/m2). The results showed that the performancedropped with the increase of plant density. One possible reason for thedrop was that the canopy structure information could not be capturedby digital images. With the increase of plant density, adjacent leavesoverlapped, resulting in structure information that was inaccessible bydigital cameras oriented towards nadir. To make the method robust tothe variation of plant density, one possible way was to adopt the 3DFig. 14. Performance of the conventional classi /uniFB01ers on the density datasets, (a) Density 120, (b) Density 270, (c) Density 420.Fig. 15. Performance of the DCNN under the in /uniFB02uence of temporal informa-tion.Fig. 16. Performance of compared methods under the in /uniFB02uence of temporal information, (a) LR-CC, (b) conventional classi /uniFB01ers.Table 6AGB estimation results under in /uniFB02uence of temporal information.Evaluation metrics MethodsDCNN LR-CC RF SVR BPNNR20.7629 0.6292 0.6994 0.6818 0.6443RMSE (kg/plot) 0.9409 1.0946 1.0877 1.2034 1.2387NRMSE (%) 23.87% 29.17% 27.53% 30.53% 32.30%J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²information of the canopy. Point clouds created by LiDAR Eitel et al.,2014 ;Greaves et al., 2015 or Photogrammetry Walter et al., 2018 )were widely adopted to utilize the 3D information for AGB estimation.Predictors derived from point clouds were strongly correlated withAGB. Although point cloud showed good potential to estimate AGB,there existed limitations that the creation of point cloud required high-cost device and heavy calculation cost. Therefore, low-cost solutions,such as depth images, can alternatively help the DCNN to maintainrobustness to plant density.5. ConclusionAn AGB estimation method for winter wheat at early growth stagesbased on the DCNN was proposed in this paper. The method took RGBimages of winter wheat canopy captured by low-cost digital camera asinput. The estimated AGB values showed strong correlations to manualmeasurements (R2= 0.808), indicating the potential of the DCNN forestimating growth-related traits. The performance of DCNN was com-pared to the conventional methods adopted to estimate AGB. Imagedatasets with di /uniFB00erent plant densities (120, 270 and 420 plants/m2)and temporal in /uniFB02uence were constructed to test the methods. Resultsshowed the performances of all the methods were in /uniFB02uenced to varyingdegrees by plant density while the DCNN achieved the best robustness.Experiment on temporal in /uniFB02uence showed the DCNN had good gen-eralization ability. Color information of canopy images and the numberof training images were the in /uniFB02uence of factors to the DCNN. Toachieve robust estimation results, it was necessary to use considerablenumber of RGB images for the DCNN. Learning rate and mini-batch sizewere two factors that could in /uniFB02uence the accuracy of DCNN for AGBestimation as well. It was suggested to do parameter search to /uniFB01nd thesuitable values for the DCNN. Experiments indicated that the DCNN inthis study was not capable of estimating AGB at late growth stages dueto the limited details of the canopy presented by the relatively smallsize images of the input layer. It was suggested to use images of higherresolution for the DCNN to estimate AGB at late growth stages.Since the DCNN makes reliable tool for AGB estimation, there is agood chance that the method is extended to high-throughput pheno-typing use by combining with mobile devices. Canopy images can becaptured by cameras on the mobile devices and transferred to the serverside where pre-trained DCNN network was stored. Once the networkgave the estimation result, it could be transferred back to the mobiledevices to be demonstrated to users. Moreover, the DCNN can be in-tegrated into the high-throughput phenotyping platforms since its inputimages can be captured by low-cost digital cameras. The AGB estima-tion results showed that the DCNN had good potential for the esti-mation of the growth-related traits. The same technique used in AGBestimation for winter wheat can be applied to other growth-relatedtraits by modifying the datasets, as well as to di /uniFB00erent types of crop. Inorder to improve the /uniFB03ciency, an automatic image processing methodfor detecting the markers will be developed in the future work.AcknowledgmentsThe Authors wish to acknowledge The National Key Research andDevelopment Program of China (2016YFD0300606), The NationalNatural Science Foundation of China (31801264) and The National KeyResearch and Development Program of China (2017YFD0300401) fortheir funding support of this research.ReferencesBai, G., Ge, Y., Hussain, W., Baenziger, P.S., Graef, G., 2016. multi-sensor system forhigh throughput /uniFB01eld phenotyping in soybean and wheat breeding. Comput.Electron. Agric. 128, 181 –192.Baresel, J.P., Rischbeck, P., Hu, Y., Kipp, S., Hu, Y., Barmeier, G., Mistele, B., 2017. Use ofa digital camera as alternative method for non-destructive detection of the leafchlorophyll content and the nitrogen nutrition status in wheat. Comput. Electron.Agric. 140, 25 –33.Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M.L., Bareth, G.,2015. Combining UAV-based plant height from crop surface models, visible, and nearinfrared vegetation indices for biomass monitoring in barley. Int. J. Appl. Earth Obs.Geoinf .Breiman, L., 2001. Random forests. Mach. Learn. 45 (1), –32.Casadesús, J., Villegas, D., 2014. Conventional digital cameras as tool for assessing leafarea index and biomass for cereal breeding. J. Integr. Plant Biol. 56, –14.Chang, Y.K., Zaman, Q.U., Rehman, T.U., Farooque, A.A., Esau, T., Jameel, M.W., 2017. Areal-time ultrasonic system to measure wild blueberry plant height during harvesting.Biosyst. Eng. 157, 35 –44.Chung, Y.S., Choi, S.C., Silva, R.R., Kang, J.W., Eom, J.H., Kim, C., 2017. Case study:estimation of sorghum biomass using digital image analysis with Canopeo. BiomassBioenergy 105, 207 –210.Clevers, J.G.P.W., van der Heijden, G.W.A.M., Verzakov, S., Schaepman, M.E., 2007.Estimating grassland biomass using svm band shaving of hyperspectral data.Photogramm. Eng. Rem. Sens. 73 (10), 1141 –1148 .Daughtry, C.S.T., Walthall, C.L., Kim, M.S., de Colstoun, E.B., McMurtrey, J.E., 2000.Estimating corn leaf chlorophyll concentration from leaf and canopy re /uniFB02ectance.Remote Sens. Environ. 74, 229 –239.Ding, W., Taylor, G., 2016. Automatic moth detection from trap images for pest man-agement. Comput. Electron. Agric. 123, 17 –28.Eitel, J.U.H., Magney, T.S., Vierling, L.A., Brown, T.T., Huggins, D.R., 2014. LiDAR basedbiomass and crop nitrogen estimates for rapid, non-destructive assessment of wheatnitrogen status. Field Crop. Res. 159, 21 –32.Fan, X., Kawamura, K., Guo, W., Xuan, T.D., Lim, J., Yuba, N., Kurokawa, Y., Obitsu, T.,Lv, R., Tsumiyama, Y., Yasuda, T., Wang, Z., 2017. simple visible and near-infrared(V-NIR) camera system for monitoring the leaf area index and growth stage of Italianryegrass. Comput. Electron. Agric. –10.Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.Ferreira, A.D.S., Freitas, D.M., Silva, G.G.D., Pistori, H., Folhes, M.T., 2017. Weed de-tection in soybean crops using convnets. Comput. Electron. Agric. 143, 314 –324.Ghosal, S., Blystone, D., Singh, A.K., Ganapathysubramanian, B., Singh, A., 2018. Anexplainable deep machine vision framework for plant stress phenotyping. Proc. Natl.Acad. Sci. U. S. A. –6.Gizaw, S.A., Garland-Campbell, K., Carter, A.H., 2016. Use of spectral re /uniFB02ectance forindirect selection of yield potential and stability in Paci /uniFB01c Northwest winter wheat.Field Crop. Res. 196, 199 –206.Gnyp, M.L., Bareth, G., Li, F., Lenz-Wiedemann, V.I.S., Koppe, W., Miao, Y., Hennig, S.D.,Jia, L., Laudien, R., Chen, X., Zhang, F., 2014. Development and implementation of amultiscale biomass model using hyperspectral vegetation indices for winter wheat inthe North China Plain. Int. J. Appl. Earth Obs. Geoinf .Greaves, H.E., Vierling, L.A., Eitel, J.U.H., Boelman, N.T., Magney, T.S., Prager, C.M.,Gri/uniFB03n, K.L., 2015. Estimating aboveground biomass and leaf area of low-statureArctic shrubs with terrestrial LiDAR. Remote Sens. Environ. 164, 26 –35.Grinblat, G.L., Uzal, L.C., Larese, M.G., Granitto, P.M., 2016. Deep learning for plantidenti /uniFB01cation using vein morphological patterns. Comput. Electron. Agric. 127,418–424.Jannoura, R., Brinkmann, K., Uteau, D., Bruns, C., Joergensen, R.G., 2015. Monitoring ofcrop biomass using true colour aerial photographs taken from remote controlledhexacopter. Biosyst. Eng. 129, 341 –351.Jiang, Y., Li, C., Paterson, A.H., 2016. High throughput phenotyping of cotton plantheight using depth images under /uniFB01eld conditions. Comput. Electron. Agric. 130,57–68.Krause, J., Sugita, G., Baek, K., Lim, L., 2018. WTPlant (what ’s that plant?): deeplearning system for identifying plants in natural images. Proceedings of the 2018ACM on International Conference on Multimedia Retrieval ICMR ’18. pp. 517 –520.Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet classi /uniFB01cation with deepconvolutional neural networks. Adv. Neural Inf. Process. Syst. –9.LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521 (7553), 436 –444.Liebisch, F., Kirchgessner, N., Schneider, D., Walter, A., Hund, A., 2015. Remote, aerialphenotyping of maize traits with mobile multi-sensor approach. Plant Methods 11 .Ma, J., Du, K., Zhang, L., Zheng, F., Chu, J., Sun, Z., 2017. segmentation method forgreenhouse vegetable foliar disease spots images using color information and regiongrowing. Comput. Electron. Agric. 142, 110 –117.Ma, J., Du, K., Zheng, F., Zhang, L., Gong, Z., Sun, Z., 2018. recognition method forcucumber diseases using leaf symptom images based on deep convolutional neuralnetwork. Comput. Electron. Agric. 154, 18 –24.Moeckel, T., Safari, H., Reddersen, B., Fricke, T., Wachendorf, M., 2017. Fusion of ul-trasonic and spectral sensor data for improving the estimation of biomass in grass-lands with heterogeneous sward structure. Remote Sens. 9, –14.Mohanty, S.P., Hughes, D., Salathe, M., 2016. Inference of Plant Diseases from LeafImages Through Deep Learning. arXiv1604.03169 [cs]. pp. –6.Naito, H., Ogawa, S., Valencia, M.O., Mohri, H., Urano, Y., Hosoi, F., Shimizu, Y., Chavez,A.L., Ishitani, M., Selvaraj, M.G., Omasa, K., 2017. Estimating rice yield related traitsand quantitative trait loci analysis under di /uniFB00erent nitrogen treatments using simpletower-based /uniFB01eld phenotyping system with modi /uniFB01ed single-lens re /uniFB02ex cameras.ISPRS J. Photogramm. Remote Sens. 125, 50 –62.Patrignani, A., Ochsner, T.E., 2015. Canopeo: powerful new tool for measuring frac-tional green canopy cover. Agron. J. 107, 2312 –2320 .Pittman, J.J., Arnall, D.B., Interrante, S.M., Mo /uniFB00et, C.A., Butler, T.J., 2015. Estimation ofbiomass and canopy height in Bermudagrass, Alfalfa, and wheat using ultrasonic,laser, and spectral sensors. Sensors (Switzerland) 15, 2920 –2943 .Pölönen, I., Saari, H., Kaivosoja, J., Honkavaara, E., Pesonen, L., 2013. Hyperspectralimaging based biomass and nitrogen content estimations from light-weight UAV.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²Proc. SPIE Int. Soc. Opt. Eng. 888, 521 –525.Possoch, M., Bieker, S., Ho /uniFB00meister, D., Bolten, A., Schellberg, J., Bareth, G.,Conservation, R., Group, C.S., Conservation, R., Ecology, P., Model, C.S., 2016. Multi-temporal crop surface models combined with the RGB vegetation index fromUAV-based images for forage monitoring in grassland. ISPRS Int. Arch. Photogramm.Remote Sens. Spat. Inf. Sci. XLI, 12 –19.Rahaman, M.M., Chen, D., Gillani, Z., Klukas, C., Chen, M., 2015. Advanced phenotypingand phenotype data analysis for the study of plant growth and development. Front.Plant Sci. 6, –15.Rasmussen, J., Ntakos, G., Nielsen, J., Svensgaard, J., Poulsen, R.N., Christensen, S.,2016. Are vegetation indices derived from consumer-grade cameras mounted onUAVs su /uniFB03ciently reliable for assessing experimental plots? Eur. J. Agron. 74, 75 –92.Rischbeck, P., Elsayed, S., Mistele, B., Barmeier, G., Heil, K., Schmidhalter, U., 2016. Datafusion of spectral, thermal and canopy height parameters for improved yield pre-diction of drought stressed spring barley. Eur. J. Agron. 78, 44 –59.Schirrmann, M., Giebel, A., Gleiniger, F., /uniFB02anz, M., Lentschke, J., Dammer, K.H., 2016a.Monitoring agronomic parameters of winter wheat crops with low-cost UAV imagery.Remote Sens. 8, 706 .Schirrmann, M., Hamdorf, A., Garz, A., Ustyuzhanin, A., Dammer, K.H., 2016b.Estimating wheat biomass by combining image clustering with crop height. Comput.Electron. Agric. 121, 374 –384.Simonyan, K., Zisserman, A., 2014. Very Deep Convolutional Networks for Large-ScaleImage Recognition. arXiv preprint arXiv:1409. 1556. .Tucker, C.J., 1977. Asymptotic nature of grass canopy spectral re /uniFB02ectance. Appl. Opt. 16,1151 –1156 .Tucker, C.J., 1979. Red and photographic infrared linear combinations for monitoringvegetation. Remote Sens. Environ. 8, 127 –150.Walter, J., Edwards, J., McDonald, G., Kuchel, H., 2018. Photogrammetry for the esti-mation of wheat biomass and harvest index. Field Crop. Res. 216, 165 –174.Wang, L., Zhou, X., Zhu, X., Dong, Z., Guo, W., 2016. Estimation of biomass in wheatusing random forest regression algorithm and remote sensing data. Crop J. 4,212–219.Woebbecke, D.M., Meyer, G.E., Von Bargen, K., Mortensen, D.A., 1995. Color indices forweed identi /uniFB01cation under various soil, residue, and lighting conditions. Trans. Am.Soc. Agric. Eng. 38, 259 –269.Xiong, X., Duan, L., Liu, L., Tu, H., Yang, P., Wu, D., Chen, G., Xiong, L., Yang, W., Liu, Q.,2017. Panicle-SEG: robust image segmentation method for rice panicles in the /uniFB01eldbased on deep learning and superpixel optimization. Plant Methods 13, –15.Zhang, L., Verma, B., Stockwell, D., Chowdhury, S., 2018. Density weighted connectivityof grass pixels in image frames for biomass estimation. Expert Syst. Appl. 101,213–227.J. Ma et al. (XURSHDQ-RXUQDORI$JURQRP\²