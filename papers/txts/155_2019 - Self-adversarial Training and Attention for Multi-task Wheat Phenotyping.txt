This manuscript is in press. It ha been accepted for publication in Applied Engineering in Agriculture When the final, edited version is posted online this in-press version will be removed. Example citation: Authors. Year. Article title. Appl. Eng. Agric. (in press). DOI number. The DOI for this manuscript, active after publicat ion, will be https:// doi.org/10.13031/aea.13406. SELF-ADVERSARIAL TRAINING AND ATTENTION FOR MULTI -TASK WHEAT PHENOTYPING Gensheng Hu, Lidong Qian, Dong Liang, Mingzhu Wan The authors are Gensheng Hu, Professor, School of Electronics nd Information Engineering, Anhui University, Hefei, Anhui, China; Lidong Qian, under postgraduate, School of Electronics nd Information Engineering, Anhui Un iversity, Hefei, Anhui, China; Dong Liang, Professor, School of Electronics and Information Engineering, Anhui University, Hefei, Anhui, China; Mingzhu Wan undergraduate, School of Informa tion Science and Technology, Fudan University, Shan ghai, China. Corresponding author: Gensheng Hu, No. 111 Jiulong Road, Economic and Technological Developmen Zone, Hefei, Anhui, China; phone: +86-134-8560-7849; e-mail: hugs2906@sina.com. ABSTRACT Phenotypic monitoring provides important data support for precision agriculture management. This study proposes deep learning-based method to gain an accurate count of wheat ears and spikelets. The deep learning networks incorporate self-adversarial training and attention mechani sm with stacked hourglass networks. Fo ur stacked hourglass networks follow holistic attention map to construct generator of self-adversari al networks. The holistic attention maps enable the networks to focus on the overall consistency of th whole wheat. The discriminator of self- adversarial networks displays the same structure as the generator, which causes adversarial loss to the generator. This process improves the generator’ learning ability and prediction accuracy for occluded wheat ears. This method yields higher wheat ear count in the Annotated Crop Image Database (ACID) data set than the previous state-of-the-art algorithm. Keywords. Plant phenotype, attention mechanism, self-adversarial networks, stacked hourglass. Research on plant phenotype usually includes plant growth anal ysis, root phenotypic analysis disease identification, and leaf and seed enumeration. Image-based plant phenotypic analysis can be classified as holistic or component analyses (Choudhury et al., 2016). Holistic analysis mainly considers the geometric properties of the whole plant. Component analysis considers various parts of plant, such as spikelets, leaves, stems, and fruits. Wheat is an important crop in the world, and its phenotype analysis is related to wheat yield and food security. This study focuses on the component analysis of wheat ear and spikelet counting, which are important re search subjects of wheat phenotype. In practical application, images of wheat are derived by cameras in wheat fields and wheat ears and spikelets are counted by using image processing algorithms. The re sult can be used to estimate wheat yiel and analyze wheat ear distribution. Shi et al. (2013) developed clustering algor ithm to estimate plant counting, plant location, and interplant spacing. Perrin et al (2005) presented framework for extracting tree crowns by minimizing the energy function. The framework produced good results in relatively short period of time, even in the case of dense plantations. Fe rnandez–Gallego et al (2018) proposed wheat ear counting method for field conditions. Ear counting was determined by using Find Maxima to segment local peaks. Zhou et al. (2018) proposed multi-feature optimization an TWSVM method for wheat ear counting under field conditions. The aforementioned methods adopt classical machine learning or image processing technologies for plant counting. These methods need to manually extract features and expect certain requirements for the shape and color of the targets in the origina images. Deep learning networks, such as DC NN, RCNN, R-FCN, GoogLeNet, ResNet, VGGNet, AlexNet, and GAN, have been used for plant phenotyping in recent years (Hu, 2018; Fuentes, 2017; Singh, 2018; Valerio, 2017). GAN allows unsupervised training of generative models (Goodfellow et al., 2014), but it is unstable and difficult to train. DCGAN, an full convolutiona architecture, successfully solved the limitation of GAN (Radford et al., 2015). DCGAN can generate heat maps of labels as in saliency (Luc et al., 2016). Adding the adversarial training stra tegy to plant phenotyping can bring benefits, such as improved accuracy and recall rate. Visual attention model is effective in computation and image understanding. Existing methods usually use recursive neural networks to generate the attention map of an image area at each step and combine the information of different steps to make final decision (Bahdanau, 2015; Ba, 2015; Kuen, 2016). Thus far, no study has applied attention model to wheat phenotyping. This study aims to further explore the state-of-the-art progr ess in wheat phenotypic research. deep ConvNet model with attention mechanism is designed to lear the structure of wheat ear phenotypes through adversarial training. The designed networks perform multi-task learning, which simultaneously locat and count ears and spikelets. The contributions of this study are presented as follows: ●A self-adversarial training architecture is applied in wheat phenotypic multi-task learning. model is developed to solve wheat phenotypic problem by using Deep Convolutional Generative Adversarial Networks (DCGAN) techniques. ●A visual attention mechanism is added to self-adversarial tr aining. This mechanism drives the model to focus on the regions of interest and learn the stru cture of wheat ears and spikelets. ●The proposed method is evaluated using ACID data set (Pound et al., 2017), and the effects of different components on the design are analyzed. The experimental results show improved F1 score on the data set. MATERIALS AND METHODS DATA ACQUISITION AND PRE -PROCESSING Wheat images for training an validating the proposed algorithm were deri ved from the annotated crop image database (ACID) (Pound et al., 2017). ACID is the latest date set of wh eat plants under glasshouse conditions and contains 534 images, including more than 4,000 annotated ears and 48,000 annotated spikelets. Figure shows the annotation of wheat ears and spikelets. The native resolution of these images is 1956×1530. The occluded spikelets are not annotated, but partially occluded ears are retained. Figure shows few challenging examples from the ACID data set. Figure 1. Annotation of wheat ears and spikelets Deep network architecture restricts the size of the input images due to hardware and software limitations. Each wheat image used in this study is divided into several regions during the training process, and the target ear is centered on the images wi th roughly the same scale. The default initial cropping is 384 pixe ls, and the image patches are wrapped to the size of 256×256. The image patches are randomly rotated, scaled and lipped to ensure robustness of the model training. Figure 2. Challenging examples from the ACID data set (a) slig ht occlusion, (b) severe occlus ion, and (c) self-similarity SELF-ADVERSARIAL TRAINING WITH ATTENTION MECHANISM Generative adversarial networks (GAN) consist of generator and discriminator (Goodfello et al., 2014). The generator is used to simulate the original target data distribution and ge nerate target samples. The discriminator is used to determine whether the generated samples are real. These components work adversely to learn the representative features of the given data set. GAN has been used to identify plant diseases and insect pests (Hu, 2019; Zhou, 2019). The generator used in the proposed self-adversarial networks is four-stacked hourglass network accompanied by four attention maps. After feeding forward through the first part, the generator will obtain two sets of heatmaps that indicate the confidence scores of wheat ears and spik elets at every location. The discriminato displays the same architecture as the generator. However, the RGB image is encoded separately with the generated heatmaps and the ground-truth heatmaps, which are used as input of discriminator. Figure shows an overview of the propos ed network architecture. Figure 3. Overview of the proposed network architecture GENERATOR The generator uses hourglass network as the base network and provides repetitive top–down, bottom–up structure. The generator helps capture information from input images at different scales. Many cues, including wheat orientation, spikelet arrangement, and the relationship among adjacent spikelets, can be identified at different scales in the images. Additionally, attention mechanism is introduced and combined with the hour glass network. This process allows the generator not only to learn the features from multiple scales but also focus on the hole wheat. Each blue block in Figure represents one or more residual blocks, which combine convolution, batch normalization, and an additional skip layer to avoid vanishing gradients and help train in this very deep network. All residual blocks output 256 features. Each yellow block in Figure represents an attention map. The generated attention maps are used to reweight the features to automatically infer the regions of interest. he network inputs are represented by 256×256 RGB images. After se ries of initial processing, the size of the original image is changed to 256×256, and the output size of the generator’s final attention mapping is 64×64×2, where represents two classes of ears and spikelets. DISCRIMINATOR The discriminator distinguishes real data from the generated on es. The discriminator architecture is the same as that of the generator. The inputs of the discriminator correspond to ground-truth or generated heatmaps, which are bound to the corresponding RGB imag es of the wheat. The discriminator learns whether the locations of ears and spikelets on the heatmaps are correct and correspond to the wheat in the input color images. The discriminator also reconstructs two sets of new heatmaps which exhibit quality that depends on the similarity between the reconstructed and inputted heatmaps. Discriminator loss corresponds to the error between the inputted and reconstructed heatmaps. ATTENTION MECHANISM Visual attention is an important mechanism for the brain to effectively understand the scene. Traditional models define regions of interest manually by set of rectangle bounding boxes. An attention model provides principled way to focus on target regions by generating attention maps. Attention model has achieved great success in occlusion and self-similar human pose estimation (Chu et al., 2017). Occlusion and self-similarity problems also exist in wheat phenotyping. Therefore, an attention model is introduced into the proposed network arch itecture. holistic attention map is used to encode the configurations of the whole wheat within each hourglass network. Conventional attention mechanisms often use global softmax to model spatial correlations. disadvantage is that the entire image is normalized on the basis of the constant factor, and he local neighborhood spatial correlation is ignored. Conditional ran dom fields (CRF) can be used to model the spatial correlation 6 so attention map-driven networks can concentrate on complex wheat ear configura tions (Chu et al., 2017). In the CRF model, hard attention method is adopted. The attention label of the i-th position is iy= {0, 1}. The probability for iy=1 is obtained iteratively using the mean-field approximation as follows: ,1(( ) 1(1 )(( ( ) 1uitui j tjityiw tσψσψ−= Φ== +Φ > (1) Where ) 1/ (1 exp( ))x σ =+− is the sigmoid function, ()uiψ measures the cost of attention label =1iy which can be derived from feature maps, and ,ijw is filter independent on time steps. Convolutional features and fe ature map are denoted as fand s, respectively. Thus, sg Wfb )a=∗ (2) Where Wa represents convolution filters, and is nonlinear activation function. The attention map tΦ of stage can be expressed as follows (Chu et al., 2017): 1(W s) 0(, W)(W 1, 2,3kkt kttMstσσ− ∗=Φ= ∗Φ (3) Where Mdenotes weight shared convolution se quence for the mean field approximation, ()xσ is the sigmoid function, andWkis the spatial correlation kernel which is shared at different time steps. TRAINING The generator is trained by the back-propagation of its own loss MSEL and the adversarial loss advL from the discriminator. The loss function GL of the generator is presented as (Chou et al., 2018): GM E a v=+LLL (4) Where Gμ is hyperparameter used to control the weight of the adversarial lossadvL The generator consists of stacks of hourglass modules with attention model. The output of each attention model contai ns attention maps, each of which contains Gaussian peak at the ground-truth location of the j-th ear. Every ear contains the lo cations of the spikelets. The loss MSELof the generator itself can be expressed as: 22MSE m11 111ˆ () )Q NM NMij ij ijk ijkij ij kLH H λ== ====− − (5) Where ijH is the ground-truth heatmap of j-th wheat ear on i-th stack, ˆijH is the generated ear heatmap. ijkH is the k-th spikelet of j-th wheat ear on i-th stack in the ground-truth heatmap, ˆijkH is the generated spikelet heatmap, is the number of spikelets in each ear, and mλis balance parameter used to control the weight loss of spikelets. Using MSEL as loss function, the generator is enforced to learn image feat ures, which are important for localizing ears and spikelets. Adversarial loss advL enforces the generator to reduce unreasonable prediction and can be expressed as: 2adv1ˆ (( ) )MjjjLH H X==− (6) Where ˆjH is the output heatmap of the generator, is the input image, and ˆ(, )j DH is the output heatmap of the discriminator. Adversarial loss advL calculates the error between the ge nerated and reconstructed heatmaps. The generated and ground-truth heatmaps of ears and spikelets are separately fed into the discriminator for training. Two sets of heatmaps are reconstructed to calculating realL and fakeL Discriminator loss is expressed as: 2real12fake1real fake1 real fake(( ) )ˆˆ(( ) )()MjjjMjjjtttLH H XLH H XLL Lkk LL λγ==+=−=−=−=+ −D (7) Where LDis the discriminator loss for optimizing each pixel, tk is introduced to keep the balance between generator and discriminator in order to prevent gradient from collapsing, kλand γare hyper parameters. kλis equivalent to the learning rate of kandγdecides the balance ratio during the optimization. In this study, 0kis initialized as 0, kλis set to 0.001, and γis set to 0.5. When the discriminator overwhelms the generator, realLγ will be less thanfakeL andtkwill decrease to make the termrealL more dominant. Thus the generator will catch up andfakeL will decrease to achieve the balance. It works in the same way for the opposite case. ALGORITHM STEPS The specific algorithm (workf low) proceeds as follows: Step 1: Each wheat image derived from ACID database is divide into several regions, and the target ear is centered on the images with roughly the same scale; Step 2: All image patches are wrapped to the size of 256×256 and divided into training and test samples; Step 3: Training samples are augmented by random rotation, scaling, and flipping; Step 4: The augmented training samples are used as input to train the proposed network model (Figure 2); Step 5: The test samples are input into the trained networks, and the generator outputs the number and the positions of wheat ears and spikelets. RESULTS AND DISCUSSIONS NUMERICAL RESULTS This study uses F1 score and recall as quantitative evaluation in dexes (Pound et al., 2017). The ear F1 scores of different methods (Hang, 2016; Pound, 2017; He, 2016; Xie, 2017) during training and testing at normalized distance of 0.2 are compared and shown in Figures and 5. The recall values of different methods during testing at normalized distance of 0.2 are compared and displayed in Figures 6. Different normalization distances ar also compared and presented in Figure 7. In testing, VGG, ResNet50, ResNeXt50, hourglass network and the proposed method achieve 85%, 86%, 82%, 89%,and 90.5% F1 scores, and 82.3%,83.1%,82.9%,85.3%,90.8% recall values. The hourglass network, which is adopted as baseline in the proposed method, is superior to the other deep learning networks. F1 score and recall values of the proposed method improve by 1.5% and 5.5% compared with those of the hourglass network. The proposed method solves the problem of wheat ear occlusion effectively, but the occluded spikelets are not annotated in the ACID data set. Thus, the recognition accuracy of spikelets is not greatly improved. Figure 4. Comparison of ear F1 scores during training Figure 5. Comparison of ear F1 scores during testing Figure 6. Comparison of ear recall during testing 10 Figure 7. Comparisons of normalized distances in the ACID data set COMPONENT ANALYSIS The experiments use four stacked hourglass networks as baselin to evaluate the efficacy of the combination of attention mechanism and adversarial networks (Pound et al., 2017). The proposed components, namely, the attention model and the adversarial networks, are analyzed by comparing their F1 scores based on the baseline networks. Figure shows the performance of the proposed components. Attention model The holistic attention model is added to the end of each hourglass stack. The model yields 84.9% F1 score, which is 0.5% higher than the baseline networks. Adversarial network The self-adversarial networks are used to repl ace the baseline networks to evaluate their efficacy. The addition of self-adversarial networks improved the F1 score by 1.3%. Figure 8. Comparisons of self-adversarial tr aining, attention model and baseline network 11 CONCLUSION In this study, attention model and adversarial networks are combined for multi-task wheat phenotyping. The adversarial networks displayed the same architecture as the generator. Serving as critic, the adversar ial networks distinguish the unreasonable locations of ears and spikelets and provide useful hints for generator to improve heatmaps. Attention model drives the proposed networks to concentrate on complex wheat ear configurations. The proposed method is evaluated using the ACID data set. The experimental results verify the effectiveness of the proposed method. The network structure that combines the attention model with the adversarial networks is also helpful in realizing other visual tasks. ACKNOWLEDGEMENTS This work was supported by the National Natural Science Fo undation of China under Grant 61672032. At the same time, thanks to Michael Pound for providing his data set to facilitate the next work of the paper. REFERENCES Ba, J., Mnih, ., Kavukcuoglu, K. (2015). Multipl object recognition with visual attention. Int. Conf. of Learning Representation. Bahdanau, D., Cho, K., Bengio, . (2015). Neural machine translation by jointly learning to align and translate. Int. Conf. of Learning Representation. Chou, C. J., Chien, J. T., Chen, H. T. (2018). Self adversarial training for human pose estimation. Asia-Pacific Signal and Information Process. Association Annual Summit and Conf. IEEE. Choudhury, S. D., Stoer ger, ., Samal, A., Schnable, J. C., Liang, Z., Yu, J. G. (2016). Automated vegetative stage phenotyping analysis of maize plants using visible light images. KDD Workshop on Data Science for Food, Energy and Water. Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A. L., Wang, X. (2017). Multi-context attention for human pose estimation. Proc. Conf. on Computer Vision and Pattern Recognition. IEEE. Fernandez-Gallego, J. A., Kefauver, S. C., Gutiérrez, N. ., Nieto-Taladriz, M. T., Araus, J. L. (2018). Wheat ear counting in-field conditions: high throughput and low-cost approach using RGB images. Plant Methods 14(1), 22. Fuentes, A., Yoon, S., Kim, S., Park, D. (2017). robust deep-learning-based detector for real-time tomato 12 plant diseases and pests recognition. Sensors 17(9), 2022. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Bengio, . (2014). Generative adversarial nets. Int. Conf. on Neural Information Process. Systems MIT Press. Hang, S. T., Tatsuma, A., Aono, M. (2016). Bluefiel (KDE TUT) at LifeCLEF 2016 Plant Identification Task. Conf. and Labs of the Evaluation Forum. Hu, J., Chen, Z., Yang, M., Zhang, R., Cui, . (2 018). multiscale fusion convolutional neural network for plant leaf recognition. IEEE Signal Process. Letters 25(6), 853-857. Hu, G., Wu, H., Zhang, Y., Wan, M. (2019). low shot learning method for tea leaf's disease identification. Comput. Electron. Agric. 163, 104852. Kuen, J., Wang, Z., Wang, G. (2016). Recurrent attentional networks for saliency detection. Proc. Conf. on Computer Vision and Pattern Recognition IEEE. Luc, P., Couprie, C., Chintala, S., Verbeek, J. 2016). Semantic segmentation us ing adversarial networks. Conf. and Workshop on Neural Information Process. Systems Perrin, G., Descombes, X., Zerubia, J. (2005, September). marked point process model for tree crown extraction in plantations. Int. Conf. on Image Process. IEEE. Pound, M. P., Atkinson, J. A., Wells, D. M., Pridmore, T. P., French, A. P.(2017). Deep learning for multi-task plant phenotyping. Proc. Int. Conf. on Computer Vision IEEE. Radford, A., Metz, L., Chintala, S. (2015). Unsuperv ised representation learning with deep convolutional generative adversarial networks. Computer Science Shi, ., Wang, N., Taylor, R. K., Raun, W. R., Hardin J. A. (2013). Automatic co rn plant location and spacing measurement using laser line-scan technique. Precision Agriculture 14(5), 478-494. Singh, A. K., Ganapathysubramanian, B., Sarkar, ., Singh, A. (2018). Deep learning for plant stress phenotyping: trends and future perspectives. Trends in Plant Science 23(10),883-898. Valerio Giuffrida, M., Scharr, H., Tsaftaris, S. A. (2017). ARIGAN: synthetic Arabidopsis plants using generative adversarial network. Proc. Int. Conf. on Computer Vision IEEE. Zhou, C., Liang, D., Yang, X., Yang, H., Yue, J., Yang, G. (2018). Wheat ears counting in field conditions 13 based on multi-feature optimization and TWSVM. Frontiers in Plant Science 9, 1024. Zhou, H., Miao, H., Li, J., Jian, F., Jayas, D. S. (20 19). low-resolution image restoration classifier network to identify stored-grain insects from images of sticky boards. Comput. Electron. Agric. 162, 593-601. Template for ASABE Journal Authors Author First name or initial Middle name or initial Surname Suffix (Jr., III, etc.) Role (job title, etc.) Email (and phone for contact author) Contact author? yes or no Gensheng Hu Professor hugs2906@sina.com yes Affiliation for Author Organization Address Country URL or other info. National Engineering Research Center for Agro-Ecological Big Data Analysis Application, Anhui University No. 111 Jiulong Road, Economic and Technological Development Zone, Hefei, Anhui, China China http://ae.ahu.edu.cn/ Author (repeat Author and Affiliation tables for each author) First name or initial Middle name or initial Surname Suffix (Jr., III, etc.) Role (job title, etc.) Email (and phone for contact author) Contact author? yes or no Lidong Qian Graduate Student 2478008518@qq.com no Affiliation for Author Organization Address Country URL or other info. School of Electronics and Information Engineering, Anhui University No. 111 Jiulong Road, Economic and Technological Development Zone, Hefei, Anhui, China China http://dy.ahu.edu.cn/ Author (repeat Author and Affiliation tables for each author) First name or initial Middle name or initial Surname Suffix (Jr., III, etc.) Role (job title, etc.) Email (and phone for contact author) Contact author? yes or no Dong Liang professor dliang@ahu.edu.cn no Affiliation for Author Organization Address Country URL or other info. School of Electronics and Information Engineering, No. 111 Jiulong Road, Economic and Technological Development Zone, China http://dy.ahu.edu.cn/ 14 Anhui University Hefei, Anhui, China Author (repeat Author and Affiliation tables for each author) First name or initial Middle name or initial Surname Suffix (Jr., III, etc.) Role (job title, etc.) Email (and phone for contact author) Contact author? yes or no Mingzhu Wan undergraduate 16307130110@fudan.edu.cn no Affiliation for Author Organization Address Country URL or other info. School of Information Science and Technology, Fudan University No. 220 Handan Road, School of Information Science and Technology, Fudan University, Shanghai, China China http://www.it.fudan.edu.cn/