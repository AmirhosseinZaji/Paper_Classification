agronomyArticleA Mixed Data-Based Deep Neural Network toEstimate Leaf Area Index in Wheat Breeding TrialsOrly Enrique Apolo-Apolo, Manuel Pérez-Ruiz, Jorge Martínez-GuanterandGregorio Egea *Area of Agroforestry Engineering, Technical School of Agricultural Engineering (ETSIA), Universidad de Sevilla.Ctra. Utrera km 1, 41013 Sevilla, Spain; eapolo@us.es (O.E.A.-A.); manuelperez@us.es (M.P.-R.);martinezj@us.es (J.M.-G.)*Correspondence: gegea@us.es; Tel.:+34-954-486-421Received: 27 November 2019; Accepted: 16 January 2020; Published: 26 January 2020/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046Abstract:Remote and non-destructive estimation of leaf area index (LAI) has been challenge inthe last few decades as the direct and indirect methods available are laborious and time-consuming.The recent emergence of high-throughput plant phenotyping platforms has increased the need todevelop new phenotyping tools for better decision-making by breeders. In this paper, novel modelbased on artiﬁcial intelligence algorithms and nadir-view red green blue (RGB) images taken from aterrestrial high throughput phenotyping platform is presented. The model mixes numerical datacollected in wheat breeding ﬁeld and visual features extracted from the images to make rapid andaccurate LAI estimations. Model-based LAI estimations were validated against LAI measurementsdetermined non-destructively using an allometric relationship obtained in this study. The modelperformance was also compared with LAI estimates obtained by other classical indirect methodsbased on bottom-up hemispherical images and gaps fraction theory. Model-based LAI estimationswere highly correlated with ground-truth LAI. The model performance was slightly better than thatof the hemispherical image-based method, which tended to underestimate LAI. These results showthe great potential of the developed model for near real-time LAI estimation, which can be furtherimproved in the future by increasing the dataset used to train the model.Keywords:plant phenotyping; leaf area; index estimation; artiﬁcial intelligence; wheat; breeding;crop monitoring1. IntroductionThe world’s population is expected to increase to about billion people by 2050 [1]. In this context,food production will need to increase by 70% despite the limited availability of arable lands, theincreasing need for fresh water and the impact of climate change [2]. This increase will have to occurin all crops, but especially in those staple crops such as wheat, which provide about 20% of the dailycaloric intake for human being [3]. Thus, the demand for wheat is expected to increase by around60% in the next few decades, but the potential yield is expected to decrease by 20% due to the climatechange [4]. Added to this perspective is the fact that breeders have only achieved an increase in wheatyield of less than 1% per year while forecasts demand an increase of around 2% to meet the growingglobal food demand [5,6].Conventional breeding cycle takes years (even decades) to develop and evaluate new breedinglines [7]. The main reason for this is that new cultivars need to be tested at multiple locationsover several years to make sure the target environments are covered [8]. large part of the workdone by breeders consists of evaluating cultivars in the ﬁeld by taking data manually to supportdecision-making [9]. This process is costly and time-consuming, since measurements are carriedAgronomy2020,10, 175; doi:10.3390/agronomy10020175www.mdpi.com/journal/agronomyAgronomy 2020,10, 175 of 21out over thousands of small plots which require time and specialized technicians 2]. To bridge thisgap, breeders demand solutions which will allow them to evaluate cultivars in the ﬁeld faster andcheaper 10]. In this context, high-throughput plant phenotyping using aerial and terrestrial platformshas emerged as promising tool 11,12]. This kind of platform allows accurate remote sensing ofcrop responses to abiotic and biotic stresses 13,14]. Moreover, automated phenotyping platformsusually measures in non-destructive way, allowing breeders to evaluate the same plant multipletimes throughout the season 15].Among the large number of phenotypes of interest to be monitored in breeding programs,biophysical crop variables are considered especially important 16]. These variables provide informationabout crop’s health status since they are ↵ected by both physical and biological crop traits that arein turn inﬂuenced by biotic and abiotic stresses 17]. The leaf area index (LAI), biomass, plant height,the fraction of vegetation cover and the fraction of absorbed photosynthetically active radiation (FPAR)are some of these variables of interest 18]. The LAI is deﬁned as the leaf area of the canopy per unitarea projected on the soil 19,20], or as the photosynthetically active leaf area per unit soil area 21,22].Its value is extremely descriptive as an indicator of the ability of the canopy to intercept incomingphotosynthetically active radiation (PAR) 23]. Moreover, due to carbon being ﬁxed by the interceptionof radiation and then converted into chemical energy, this index can be used to estimate the crop ﬁnalyield 24]. The LAI is also ↵ected by abiotic stresses such as drought and is good tool for evaluatingthe growth and development of crops in breeding programs 25].Methods for LAI estimation are grouped in direct and indirect methods, and have been widelyassessed and reviewed in the literature 19,26]. Direct methods are the most accurate and helpful forvalidating indirect measurements, but they are extremely time-consuming and prohibitively expensivewhen applied to large crop areas 20]. Moreover, their application in breeding programs would beof little use since they are often destructive and the sampled area within the small plots could benon-representative and lead to biased results 4,21]. Alternatively, in recent years, indirect methodsbased on bottom-up hemispherical photography, FPAR-inversion, spectral reﬂectance measurementsand 3D point-clouds analysis have been developed 19,22,27,28]. The essence of the indirect methodsis related to how light interacts with the canopy as measured in three ways: transmission, absorptance,and reﬂectance 29]. Among all the indirect methods available for estimating LAI, the technique thatuses hemispherical images taken with ﬁsheye-type lens is the most used due to its robustness 30,31].This method is based on the estimated position, size, density, and distribution of canopy gaps, whichcharacterize the canopy geometry through which the intercepted solar radiation is measured 32]. Thegap fraction is calculated using thresholding in order to distinguish pixels that are occupied by leavesfrom pixels that are occupied by the sky or ground 33]. Hence, images under uniformly di ↵use lightconditions to avoid sun spots in the background are required 34]. In addition to light conditions, theresolution, especially for small leaves and tall canopies, is another potential limitation 19].Despite indirect methods being quite accurate, the repeatability in image capture still requirestime-consuming post-processing 19,25]. The development of deep neural networks, also known asdeep learning (DL) techniques, subset of machine learning (ML) and, speciﬁcally, convolutionalnetworks (CNN or ConvNet), has emerged as powerful less time-consuming and less costly alternativeto estimate LAI and many others basic plant phenotyping tasks 35,36]. For example, Lue et al. 37]developed an in-ﬁeld automatic wheat disease diagnosis system based on weekly supervised deeplearning framework using red green blue (RGB) images. model to detect and characterize wheatspikes from wheat images as the input was designed by Hasan et al. 38]. Ma et al. 39] proposed adeep learning model to estimate the ground biomass of winter wheat from images captured under ﬁeldconditions. In terms of LAI estimation, Durbha et al. 40] suggested method based on support vectormachine regression (SVR) using multiangle imaging spectroradiometry of wheat. An approach thatincludes images from Sentinel 2A as input and manual measurements of LAI using an LAI-2000 PlantCanopy Analyzer (Li-Cor, Inc., Lincoln, NE, USA) was proposed by Jin et al. 41] for LAI estimationin maize microplots. Houborg and McCabe 42] applied hybrid training approach on the basis ofAgronomy 2020,10, 175 of 21two common decision tree regression algorithms (cubist and random forest models). These tools havedemonstrated high accuracy, but low scalability for terrestrial use in high-throughput phenotypingplatform (HTPP) due to the required deployment of proximal sensors with high-resolution imagingtechnologies suitable to plot sizes 43]. Nevertheless, according to Tsaftaris et al. 44], many of thetools based on images developed for plant phenotyping require prior processing, and this has severaldrawbacks which add new bottleneck to the breeding process. On the other hand, some researcherssuggest that development in the leaf area carried out during the middle and late crop growth stagesdo not produce any changes in the crop canopy cover (CC) 45]. Hence, LAI values could still beincreasing even when the crop canopy already covers approximately 70%–80% of the ground area.This means that LAI estimation using only data from image segmentation can be improved by addingother parameters that ↵ects the growth and development of plants, as recently shown by Longsonand Cambardella 46] who developed statistical model to determine LAI from ground cover andplant height measurements.Taking all this into consideration, the aim of this paper was to investigate the suitability of usingﬁeld-based nadir-view RGB images taken from high-throughput phenotyping platform to obtainhigh-accuracy LAI estimates and provide reliable data for wheat breeding programs. The speciﬁcobjectives, given this approach, were the following: (i) to develop an allometric relationship fornon-destructive yet direct estimates of LAI to be used as ground truth; (ii) to build DL model thatcombines images and numerical features of the crop for accurate LAI estimations; and (iii) to comparethe results obtained by the DL model approach with those obtained by the direct method using theallometric relationship and commonly used indirect method based on bottom-up hemisphericalimages taken underneath the vegetation.2. Materials and Methods2.1. Description of the Experimental SiteThe experiment was performed at an experimental ﬁeld located in Escacena del Campo, Huelva,Spain (latitude: 37.4525 N; longitude: 6.36194 W) that belongs to the wheat breeding company(Agrovegetal S.A., Seville, Spain). The trial was conducted during two experimental seasons (2017–18and 2018–19) under rainfed conditions. In the ﬁrst season, wheat was sown on 25 November 2017and harvested on 15 June 2018. In the second season, the wheat was sown on December 2018and harvested 183 days after sowing (DaS), that is, on June 2019. total of 10 cultivars and threereplicates per cultivar were selected from trial with 25 cultivars and three replicates per cultivarin randomized block design (Figure 1). The cultivars investigated were the following: Antequera,Conil, Galera, Gazul, Marchena, Montalb án, THA 3753, THA 3829, Tujena, and Valbona. The initialplot dimensions were 6.50 long by 1.20 wide according to the working width of the seed sowingmachine. Thirty DaS, the plots were resized along their longitudinal dimension with an herbicideapplication, setting as ﬁnal dimensions for each plot ⇥1.20 m. The weather conditions for bothgrowing seasons were dry, as is usual in the region. During the second season, single irrigation eventof 80 mm was applied on March due to an extended spring drought.Agronomy 2020,10, 175 of 21Agronomy 2020, 10, FOR PEER REVIEW of 22 Figure 1. Aerial view of the experimental site (a), aerial image of the wheat breeding trial (b), details of few individual plots (c). 2.2. Allometric Relationship and Direct Estimates of Leaf Area Index (LAI) Destructive measurements are the only way to validate LAI estimations [19]. So, during both trial seasons, three plants per plot were taken from the zone which was later resized with an herbicide application. Consequently, the conditions of the experimental unit were not affected. Then, the leaves were stored in cooler containers and immediately taken to the laboratory for analyses (Figure 2a). For each leaf, the main linear dimensions, length (L) and maximum width (W), were measured. Then, during the first season (2018), the area of the leaves was measured using leaf area meter (LI-COR 3100; LI-COR Biotechnology, Lincoln, NE, USA). In the second season (2019), the leaves were placed on sheet of paper with square of known dimensions (1 cm2) in the background, as shown in Figure 2c. The ImageJ® software was used to measure the leaf area and its dimensions, as proposed by Ahmad et al. [47]. With all this information, an allometric relationship to derive the leaf area of individual leaves from and measurements was developed. Figure 2. Data collection process for building the allometric relationship. Leaves stored in plastic bags to be transported in cooler (a); measurements of the unitary leaf area using the LICOR-3100 (b); and leaves placed on sheet of paper with the squares in the background (c). With the purpose of validating the LAI estimated with the DL model through the growing season, three randomly selected representative plants per plot were marked. On the same days that the digital photographs were taken (Table 1), the linear dimensions (L, W) of all plant leaves and the plant height were measured for each selected plant using flexible tape graduated in millimeters, as Figure 1. Aerial view of the experimental site a), aerial image of the wheat breeding trial b), details ofa few individual plots c).2.2. Allometric Relationship and Direct Estimates of Leaf Area Index (LAI)Destructive measurements are the only way to validate LAI estimations 19]. So, during bothtrial seasons, three plants per plot were taken from the zone which was later resized with an herbicideapplication. Consequently, the conditions of the experimental unit were not ↵ected. Then, the leaveswere stored in cooler containers and immediately taken to the laboratory for analyses (Figure 2a). Foreach leaf, the main linear dimensions, length (L) and maximum width (W), were measured. Then,during the ﬁrst season (2018), the area of the leaves was measured using leaf area meter (LI-COR3100; LI-COR Biotechnology, Lincoln, NE, USA). In the second season (2019), the leaves were placedon sheet of paper with square of known dimensions (1 cm2) in the background, as shown inFigure 2c. The ImageJ®software was used to measure the leaf area and its dimensions, as proposedbyAhmad et al. 47]. With all this information, an allometric relationship to derive the leaf area ofindividual leaves from and measurements was developed.Agronomy 2020, 10, FOR PEER REVIEW of 22 Figure 1. Aerial view of the experimental site (a), aerial image of the wheat breeding trial (b), details of few individual plots (c). 2.2. Allometric Relationship and Direct Estimates of Leaf Area Index (LAI) Destructive measurements are the only way to validate LAI estimations [19]. So, during both trial seasons, three plants per plot were taken from the zone which was later resized with an herbicide application. Consequently, the conditions of the experimental unit were not affected. Then, the leaves were stored in cooler containers and immediately taken to the laboratory for analyses (Figure 2a). For each leaf, the main linear dimensions, length (L) and maximum width (W), were measured. Then, during the first season (2018), the area of the leaves was measured using leaf area meter (LI-COR 3100; LI-COR Biotechnology, Lincoln, NE, USA). In the second season (2019), the leaves were placed on sheet of paper with square of known dimensions (1 cm2) in the background, as shown in Figure 2c. The ImageJ® software was used to measure the leaf area and its dimensions, as proposed by Ahmad et al. [47]. With all this information, an allometric relationship to derive the leaf area of individual leaves from and measurements was developed. Figure 2. Data collection process for building the allometric relationship. Leaves stored in plastic bags to be transported in cooler (a); measurements of the unitary leaf area using the LICOR-3100 (b); and leaves placed on sheet of paper with the squares in the background (c). With the purpose of validating the LAI estimated with the DL model through the growing season, three randomly selected representative plants per plot were marked. On the same days that the digital photographs were taken (Table 1), the linear dimensions (L, W) of all plant leaves and the plant height were measured for each selected plant using flexible tape graduated in millimeters, as Figure 2. Data collection process for building the allometric relationship. Leaves stored in plastic bagsto be transported in cooler a); measurements of the unitary leaf area using the LICOR-3100 b); andleaves placed on sheet of paper with the squares in the background c).With the purpose of validating the LAI estimated with the DL model through the growing season,three randomly selected representative plants per plot were marked. On the same days that the digitalAgronomy 2020,10, 175 of 21photographs were taken (Table 1), the linear dimensions (L, W) of all plant leaves and the plant heightwere measured for each selected plant using ﬂexible tape graduated in millimeters, as shown inFigure 3. The measured dimensions were used to calculate the leaf area of each individual leaf usingthe abovementioned allometric relationship. The plant leaf area was then calculated as the sum of allthe unitary leaf areas. The LAIfor all plots was obtained according to the following expression:LAI=PLA⇣m2⌘PS(m2)(1)where PLA is the plant leaf area and PSthe plant spacing, estimated as the inverse of plant density.Table 1. Dates of measurements and imagery acquisition during the trial seasons.Dates DaS Season25 January 61a2017–201827 March 122 2017–201817 April 143 2017–20182 February 58a,b2018–201914 February 70 2018–201925 February 81 2018–201929 March 113 2018–2019aDestructive leaf area index (LAI) measurements were performed on these dates.bOn this day, bottom-uphemispherical images were also taken. DaS: days after sowing.Agronomy 2020, 10, FOR PEER REVIEW of 22 shown in Figure 3. The measured dimensions were used to calculate the leaf area of each individual leaf using the abovementioned allometric relationship. The plant leaf area was then calculated as the sum of all the unitary leaf areas. The LAI for all plots was obtained according to the following expression: 𝐿𝐴𝐼= 𝑃𝐿𝐴 (𝑚ଶ)𝑃𝑆 (𝑚ଶ) (1) where PLA is the plant leaf area and PS the plant spacing, estimated as the inverse of plant density. Table 1. Dates of measurements and imagery acquisition during the trial seasons. Dates DaS Season 25 January 61 2017–2018 27 March 122 2017–2018 17 April 143 2017–2018 February 58 b 2018–2019 14 February 70 2018–2019 25 February 81 2018–2019 29 March 113 2018–2019 Destructive leaf area index (LAI) measurements were performed on these dates. On this day, bottom-up hemispherical images were also taken. DaS: days after sowing. Figure 3. Methodology for measuring plant height. 2.3. Image Acqusition and Processing for Deep Learning (DL) Model Most methods reviewed in the literature use single high-resolution images taken at different locations inside the crop field [29]. This provides an average LAI value of the entire crop area, which might prove to be sufficient for commercial wheat field. However, this may be not enough for the small plots used in breeding programs, since large within-plot differences in crop parameters, such as plant height, among others, are often observed [2,8]. On the other hand, it is known that video is composed of several frames (images) where each image is taken automatically, one after the other, producing the impression of moving image [48]. The number of frames per second (fps) that camera is able to obtain depends on the technical features of the sensor in question. In order to overcome the existing within-plot crop variability, breeders use replicates to increase confidence in their results [12,14]. In this study, smartphone model Huawei P8 was located at the front of an HTPP platform (Figure 4a) at height of 0.50 over the average wheat canopy height. The device was used to take nadir-view images and to record video along the entire plot (Figure 4b). The Figure 3. Methodology for measuring plant height.2.3. Image Acqusition and Processing for Deep Learning (DL) ModelMost methods reviewed in the literature use single high-resolution images taken at di ↵erentlocations inside the crop ﬁeld 29]. This provides an average LAI value of the entire crop area, whichmight prove to be su cient for commercial wheat ﬁeld. However, this may be not enough forthe small plots used in breeding programs, since large within-plot di ↵erences in crop parameters,such as plant height, among others, are often observed 2,8]. On the other hand, it is known that avideo is composed of several frames (images) where each image is taken automatically, one after theother, producing the impression of moving image 48]. The number of frames per second (fps) thata camera is able to obtain depends on the technical features of the sensor in question. In order toovercome the existing within-plot crop variability, breeders use replicates to increase conﬁdence inAgronomy 2020,10, 175 of 21their results 12,14]. In this study, smartphone model Huawei P8 was located at the front of an HTPPplatform (Figure 4a) at height of 0.50 over the average wheat canopy height. The device was usedto take nadir-view images and to record video along the entire plot (Figure 4b). The smartphone wasdriven alongside each plot at speed of 0.27 ±0.09 s1. Each digital video was recorded with anaverage recording duration of 25–30 s. The smartphone used in this experiment was able to acquire30 fps. Bearing this in mind, Python script was developed to obtain all frames from each one ofthe videos recorded. An average of 500 frames (Figure 4c) per video was obtained using this code.However, only 20 images per plot were selected to avoid including images from the perimeter of theplot (border ↵ect). Moreover, method suggested by Rosebrock 49] for image segmentation, wherecolor boundaries (lower and upper) in the RGB color space are established, was adopted to extractthe CC value for each image. More details are provided in the Supplementary Material (S1). Thisinformation was used as an input in the DL model, which will be explained in Section 2.5.Agronomy 2020, 10, FOR PEER REVIEW of 22 smartphone was driven alongside each plot at speed of 0.27 0.09 s−1. Each digital video was recorded with an average recording duration of 25–30 s. The smartphone used in this experiment was able to acquire 30 fps. Bearing this in mind, Python script was developed to obtain all frames from each one of the videos recorded. An average of 500 frames (Figure 4c) per video was obtained using this code. However, only 20 images per plot were selected to avoid including images from the perimeter of the plot (border effect). Moreover, method suggested by Rosebrock [49] for image segmentation, where color boundaries (lower and upper) in the RGB color space are established, was adopted to extract the CC value for each image. More details are provided in the Supplementary Material (S1). This information was used as an input in the DL model, which will be explained in Section 2.5. Figure 4. Image acquisition using the high-throughput phenotyping platform (HTPP). (a) picture of the HTPP platform working in the field. detail of the way the device was attached to the HTPP and frame taken with the smartphone are shown in (b) and (c), respectively. 2.4. Hemispherical Images Acquisition and Processing According to Jonckheere et al. [19], bottom-up hemispherical images can provide precise indirect estimates of LAI. This kind of image is captured using fisheye-type lens that allows taking Figure 4. Image acquisition using the high-throughput phenotyping platform (HTPP). a) picture ofthe HTPP platform working in the ﬁeld. detail of the way the device was attached to the HTPP and aframe taken with the smartphone are shown in b) and c), respectively.2.4. Hemispherical Images Acquisition and ProcessingAccording to Jonckheere et al. 19], bottom-up hemispherical images can provide precise indirectestimates of LAI. This kind of image is captured using ﬁsheye-type lens that allows taking picturesAgronomy 2020,10, 175 of 21with ﬁeld of view (FOV) of around 180 degrees. GoPro camera (Hero3 +, GoPro, San Mateo,CA, USA) was used in this work to take the bottom-up hemispherical images. To accomplish this,the camera was equipped with ﬁsheye lens with an FOV close to 170 degrees 50]. To obtain thebottom-up hemispherical images (Figure 5a) the camera was placed on the ground underneath theplants and located in the center of each plot with the interval timer shooting mode on (Figure 5b).Agronomy 2020, 10, FOR PEER REVIEW of 22 pictures with field of view (FOV) of around 180 degrees. GoPro camera (Hero3+, GoPro, San Mateo, CA, USA) was used in this work to take the bottom-up hemispherical images. To accomplish this, the camera was equipped with fisheye lens with an FOV close to 170 degrees [50]. To obtain the bottom-up hemispherical images (Figure 5a) the camera was placed on the ground underneath the plants and located in the center of each plot with the interval timer shooting mode on (Figure 5b). Figure 5. The device (Go Pro camera) used to take the bottom-up hemispherical images during the study (b) and an example of the resulting images (a). Both the videos and images were acquired according to the specifications of each device, shown in Table 2. The bottom-up hemispherical images were analyzed using the Can-Eye software (see supplementary material (S2) for further details), developed by the French National Institute of Agronomical Research (INRA), as suggested by Demarez et al. [30]. This image-processing method provides LAI and CC estimations, along with others crop parameters related to canopy architectural traits. This is laborious and time-consuming task, since manual segmentation process to differentiate between sky (background) and canopy pixels is required. Table 2. Technical features of each one of the cameras used in this study. Technical Features GoPro Huawei P8 Model HERO3 Black Edition HUAWEI GRA-UL10 Weight (g) 74 144 Sensor type CMOS CMOS Sensor size 6.17 4.55 4.62 6.16 Pixel size (µm) 1.55 1.12 Image/video resolution 4000 3000 1920 1080 Focal length (mm) 2.77 3.83 Output format JPG MP4 CMOS (complementary metal-oxide semiconductor) sensors utilize Bayer color filter mosaic arrays. 2.5. Deep-Learning Model Description Most studies carried out on LAI estimation have focused on using only visual features from the images. These methodologies are suitable in some controlled-conditions scenarios but they are time-consuming, of doubtful accuracy and labor-intensive due to the required image pre-processing. To overcome these limitations, new approach based on deep-learning modeling is proposed, mixing visual features from images and plant architectural parameters measured at ground level to train the model. According to Gulli and Pal [51], mixed-data neural networks are more complicated in structure but they are more accurate at making predictions than those using only one type of data in the Figure 5. The device (Go Pro camera) used to take the bottom-up hemispherical images during thestudy b) and an example of the resulting images a).Both the videos and images were acquired according to the speciﬁcations of each device, shownin Table 2. The bottom-up hemispherical images were analyzed using the Can-Eye software (seeSupplementary Material (S2) for further details), developed by the French National Institute ofAgronomical Research (INRA), as suggested by Demarez et al. 30]. This image-processing methodprovides LAI and CC estimations, along with others crop parameters related to canopy architecturaltraits. This is laborious and time-consuming task, since manual segmentation process to di ↵erentiatebetween sky (background) and canopy pixels is required.Table 2. Technical features of each one of the cameras used in this study.Technical Features GoPro Huawei P8Model HERO3 +Black Edition HUAWEI GRA-UL10Weight (g) 74 144Sensor type CMOScCMOScSensor size 6.17 ⇥4.55 4.62 ⇥6.16Pixel size µm) 1.55 1.12Image /video resolution 4000 ⇥3000 1920 ⇥1080Focal length (mm) 2.77 3.83Output format JPG MP4cCMOS (complementary metal-oxide semiconductor) sensors utilize Bayer color ﬁlter mosaic arrays.2.5. Deep-Learning Model DescriptionMost studies carried out on LAI estimation have focused on using only visual features fromthe images. These methodologies are suitable in some controlled-conditions scenarios but they aretime-consuming, of doubtful accuracy and labor-intensive due to the required image pre-processing.To overcome these limitations, new approach based on deep-learning modeling is proposed, mixingvisual features from images and plant architectural parameters measured at ground level to trainthe model.Agronomy 2020,10, 175 of 21According to Gulli and Pal 51], mixed-data neural networks are more complicated in structurebut they are more accurate at making predictions than those using only one type of data in the trainingprocess. Consequently, the Keras functional API (application programming interface) 52] was used inthis study to deﬁne the model. This API provides the ﬂexibility needed to deﬁne model that takesdi↵erent forms of data as inputs and subsequently combines them 51,53]. In this way, multi-inputnetwork with two branches for both images and data was built, as depicted in Figure 6. The ﬁrst branchconsisted of multilayer perceptron (MLP) with two layers: fully connected (Dense) input layer anda fully connected hidden layer, both with ReLU (Rectiﬁed Linear Unit) activation. This structure wasdesigned to handle numerical inputs (i.e., plant height, CC, DaS, etc.). The other branch was CNNwhich used RGB color images as input. This network was composed of three convolution layers placedin sequential manner with 2D convolutional neural network followed by batch normalization,ReLU activation, and max-pooling. At the end of these three convolution layers, the next layer wasﬂattened, and then fully connected (FC) layer with batch normalization, density, and dropout rateof 50% was added. The max-pooling, dropout, and dense layers were used to reduce the parametersand boost the training. Subsequently, another FC layer was applied to match the nodes coming outof the multi-layer perceptron. This latter step is not requirement, but it was added to support andbalance the branches 54]. Finally, the outputs of both branches were concatenated together to be usedas input in the ﬁnal set of layers of the network. This part of the model had an FC layer with two denselayers, where the ﬁnal one was used as regression and its output was the LAI estimation value.Agronomy 2020, 10, FOR PEER REVIEW of 22 training process. Consequently, the Keras functional API (application programming interface) [52] was used in this study to define the model. This API provides the flexibility needed to define model that takes different forms of data as inputs and subsequently combines them [51,53]. In this way, multi-input network with two branches for both images and data was built, as depicted in Figure 6. The first branch consisted of multilayer perceptron (MLP) with two layers: fully connected (Dense) input layer and fully connected hidden layer, both with ReLU (Rectified Linear Unit) activation. This structure was designed to handle numerical inputs (i.e., plant height, CC, DaS, etc.). The other branch was CNN which used RGB color images as input. This network was composed of three convolution layers placed in sequential manner with 2D convolutional neural network followed by batch normalization, ReLU activation, and max-pooling. At the end of these three convolution layers, the next layer was flattened, and then fully connected (FC) layer with batch normalization, density, and dropout rate of 50% was added. The max-pooling, dropout, and dense layers were used to reduce the parameters and boost the training. Subsequently, another FC layer was applied to match the nodes coming out of the multi-layer perceptron. This latter step is not requirement, but it was added to support and balance the branches [54]. Finally, the outputs of both branches were concatenated together to be used as input in the final set of layers of the network. This part of the model had an FC layer with two dense layers, where the final one was used as regression and its output was the LAI estimation value. Figure 6. Artificial neural network architecture for leaf area index (LAI) estimation using visual features and crop parameters as numerical inputs. Figure 6. Artiﬁcial neural network architecture for leaf area index (LAI) estimation using visual featuresand crop parameters as numerical inputs.Agronomy 2020,10, 175 of 21In order to train the model according to the neural network structure, dataset with the datacollected from six out of seven sampling dates (Table 1) was built and structured as follows: eachwheat plot was represented by groups of images tiled into one montage of images (20 imagesfor each wheat plot in total). Each montage was associated with numerical value of the CC, plantheight, average leaf area of an individual wheat leaf per plot, DaS, and LAI. total of 3600 imageswere used in the training process. Images from DaS 58 were excluded for model training and used forvalidation purposes. In order to evaluate the ↵ect on model performance of adding crop traits as inputparameters of the DL model during the training process, the model was trained in four stages: (i) themodel was trained using only RGB images, CC and LAI values, (ii) the plant height was added as anadditional parameter together with those previously used, (iii) the average leaf area of an individualwheat leaf per plot was added and (iv) the DaS was included as ﬁnal parameter to be used as inputfor model training. The number of nodes in the MPL was updated according to the inputs in eachof the stages. The dataset was split into three subsets: 70% for training, 15% for validation, and 15%for testing, as suggested by Rosebrock 54] for this kind of models. The whole model was compiledusing the mean absolute percentage error (MAPE) as loss function and the Adam’s optimizationalgorithm 55] as an optimizer with learning rate of 0.001 and decay factor of 0.001 /200. Finally,when ﬁtting the model, all the weights were tuned by backpropagation; the number of epochs usedand the batch sizes were 200 and 8, respectively.Due to CNN’s high requirements in terms of hardware and graphics processing unit (GPU)resources, Google Colaboratory (also known as Colab) ↵ered by Google was used to implementand train the model. Colab, cloud service based on Jupyter Notebooks, provides free single 12GB NVIDIA Tesla K80 GPU that can be used for up to 12 continuously. For the local computingprocesses, MacBook Pro laptop (MacOs High Sierra 10.13.4) with 2.5 GHz Intel Core i7 processor,16 GB of RAM, and AMD Radeon R9 M370X 2048 MB /Intel Iris Pro 1536 MB graphics was used. TheOpen-Source Computer Vision (OpenCV) library 56], which includes several hundred computervision algorithms, was used to process images 54].2.6. Statistical AnalysisLinear regressions were used to compare LAI values estimated using direct and indirect methods.The analysis was performed using RStudio®[57]. The mean absolute error MAE Equation (1)) andthe Root-Mean-Square Error RMSE Equation (2)) were used.MAE =1nnXt=1|AtFt| (2)RMSE =sPnt=1(AtFt)2n(3)Here, nrefers to the number of compared values, Atis the actual observed value, Ftis the forecastvalue and Atis the actual observed mean value. The RMSE andMAE represent the average di ↵erencesbetween the model Ftand the Atvalues. However, it is normalized statistic that determines therelative magnitude of the residual variance (“noise”) compared to the measured data variance (“dataor information”). It is important to include these absolute error measures in model evaluation becausethey provide an estimate of model error in the units of the variable. The MAE provides more robustmeasure of average model error than the RMSE since it is not inﬂuenced by extreme outliers 58].Finally, analysis of variance (ANOVA) was used to determine the signiﬁcant di ↵erences p<0.05)among cultivars in terms of LAI values obtained. Means separation was performed using Duncan’smultiple range test.Agronomy 2020,10, 175 10 of 213. Results and Discussion3.1. Allometric Relationships for Direct LAI EstimationObtaining an allometric relationship to estimate the leaf area of individual wheat leaves in anon-destructive way is of great interest for the validation of other non-destructive methods in trialswhere the plant material cannot be sampled. This occurs in crop breeding trials where variation in thenumber of plants per subplot would alter the productivity results of the cultivars. In this paper, twoempirical relationships have been derived (Table 3), one for each growth period. Both relationshipswere based on ﬁrst-degree equation, =ax+b, where the independent variable was calculated as theproduct of and L, and and are, respectively, the slope and intercept of the linear relationship. Thederived relationships explained 91% and 94% of the observed variability in the leaf area of individualleaves in the ﬁrst and second season, respectively. Slope values of 0.75 cm2cm2and 0.78 cm2cm2were obtained for the ﬁrst and second season, respectively. Intercept values of 0.9 cm2and 0.5 cm2were also observed for the respective two seasons. Similar results were reported by Chanda andSingh 59]. However, the number of observations in their study was slightly lower. Other researchcarried out by Calderini et al. 60] reported slope value of 0.83. The authors also made measurementsduring two seasons, although they sampled ﬁve plants per plot, likely giving most robust predictor.Bryson et al. 61] also obtained slope value of 0.83 with coe cient of determination of 0.95 in ﬁeldtrial with 20 winter wheat varieties. In this study we decided to use single relationship derived withthe pooled data of both experimental seasons, as suggested by Chanda and Singh 59].Table 3. Allometric relationships between L*W (product of length and maximum leaf width) and leafarea of individual wheat leaves (LA).Season DaSRegressionEquationR2 Standard Error[cm2]No. ofObservations2017–18 61 LA =0.75LW 0.90 0.91 0.24 4112018–18 58 LA =0.78LW +0.50 0.94 0.41 855Combined LA =0.71LW +0.92 0.94 0.09 1266DaS: Days after sawing when the leaves were collected. All the relationships were signiﬁcant at p<0.001.3.2. Spatial Canopy Cover (CC) Variability in Wheat PlotAccording to Shariﬁ 18] and Jonckheere et al. 19] among others authors cited in the literature,the main disadvantage of image segmentation techniques is their high sensitivity to the changes inlight conditions. Figure 7shows the CC values obtained by analyzing 10 frames of single wheat ploton di ↵erent DaS (61, 122 and 143) during the season 2017–18. The standard deviation (SD) of CC foreach one of the DaS assessed was 1.13, 6.95 and 3.04 %, respectively. It can be observed that the valueof SD is lower when CC is below 30% and notably higher in those DaS with CC greater than 65%.These results show that zones with di ↵erent CC values can exist within single wheat plot. Thesedi↵erences are more accentuated as the crop approaches CC values close to 80%, where techniquesused for image segmentation have di culties distinguishing background pixels from those that areactually vegetation. Therefore, we can assume that incorporating several images of the same wheatplot will give greater robustness to the model.Agronomy 2020,10, 175 11 of 21Agronomy 2020, 10, FOR PEER REVIEW 11 of 22 Figure 7. Canopy cover (CC) values for 10 frames within single wheat plot on different days after sowing (DaS) during 2017–18 season. The error bars represent the standard deviation (SD) of the data. 3.3. LAI Estimates Using Bottom-Up Hemispherical Images The reliability of using bottom-up hemispherical images to estimate the LAI of wheat plots through gap fraction analysis using the Can-Eye software was assessed in 10 wheat cultivars. LAI estimates (True LAI) with this indirect method (LAI-hemis) where evaluated against direct LAI measurements (measured LAI) (Figure 8). The level of agreement between both LAI values was high (R2 0.90, MAE 0.32, RMSE 0.40), although the indirect method tended to underestimate the LAI values as denoted by the slope of the linear regression (0.76). Other authors already stated that most indirect methods used for LAI estimation generally lead to an underestimation of LAI [62,63], in agreement with our findings. In any case, the LAI values obtained in this study were similar to those observed by Demarez et al. [30] in wheat plants using bottom-up hemispherical images. Figure 7. Canopy cover (CC) values for 10 frames within single wheat plot on di ↵erent days aftersowing (DaS) during 2017–18 season. The error bars represent the standard deviation (SD) of the data.3.3. LAI Estimates Using Bottom-Up Hemispherical ImagesThe reliability of using bottom-up hemispherical images to estimate the LAI of wheat plots throughgap fraction analysis using the Can-Eye software was assessed in 10 wheat cultivars. LAI estimates(True LAI) with this indirect method (LAI-hemis) where evaluated against direct LAI measurements(measured LAI) (Figure 8). The level of agreement between both LAI values was high R2=0.90,MAE =0.32, RMSE =0.40), although the indirect method tended to underestimate the LAI values asdenoted by the slope of the linear regression (0.76). Other authors already stated that most indirectmethods used for LAI estimation generally lead to an underestimation of LAI 62,63], in agreementwith our ﬁndings. In any case, the LAI values obtained in this study were similar to those observed byDemarez et al. 30] in wheat plants using bottom-up hemispherical images.Agronomy 2020,10, 175 12 of 21Agronomy 2020, 10, FOR PEER REVIEW 12 of 22 Figure 8. Relationship between measured LAI (direct method) and LAI estimated by bottom-up hemispherical images (LAI-hemis). The number of plots analyzed was = 30. The straight line represents the best-fit linear regression (p 0.001). 3.4. Performance of the Deep-Learning Model Although the model can be tested with the percentage (15%) of images randomly split, it was tested with an independent dataset of images taken on DaS 58. This date was selected so that the comparison of the conventional method and the DL model could be performed with images taken on the same day. Of this dataset, 10 images per plot were randomly selected and used to run the four DL model versions previously trained with images and crop traits that were sequentially added as input parameters (see Section 2.5). Figure shows the correlation between LAI measurements (direct method) and LAI estimates made by the four versions of the model (LAI-model). Although the model performed reasonably well when it was trained with images plus CC and LAI values used as inputs, the accuracy of LAI predictions increased as important parameters for the growth and development of wheat were added as inputs for model training. Correlation coefficients higher than 0.6 were obtained with all model versions. However, it was particularly significant when the accuracy increase observed when plant height and the average leaf area of individual wheat leaves were added as inputs for model training. The best performance was achieved when DaS was included as input, achieving meaningful R2 value of 0.87. Low RMSE and MAE values were observed in all model versions. Figure 8. Relationship between measured LAI (direct method) and LAI estimated by bottom-uphemispherical images (LAI-hemis). The number of plots analyzed was n=30. The straight linerepresents the best-ﬁt linear regression p<0.001).3.4. Performance of the Deep-Learning ModelAlthough the model can be tested with the percentage (15%) of images randomly split, it wastested with an independent dataset of images taken on DaS 58. This date was selected so that thecomparison of the conventional method and the DL model could be performed with images taken onthe same day. Of this dataset, 10 images per plot were randomly selected and used to run the fourDL model versions previously trained with images and crop traits that were sequentially added asinput parameters (see Section 2.5). Figure 9shows the correlation between LAI measurements (directmethod) and LAI estimates made by the four versions of the model (LAI-model) Although the modelperformed reasonably well when it was trained with images plus CC and LAI values used as inputs,the accuracy of LAI predictions increased as important parameters for the growth and development ofwheat were added as inputs for model training. Correlation coe cients higher than 0.6 were obtainedwith all model versions. However, it was particularly signiﬁcant when the accuracy increase observedwhen plant height and the average leaf area of individual wheat leaves were added as inputs for modeltraining. The best performance was achieved when DaS was included as input, achieving meaningfulR2value of 0.87. Low RMSE andMAE values were observed in all model versions.Agronomy 2020,10, 175 13 of 21Agronomy 2020, 10, FOR PEER REVIEW 13 of 22 Figure 9. Linear regressions between measured LAI (direct method) and LAI estimates made by the four model versions (LAI-model). LAI-model (i): the model was trained using only red green blue (RGB) images, canopy cover (CC) value and LAI; LAI-model (ii): the plant height was added as additional parameter together with those previously used; LAI-model (iii): the mean leaf area of individual wheat leaves was added; LAI-model (iv): the days after sowing (DaS) was included as an input to train the model. Each point is the mean of three replicates per wheat cultivar. Figure 10 shows the training and validation loss (MAPE) values of the best performing model version (iv). Loss values indicate how well or poorly certain model performs after each iteration of optimization [54]. In this case it can be observed that the mean absolute percentage error started very high but continued to fall throughout the training process. Consequently, at the end of the training process, loss value of 12.66% on the testing set was obtained. This implies that, on average, the network will be around 13% off in its LAI predictions. In this kind of model where the dataset is made up of different types of data, the weights in the neural network are randomly initialized. Hence, slightly different results will be obtained in terms of MAPE when initialization of weights is poor. Figure 9. Linear regressions between measured LAI (direct method) and LAI estimates made bythe four model versions (LAI-model). LAI-model (i): the model was trained using only red greenblue (RGB) images, canopy cover (CC) value and LAI; LAI-model (ii): the plant height was addedas additional parameter together with those previously used; LAI-model (iii): the mean leaf area ofindividual wheat leaves was added; LAI-model (iv): the days after sowing (DaS) was included as aninput to train the model. Each point is the mean of three replicates per wheat cultivar.Figure 10shows the training and validation loss (MAPE) values of the best performing modelversion (iv). Loss values indicate how well or poorly certain model performs after each iteration ofoptimization 54]. In this case it can be observed that the mean absolute percentage error started veryhigh but continued to fall throughout the training process. Consequently, at the end of the trainingprocess, loss value of 12.66% on the testing set was obtained. This implies that, on average, thenetwork will be around 13% ↵in its LAI predictions. In this kind of model where the dataset ismade up of di ↵erent types of data, the weights in the neural network are randomly initialized. Hence,slightly di ↵erent results will be obtained in terms of MAPE when initialization of weights is poor.Agronomy 2020,10, 175 14 of 21Agronomy 2020, 10, FOR PEER REVIEW 14 of 22 Figure 10. Training and validation loss values on dataset. Validation loss (val_loss) and training loss (train_loss). Since 20 images per plot were selected on DaS 58 for validation purposes, another set of 10 different images per plot were used again for LAI estimations, but this time using only LAI-model (iv) (Figure 9). Figure 11 shows the relationship between LAI measurements (direct method) and LAI estimates performed by this version of the model (LAI-model). The results show that the DL model performed reasonably well to predict the LAI of wheat, as denoted by the performance indicators: R2 0.81, MAE 0.39 and RMSE 0.49. Although these values are somewhat poorer that those obtained previously (Figure 9), it has to be noted that Figure 11 represents the data of the 30 plots evaluated instead of the mean values per cultivar as was done in Figure 9. As compared to the performance of LAI estimations with the hemispherical images and gap fraction theory (Figure 8), the slope of the LAI-model vs. LAI-measured relationship (0.94) indicates that the DL model did not underestimate LAI as previously observed with LAI-hemis. These results suggest that the model can be used as alternative to the hemispherical images, although keeping in mind the errors associated with the model predictions. Figure 10. Training and validation loss values on dataset. Validation loss (val_loss) and training loss(train_loss).Since 20 images per plot were selected on DaS 58 for validation purposes, another set of 10di↵erent images per plot were used again for LAI estimations, but this time using only LAI-model(iv) (Figure 9). Figure 11shows the relationship between LAI measurements (direct method) and LAIestimates performed by this version of the model (LAI-model). The results show that the DL modelperformed reasonably well to predict the LAI of wheat, as denoted by the performance indicators:R2=0.81,MAE =0.39 and RMSE =0.49. Although these values are somewhat poorer that thoseobtained previously (Figure 9), it has to be noted that Figure 11represents the data of the 30 plotsevaluated instead of the mean values per cultivar as was done in Figure 9. As compared to theperformance of LAI estimations with the hemispherical images and gap fraction theory (Figure 8),the slope of the LAI-model vs. LAI-measured relationship (0.94) indicates that the DL model did notunderestimate LAI as previously observed with LAI-hemis. These results suggest that the model canbe used as alternative to the hemispherical images, although keeping in mind the errors associatedwith the model predictions.Table 4shows the LAI values measured (direct method) and estimated (DL model andhemispherical images) for 10 wheat cultivars and three replicates per cultivar. The DL modelthat showed the best performance has been used in the analysis. Statistically signiﬁcant di ↵erencesin LAI-measured were observed between THA 3753 and GALERA. The two indirect methods alsofound di ↵erences between these cultivars although, in their case, signiﬁcant di ↵erences were alsofound between THA3753 and both TUJENA (LAI-hemis, LAI-model) and MONTALB ÁN (LAI-model).Alternatively, comparison between methods for each cultivar was performed. However, signiﬁcantdi↵erences were not found.Agronomy 2020,10, 175 15 of 21Agronomy 2020, 10, FOR PEER REVIEW 15 of 22 Figure 11. Relationship between LAI estimated by the deep-learning (DL) model (LAI-model) and measured LAI (direct method). Table shows the LAI values measured (direct method) and estimated (DL model and hemispherical images) for 10 wheat cultivars and three replicates per cultivar. The DL model that showed the best performance has been used in the analysis. Statistically significant differences in LAI-measured were observed between THA 3753 and GALERA. The two indirect methods also found differences between these cultivars although, in their case, significant differences were also found between THA3753 and both TUJENA (LAI-hemis, LAI-model) and MONTALBÁN (LAI-model). Alternatively, comparison between methods for each cultivar was performed. However, significant differences were not found. Table 4. LAI values determined for ten wheat cultivars with the direct and the two indirect methods assessed in this study during DaS 58 (2 February 2019). Wheat Cultivar LAI-Measured SD LAI-Hemis SD LAI-Model SD THA 3753 1.59 0.56 1.72 0.44 1.52 0.32 CONIL 2.17 ab 0.65 2.25 ab 0.13 2.02 ab 0.51 GAZUL 2.69 ab 0.28 2.54 ab 0.16 3.10 ab 0.44 MARCHENA 2.85 ab 0.92 2.59 ab 0.66 2.54 ab 1.07 THA 3829 2.99 ab 0.76 2.78 ab 0.57 3.23 ab 1.08 ANTEQUERA 3.02 ab 1.11 2.85 ab 0.72 3.43 ab 0.97 VALBONA 3.03 ab 0.38 2.91 ab 0.62 3.44 ab 0.77 MONTALBÁN 3.47 ab 1.53 3.08 ab 1.28 3.50 ab 1.00 TUJENA 3.51 ab 0.96 3.36 ab 0.76 3.63 ab 1.11 GALERA 3.77 ab 1.96 3.40 ab 1.67 3.83 ab 1.87 SD: standard deviation. LAI-measured: LAI obtained by the direct approach, LAI-hemis: LAI estimates using hemispherical images and LAI-model: LAI estimates using the DL model developed. Within the same column, different letters indicate statistically significant differences according to the Duncan’s multiple range test (p 0.05). Figure 11. Relationship between LAI estimated by the deep-learning (DL) model (LAI-model) andmeasured LAI (direct method).Table 4. LAI values determined for ten wheat cultivars with the direct and the two indirect methodsassessed in this study during DaS =58 (2 February 2019).Wheat Cultivar LAI-Measured SD LAI-Hemis SD LAI-Model SDTHA 3753 1.59 0.56 1.72 0.44 1.52 0.32CONIL 2.17 ab 0.65 2.25 ab 0.13 2.02 ab 0.51T GAZUL 2.69 ab 0.28 2.54 ab 0.16 3.10 ab 0.44MARCHENA 2.85 ab 0.92 2.59 ab 0.66 2.54 ab 1.07THA 3829 2.99 ab 0.76 2.78 ab 0.57 3.23 ab 1.08ANTEQUERA 3.02 ab 1.11 2.85 ab 0.72 3.43 ab 0.97VALBONA 3.03 ab 0.38 2.91 ab 0.62 3.44 ab 0.77MONTALB ÁN 3.47 ab 1.53 3.08 ab 1.28 3.50 1.00TUJENA 3.51 ab 0.96 3.36 0.76 3.63 1.11T GALERA 3.77 1.96 3.40 1.67 3.83 1.87SD: standard deviation. LAI-measured: LAI obtained by the direct approach, LAI-hemis: LAI estimates usinghemispherical images and LAI-model: LAI estimates using the DL model developed. Within the same column,di↵erent letters indicate statistically signiﬁcant di ↵erences according to the Duncan’s multiple range test p<0.05).3.5. Novelty of the DL Model against Current Approaches Used for Biophysical Parameters EstimationAlthough in terms of accuracy no great di ↵erences have been observed between the conventionalmethod (based on using bottom-up hemispherical images) and the DL model, the latter becomes agreat alternative to the conventional method for LAI estimation as it requires short processing times.To the best of our knowledge, this is the ﬁrst study in which DL model is trained with RGB imagesand other crop traits for estimating crop biophysical parameters, such as the leaf area index. Di ↵erentapproaches and techniques can be found in the existing literature in which only one type of data (i.e.,numerical data or images) is used. For instance, Walter et al. 64] assessed the suitability of usingcolor features in RGB images taken from HTPP for canopy cover estimations. Their results weresatisfactory under greenhouse and ﬁeld conditions. However, previous image preprocessing usingImageJ software was used, which increases notably the time required to evaluate multiple cultivars,Agronomy 2020,10, 175 16 of 21as is the case in plant breeding programs. Fern ández-Gallego 65] also developed an algorithm for arapid and accurate evaluation of the number of wheat ears as alternative to the manual counts. Themethod, based mainly on the color features of the images, achieved 90% accuracy with respect to theactual number of ears. Also for wheat, Sadeghi-Tehran et al. 66] developed deep-learning modelfor CC estimation using automatic segmentation of RGB images taken on wheat plots. The proposedmethod was more robust and accurate than other classical and machine-learning methodologies.Banerjee et al. 67] compared di ↵erent methods of supervised image classiﬁcation to estimateLAI in wheat under di ↵erent soil moisture conditions from two types of images, RGB and thermal.Their results showed that the best estimates were obtained with thermal images and using supportvector machine algorithm. coe cient of determination of 0.92 was found between estimated LAIand measured LAI with an optical device. They observed better contrast between vegetation pixelsand soil pixels in the thermal image, opening the possibility for future research of using this kind ofimages as add-on to the RGB images used in this study for estimating crop biophysical traits.Other LAI estimation methodologies have been developed as alternative to the image-basedmethodologies. Recently, Feng et al. 68] suggested new method to estimate LAI in wheat cropswhen the LAI value is under saturation due to crop growth. The method is based on vegetation indicesderived from spectral measurements and explained 78% of the LAI variability observed, slightly lowerthan the value achieved by the DL model developed in this study. Satellite images have also been usedas inputs of an artiﬁcial neural network trained with radiative transfer models, PROSPECT (leaf opticalproperties model) and SAIL (canopy bidirectional reﬂectance model), to derive LAI maps in winterwheat (Novelli et al. 69]). The authors obtained coe cient of determination higher than 0.7 in twocrop periods after comparing LAI estimates at ground level performed with the optical sensor LAI-2200Plant Canopy Analyzer and satellite-derived observations. The results were similar to those obtainedusing the model developed in this paper but, due to the low spatial resolution of satellite images, thismethodology is not applicable to the small size of plots in wheat breeding programs. Alternatively,Schirrmann 70] proposed the use of UAV images at low altitude for monitoring biophysical parametersin winter wheat.3.6. Future Research and Enhancements to Increase the Model AccuracyThe proposed model has shown promising results in terms of accuracy and suitability to beimplemented in high-throughput phenotyping platform. However, the precision of the model andthe size of the dataset must be increased in terms of pictures and ﬁeld data. This is mainly due to thefact that DL models need huge amount of input data to learn the behavior of patterns across the data.In this study, the growing season lasted 202 and 183 days for each season, respectively. Nevertheless,linear dimensions and pictures used for training the model were only taken during six days. Thiscaused wide range of LAI values to be unavailable in the dataset. Hence, greater number of LAImeasurements using the allometric relationship, especially during the ﬁrst stages of crop growth arerequired. methodology based on clusters along the plot where leaf measurements can be performedusing the allometric relationship developed could improve the accuracy of the model. This approachwill increase the number of frames used to train the model and achieve better accuracy in the LAI’spredictions. It is important to highlight that, although the best way to increase the model accuracy is toincrease the number of LAI measurements to train the model, adding other biophysical parametersobtained by optical devices (i.e., gaps size, clumping index, etc.) in the training process could alsoimprove the accuracy of the model. Moreover, due to crop growth being inﬂuenced by many factors,especially weather conditions (i.e., temperature, relative humidity), the inclusion of these parametersas numerical inputs will be considered in future research.Finally, the authors consider that this type of image analysis methodology can be transferredrapidly to the sector. The great advantage of the DL-based method, as opposed to the use of hemisphericimages, is the di ↵erence in processing time of the ﬁrst (approximately 10 s) as opposed to the secondmethod, which takes approximately 10 min per image 71] and is highly dependent on the operator’sAgronomy 2020,10, 175 17 of 21subjectivity 72]. Moreover, the fact that smartphone has been used as an RGB camera for this work,opens the possibility to develop mobile application in the future. The DL-based app would be able toestimate LAI from simple RGB images taken with conventional mobile phone by technicians or plantbreeders. Similarly, the use of these models may also be of great interest to derive crop biophysicaltraits from images taken from aerial platforms such as UAVs for rapid crop phenotyping in commercialand breeding ﬁelds.4. ConclusionsThe leaf area index (LAI) is biophysical trait of great relevance for multiple plant-relateddisciplines as it describes the plants’ performance under certain environmental conditions. Despite thee↵orts made over the last decades to develop indirect methods for its estimation in ﬁeld conditions,precise LAI estimation still presents some challenges in the ﬁeld of precision agriculture, mainly relatedto the cost and processing time required by current indirect estimation methods. In this paper noveltechnique based on artiﬁcial intelligence algorithms and RGB images taken from high-throughputphenotyping platform (HTPP) has been developed for precise and rapid LAI estimation in wheatbreeding plots. The results show that the level of agreement between the model’s LAI outputs andground truth LAI is high R2=0.81) but slightly lower than that observed with other classical indirectmethods for LAI estimation based on gaps theory analysis and hemispherical photography R2=0.90).However, the latter method underestimated LAI values (slope of the regression line of 0.76), whilethe model developed in this paper predicted LAI values of the same order of magnitude to those thatwere measured (slope of 0.94). Although the obtained results are promising, the reﬁnement of thealgorithms by training the model with an increased dataset that may also include other relevant cropparameters will probably yield better prediction outputs in future works.This kind of machine-learning model based on inexpensive RGB images opens the possibilityof estimating the LAI in wheat breeding ﬁelds in fast (real-time) and economic way, fundamentalcharacteristics for its implementation in ↵ordable high-throughput phenotyping platforms. Furtherresearch is still needed to validate its suitability to compute LAI in other crop species.Supplementary Materials: The following are available online at http: //www.mdpi.com /2073-4395 /10/2/175/s1, S1:Image segmentation method for nadir-view images using Python script; S2: Image segmentation method forhemispherical images using Can-Eye software.Author Contributions: All authors made signiﬁcant contributions to this manuscript. M.P.-R. and G.E. conceivedthe experiments and provided guidance for the analysis of data and writing of the manuscript. O.E.A.-A.performed the experiments, wrote the ﬁrst draft of the paper and analyzed data. J.M.-G. provided suggestions onthe experimental design, and participated in the analysis of data and discussion of results. All authors reviewedthe manuscript. All authors have read and agreed to the published version of the manuscript.Funding: This research was supported by the Spanish Ministry of Economy, Industry and Competitiveness(Project: AGL2016-78964-R).Acknowledgments: The authors are greatly thankful for support from the Predoctoral Research Fellowship for thedevelopment of the University of Seville R&D&I program (IV.3 2017) granted to OEAA and the Torres-Quevedocontract (PTQ-17-09506) granted to JMG by the Spanish Ministry of Economy. The authors also thank the companyAgrovegetal S.A. (Spain) for allowing the trial to be conducted in its facilities and for the technical support.Conﬂicts of Interest: The authors declare that the research was conducted in the absence of any commercial orﬁnancial relationships that could be construed as potential conﬂict of interest.References1. Foley, J.A.; Ramankutty, N.; Brauman, K.A.; Cassidy, E.S.; Gerber, J.S.; Johnston, M.; Mueller, N.D.;O’Connell, C.; Ray, D.K.; West, P.C.; et al. Solutions for cultivated planet. Nature 2011,478, 337–342.[CrossRef ][PubMed ]2. Araus, J.L.; Cairns, J.E. Field high-throughput phenotyping: The new crop breeding frontier. Trends Plant Sci.2014,19, 52–61. CrossRef ][PubMed ]3. Fischer, G. Transforming the global food system. Nature 2018,562, 501–502. CrossRef ][PubMed ]Agronomy 2020,10, 175 18 of 214. Bradshaw, J.E. Plant breeding: Past, present and future. Euphytica 2017,213, 60. CrossRef ]5. Gonzalez-Dugo, V.; Hernandez, P.; Solis, I.; Zarco-Tejada, P.J. Using high-resolution hyperspectral andthermal airborne imagery to assess physiological condition in the context of wheat phenotyping. RemoteSens. 2015,7, 13586–13605. CrossRef ]6. Reynolds, M. Climate Change Crop Production CABI: Wallingford, Oxfordshire, UK; Cambridge, MA, USA,2010; ISBN 9781845936334.7. Atlin, G.N.; Cairns, J.E.; Das, B. Rapid breeding and varietal replacement are critical to adaptation of croppingsystems in the developing world to climate change. Glob. Food Sec. 2017,12, 31–37. CrossRef ]8. Lobos, G.A.; Camargo, A.V.; Del Pozo, A.; Araus, J.L.; Ortiz, R.; Doonan, J.H. Editorial: Plant phenotypingand phenomics for plant breeding. Front. Plant Sci. 2017,8, 1–3. CrossRef ]9. Chawade, A.; Van Ham, J.; Blomquist, H.; Bagge, O.; Alexandersson, E.; Ortiz, R. High-throughputﬁeld-phenotyping tools for plant breeding and precision agriculture. Agronomy 2019,9, 258. CrossRef ]10. Mir, R.R.; Reynolds, M.; Pinto, F.; Khan, M.A.; Bhat, M.A. High-throughput phenotyping for crop improvementin the genomics era. Plant Sci. 2019,282, 60–72. CrossRef ]11. White, J.W.; Andrade-Sanchez, P.; Gore, M.A.; Bronson, K.F.; Co ↵elt, T.A.; Conley, M.M.; Feldmann, K.A.;French, A.N.; Heun, J.T.; Hunsaker, D.J.; et al. Field-based phenomics for plant genetics research. Filed CropsRes. 2012,133, 101–112. CrossRef ]12. Chapman, S.; Merz, T.; Chan, A.; Jackway, P.; Hrabar, S.; Dreccer, M.; Holland, E.; Zheng, B.; Ling, T.;Jimenez-Berni, J. Pheno-Copter: Low-Altitude, Autonomous Remote-Sensing Robotic Helicopter forHigh-Throughput Field-Based Phenotyping. Agronomy 2014,4, 279–301. CrossRef ]13. Araus, J.L.; Kefauver, S.C. Breeding to adapt agriculture to climate change: ↵ordable phenotyping solutions.Curr. Opin. Plant Biol. 2018,45, 237–247. CrossRef ][PubMed ]14. Andrade-Sanchez, P.; Gore, M.A.; Heun, J.T.; Thorp, K.R.; Carmo-Silva, A.E.; French, A.N.; Salvucci, M.E.;White, J.W. Development and evaluation of ﬁeld-based high-throughput phenotyping platform. Funct.Plant Biol. 2014,41, 68–79. CrossRef ]15. Honsdorf, N.; March, T.J.; Berger, B.; Tester, M.; Pillen, K. High-throughput phenotyping to detect droughttolerance QTL in wild barley introgression lines. PLoS ONE 2014,9, e97047. CrossRef ]16. Danner, M.; Berger, K.; Wocher, M.; Mauser, W.; Hank, T. Retrieval of Biophysical Crop Variables fromMulti-Angular Canopy Spectroscopy. Remote Sens. 2017,9, 726. CrossRef ]17. Atzberger, C.; Gu érif, M.; Baret, F.; Werner, W. Comparative analysis of three chemometric techniques for thespectroradiometric assessment of canopy chlorophyll content in winter wheat. Comput. Electron. Agric. 2010,73, 165–173. CrossRef ]18. Shariﬁ, A. Estimation of biophysical parameters in wheat crops in Golestan province using ultra-highresolution images. Remote Sens. Lett. 2018,9, 559–568. CrossRef ]19. Jonckheere, I.; Fleck, S.; Nackaerts, K.; Muys, B.; Coppin, P.; Weiss, M.; Baret, F. Review of methods for in situleaf area index determination Part I. Theories, sensors and hemispherical photography. Agric. For. Meteorol.2004,121, 19–35. CrossRef ]20. Zhu, X.; Skidmore, A.K.; Wang, T.; Liu, J.; Darvishzadeh, R.; Shi, Y.; Premier, J.; Heurich, M. Improving leafarea index (LAI) estimation by correcting for clumping and woody ↵ects using terrestrial laser scanning.Agric. For. Meteorol. 2018,263, 276–286. CrossRef ]21. Kumar Panguluri, S.; Ashok Kumar, A. Phenotyping for Plant Breeding Springer: New York, NY, USA, 2013;ISBN 978-1-4614-8319-9.22. Yan, G.; Hu, R.; Luo, J.; Weiss, M.; Jiang, H.; Mu, X.; Xie, D.; Zhang, W. Review of indirect optical measurementsof leaf area index: Recent advances, challenges, and perspectives. Agric. For. Meteorol. 2019,265, 390–411.[CrossRef ]23. Bonelli, L.E.; Andrade, F.H. Maize radiation use-e ciency response to optimally distributedfoliar-nitrogen-content depends on canopy leaf-area index. Field Crops Res. 2019,5, 107557. CrossRef ]24. Zhou, B.; Serret, M.D.; Elazab, A.; Bort Pie, J.; Araus, J.L.; Aranjuelo, I.; Sanz-S áez,Á. Wheat ear carbonassimilation and nitrogen remobilization contribute signiﬁcantly to grain yield. J. Integr. Plant Biol. 2016,58,914–926. CrossRef ][PubMed ]Agronomy 2020,10, 175 19 of 2125. Confalonieri, R.; Foi, M.; Casa, R.; Aquaro, S.; Tona, E.; Peterle, M.; Boldini, A.; De Carli, G.; Ferrari, A.;Finotto, G.; et al. Development of an app for estimating leaf area index using smartphone. Trueness andprecision determination and comparison with other indirect methods. Comput. Electron. Agric. 2013,96,67–74. CrossRef ]26. dos Santos, J.C.C.; Costa, R.N.; Silva, D.M.R.; de Souza, A.A.; de Barros Prado Moura, F.; da Silva Junior, J.M.;Silva, J.V. Use of allometric models to estimate leaf area in Hymenaea courbaril L.Theor. Exp. Plant Physiol.2016,28, 357–369. CrossRef ]27. Comba, L.; Biglia, A.; Ricauda Aimonino, D.; Tortia, C.; Mania, E.; Guidoni, S.; Gay, P. Leaf Area Indexevaluation in vineyards using 3D point clouds from UAV imagery. Precis. Agric. 2019, 1–16. CrossRef ]28. del-Moral-Mart ínez, I.; Rosell-Polo, J.R.; Company, J.; Sanz, R.; Escol à, A.; Masip, J.; Mart ínez-Casasnovas, J.A.;Arnó, J. Mapping vineyard leaf area using mobile terrestrial laser scanners: Should rows be scanned on-the-goor discontinuously sampled? Sensors 2016,16, 119. CrossRef ]29. Weiss, M.; Baret, F.; Smith, G.J.; Jonckheere, I.; Coppin, P. Review of methods for in situ leaf area index (LAI)determination. Agric. For. Meteorol. 2004,121, 37–53. CrossRef ]30. Demarez, V.; Duthoit, S.; Baret, F.; Weiss, M.; Dedieu, G. Estimation of leaf area and clumping indexes ofcrops with hemispherical photographs. Agric. For. Meteorol. 2008,148, 644–655. CrossRef ]31. Gonsamo, A.; Walter, J.M.; Chen, J.M.; Pellikka, P.; Schleppi, P. robust leaf area index algorithm accountingfor the expected errors in gap fraction observations. Agric. For. Meteorol. 2018,248, 197–204. CrossRef ]32. Liu, S.; Baret, F.; Abichou, M.; Boudon, F.; Thomas, S.; Zhao, K.; Fournier, C.; Andrieu, B.; Irfan, K.;Hemmerl é, M.; et al. Estimating wheat green area index from ground-based LiDAR measurement using a3D canopy structure model. Agric. For. Meteorol. 2017,247, 12–20. CrossRef ]33. Li, Y.; Guo, Q.; Su, Y.; Tao, S.; Zhao, K.; Xu, G. Retrieving the gap fraction, element clumping index, and leafarea index of individual trees using single-scan data from terrestrial laser scanner. ISPRS J. Photogramm.Remote Sens. 2017,130, 308–316. CrossRef ]34. Woodgate, W.; Jones, S.D.; Suarez, L.; Hill, M.J.; Armston, J.D.; Wilkes, P.; Soto-Berelov, M.; Haywood, A.;Mellor, A. Understanding the variability in ground-based methods for retrieving canopy openness, gapfraction, and leaf area index in diverse forest systems. Agric. For. Meteorol. 2015,205, 83–95. CrossRef ]35. Ubbens, J.R.; Stavness, I. Deep plant phenomics: deep learning platform for complex plant phenotypingtasks. Front. Plant Sci. 2017,8.[CrossRef ][PubMed ]36. Chen, C.H.; Kung, H.Y.; Hwang, F.J. Deep learning techniques for agronomy applications. Agronomy 2019,9,142. CrossRef ]37. Lu, J.; Hu, J.; Zhao, G.; Mei, F.; Zhang, C. An in-ﬁeld automatic wheat disease diagnosis system. Comput.Electron. Agric. 2017,142, 369–379. CrossRef ]38. Hasan, M.M.; Chopin, J.P.; Laga, H.; Miklavcic, S.J. Detection and analysis of wheat spikes using ConvolutionalNeural Networks. Plant Methods 2018,14, 1–13. CrossRef ]39. Ma, J.; Li, Y.; Chen, Y.; Du, K.; Zheng, F.; Zhang, L.; Sun, Z. Estimating above ground biomass of winterwheat at early growth stages using digital images and deep convolutional neural network. Eur. J. Agron.2019,103, 117–129. CrossRef ]40. Durbha, S.S.; King, R.L.; Younan, N.H. Support vector machines regression for retrieval of leaf area indexfrom multiangle imaging spectroradiometer. Remote Sens. Environ. 2007,107, 348–361. CrossRef ]41. Jin, X.; Li, Z.; Feng, H.; Ren, Z.; Li, S. Deep neural network algorithm for estimating maize biomass based onsimulated Sentinel 2A vegetation indices and leaf area index. Crop J. 2019.[CrossRef ]42. Houborg, R.; McCabe, M.F. hybrid training approach for leaf area index estimation via Cubist and randomforests machine-learning. ISPRS J. Photogramm. Remote Sens. 2018,135, 173–188. CrossRef ]43. Gebremedhin, A.; Badenhorst, P.E.; Wang, J.; Spangenberg, G.C.; Smith, K.F. Prospects for measurement ofdry matter yield in forage breeding programs using sensor technologies. Agronomy 2019,9, 65. CrossRef ]44. Tsaftaris, S.A.; Minervini, M.; Scharr, H. Machine Learning for Plant Phenotyping Needs Image Processing.Trends Plant Sci. 2016,21, 989–991. CrossRef ][PubMed ]45. Shaﬁan, S.; Rajan, N.; Schnell, R.; Bagavathiannan, M.; Valasek, J.; Shi, Y.; Olsenholler, J. Unmanned aerialsystems-based remote sensing for monitoring sorghum growth and development. PLoS ONE 2018,13,e0196605. CrossRef ][PubMed ]Agronomy 2020,10, 175 20 of 2146. Lonsdon, S.D.; Cambardella, C.A. An approach for indirect determination of leaf area index. Am. Soc. Agric.Biol. Eng. 2019,62, 655–659. CrossRef ]47. Ahmad, S.; Rehman, A.U.; Irfan, M.; Khan, M.A. Measuring Leaf Area of Winter Cereals by Di ↵erentTechniques: Comparison. Pak. J. Life Soc. Sci. 2015,13, 117–125.48. Barhoumi, W.; Zagrouba, E. On-the-ﬂy Extraction of Key Frames for cient Video Summarization. AASRIProcedia 2013,4, 78–84. CrossRef ]49. Rosebrock, A. Practical Python and OpenCV +Case Studies PyImageSearch.com: Baltimore, MD, USA, 2016.50. Urban, S.; Leitlo ↵, J.; Hinz, S. Improved wide-angle, ﬁsheye and omnidirectional camera calibration. ISPRS J.Photogramm. Remote Sens. 2015,108, 72–79. CrossRef ]51. Gulli, A.; Pal, S. Deep Learning with Keras Packt Publishing: Birmingham, UK; Volume 73, ISBN978-1-78712-842-2.52. Chollet, F. Keras. Available online: https: //keras.io (accessed on September 2019).53. Manaswi, N.K. Deep Learning with Applications Using Python Apress: Bangalore, India, 2018; ISBN9781484235164.54. Rosebrock, A. Deep Learning for Computer Vision with Python. ImageNet Bundle PyImageSearch.com: Baltimore,MA, USA, 2018.55. Kingma, D.P.; Ba, J.L. Adam: method for stochastic optimization. In Proceedings of the 3rd InternationalConference on Learning Representations (ICLR2015), San Diego, CA, USA, 7–9 May 2015; pp. 1–15.56. Bradski, G. The opencv library (2000). Dr. Dobb’s J. Softw. Tools 2000. Available online: https: //opencv.org /(accessed on June 2019).57. Team, R.S. RStudio: Integrated Development for ; RStudio Inc.: Boston, MA, USA, 2016; Available online:https: //www.rstudio.com /(accessed on 30 October 2019).58. David Legates, G.J.M. Evaluating the use of “goodness-of-ﬁt” measures in hydrologic and hydroclimaticmodel validation. Water Resour. Res. 2007,35, 1–9.59. Chanda, S.V.; Singh, Y.D. Estimation of leaf area in wheat using linear measurements. Plant Breed. Seed Sci.2002,46, 75–79.60. Calderini, D.F.; Miralles, D.J.; Sadras, V.O. Appearance and growth of individual leaves as ↵ected bysemidwarﬁsm in isogenic lines of wheat. Ann. Bot. 1996,77, 583–589. CrossRef ]61. Bryson, R.J.; Paveley, N.D.; Clark, W.S.; Slylvester-Bradley, R.; Scott, R.K. Use of in-ﬁeld measurements ofgreen leaf area and incident radiation to estimate the ↵ects of yellow rust epidemics on the yield of winterwheat. Dev. Crop Sci. 1997,25, 77–86. CrossRef ]62. Stroppiana, D.; Boschetti, M.; Confalonieri, R.; Bocchi, S.; Brivio, P.A. Evaluation of LAI-2000 for leaf areaindex monitoring in paddy rice. Field Crops Res. 2006,99, 167–170. CrossRef ]63. Bréda, N.J.J. Ground-based measurements of leaf area index: review of methods, instruments and currentcontroversies. J. Exp. Bot. 2003,54, 2403–2417. CrossRef ]64. Walter, J.; Edwards, J.; Cai, J.; McDonald, G.; Miklavcic, S.J.; Kuchel, H. High-throughput ﬁeld imaging andbasic image analysis in wheat breeding programme. Front. Plant Sci. 2019,10.[CrossRef ]65. Fernandez-Gallego, J.A.; Kefauver, S.C.; Guti érrez, N.A.; Nieto-Taladriz, M.T.; Araus, J.L. Wheat ear countingin-ﬁeld conditions: High throughput and low-cost approach using RGB images. Plant Methods 2018,14, 1–12.[CrossRef ]66. Sadeghi-Tehran, P.; Virlet, N.; Sabermanesh, K.; Hawkesford, M.J. Multi-feature machine learning model forautomatic segmentation of green fractional vegetation cover for high-throughput ﬁeld phenotyping. PlantMethods 2017,13, 1–16. CrossRef ]67. Banerjee, K.; Krishnan, P.; Mridha, N. Application of thermal imaging of wheat crop canopy to estimate leafarea index under di ↵erent moisture stress conditions. Biosyst. Eng. 2018,166, 13–27. CrossRef ]68. Feng, W.; Wu, Y.; He, L.; Ren, X.; Wang, Y.; Hou, G.; Wang, Y.; Liu, W.; Guo, T. An optimized non-linearvegetation index for estimating leaf area index in winter wheat. Precis. Agric. 2019,20, 1157–1176. CrossRef ]69. Novelli, F.; Vuolo, F. Assimilation of sentinel-2 leaf area index data into physically-based crop growthmodel for yield estimation. Agronomy 2019,9, 255. CrossRef ]70. Schirrmann, M.; Giebel, A.; Gleiniger, F.; Pﬂanz, M.; Lentschke, J.; Dammer, K.H. Monitoring agronomicparameters of winter wheat crops with low-cost UAV imagery. Remote Sens. 2016,8, 706. CrossRef ]Agronomy 2020,10, 175 21 of 2171. Weiss, M.; Baret, F. Can_Eye V6.4.91 User Manual. Available online: https: //www6.paca.inra.fr /can-eye /News /CAN-EYE-V6.49-Release (accessed on May 2019).72. Duveiller, G.; Defourny, P. Batch processing of hemispherical photography using object-based image analysisto derive canopy biophysical variables. In Proceedings of the GEOBIA 2010: Geographic Object-Based ImageAnalysis, Ghent, Belgium, 29 June–2 July 2010; Volume XXXVIII-4 /C7.©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open accessarticle distributed under the terms and conditions of the Creative Commons Attribution(CC BY) license http: //creativecommons.org /licenses /by/4.0/).