Contents lists available at ScienceDirectComputers and Electronics in Agriculturejournal homepage: www.elsevier.com/locate/compagOriginal papersCrop yield prediction with deep convolutional neural networksPetteri Nevavuorib,/uni204E, Nathaniel Narraa, Tarmo LippingaaTampere University of Technology, FinlandbMtech Digital Solutions Oy, FinlandARTICLE INFOKeywords:Crop yield predictionConvolutional neural networkWheatBarleyUAVMultispectralNDVIGrowth phaseABSTRACTUsing remote sensing and UAVs in smart farming is gaining momentum worldwide. The main objectives are cropand weed detection, biomass evaluation and yield prediction. Evaluating machine learning methods for remotesensing based yield prediction requires availability of yield mapping devices, which are still not very commonamong farmers. In this study Convolutional Neural Networks (CNNs) –a deep learning methodology showingoutstanding performance in image classi /uniFB01cation tasks –are applied to build model for crop yield predictionbased on NDVI and RGB data acquired from UAVs. The /uniFB00ect of various aspects of the CNN such as selection ofthe training algorithm, depth of the network, regularization strategy, and tuning of the hyperparameters on theprediction /uniFB03ciency are tested. Using the Adadelta training algorithm, L2regularization with early stopping anda CNN with convolutional layers, mean absolute error (MAE) in yield prediction of 484.3 kg/ha and meanabsolute percentage error (MAPE) of 8.8% was achieved for data acquired during the early period of the growthseason (i.e., in June of 2017, growth phase 25%) with RGB data. When using data acquired later in July andAugust of 2017 (growth phase 25%), MAE of 624.3 kg/ha (MAPE: 12.6%) was obtained. Signi /uniFB01cantly, theCNN architecture performed better with RGB data than the NDVI data.1. IntroductionDevelopment-minded farmers have practiced what is now known asprecision agriculture long before the dawn of the computing age. Theywere able to deduce sources of /uniFB01eld variability and the actions to takefor trying to secure an enhanced level of crop yields. The farmers ac-complished this by taking notes of their /uniFB01elds during growing seasonsand harvest time operations and tried to /uniFB01gure out the best actions forthe year to come based on the accumulated knowledge and experience.However, as studied by Wolfert et al. (2017) the increase in data-producing devices and sensors has been an on-going trend in agri-culture having enabled the farmers to shift towards data-driven deci-sion-making. This is commonly called smart farming. comprehensivereview of various objectives and techniques used in smart farming canbe found in Kamilaris et al. (2017) .An important trend in smart farming is the use of remote sensing tofacilitate the extraction of information relevant for data-driven deci-sions Miyoshi et al., 2017; Matikainen et al., 2017 ). Remote sensingdata can be acquired from satellites such as ESA ’s Sentinel-2A, for ex-ample. The problem with the satellite data is that if there is cloudcover during the over /uniFB02ight of the satellite, no useful data are obtained.The spatial resolution of Sentinel imagery is at best 10 m, which isenough for many applications but too low to allow using texture-basedinformation in the images. Satellite data contains prede /uniFB01ned wave-length bands from both the visible and the Near Infrared (NIR) spectralregions. In satellite-borne sensors, designed keeping in mind agri-cultural applications, the spectral bands are optimized for the calcula-tion of relevant indices such as the Normalized Di /uniFB00erence VegetationIndex (NDVI), for example. The spatial and temporal resolution of sa-tellite data will improve in years to come, however, cloud cover willremain an obstacle, especially in northern climate.Using Unmanned Aerial Vehicles (UAVs), or drones, for data ac-quisition /uniFB00ers better spatial resolution, the data acquisition time canbe selected by the user and the data can be acquired also in cloudyconditions. Spectral wavelengths can be selected by using appropriatecamera; UAV-mountable RGB-NIR cameras are available at /uniFB00ordableprice. The drawback is that the UAV has to be operated locally andmanaging the data and extracting relevant information requires highlyspecialized skills. As the variety of UAVs and UAV-mountable sensors ishigh compared to satellite-borne sensors, analysis frameworks andservices based on UAV-borne data are not yet equally developed. InNäsi et al. (2017) extraction of information related to the biomass andnitrogen content of vegetation (barley and grass) in test /uniFB01elds usingvarious modalities of remote sensing data (satellite/aircraft/droneusing RGB/multispectral/hyperspectral sensors) has been considered.Information relevant for decision making in agriculture can behttps://doi.org/10.1016/j.compag.2019.104859Received 17 April 2019; Accepted 13 June 2019/uni204ECorresponding author.E-mail addresses: petteri.nevavuori@mtech. /uniFB01(P. Nevavuori), nathaniel.narra@tuni. /uniFB01(N. Narra), tarmo.lipping@tuni. /uniFB01(T. Lipping).&RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH$YDLODEOHRQOLQH-XO\(OVHYLHU%9$OOULJKWVUHVHUYHG7extracted from remote sensing data by means of machine learning.Traditional machine learning techniques involve feature extraction asan initial stage. Based on the features, di /uniFB00erent tasks such as cropclassi /uniFB01cation, weed detection or yield prediction can be addressed. InRuß (2009) several traditional machine learning techniques have beenapplied to the task of yield prediction. It is, however, often di /uniFB03cult to/uniFB01nd optimal features and the ability of the traditional methods to learnfrom the data is limited. With advancements in computational tech-nology, the development and training of novel multilayer algorithmshas become feasible. These methods are commonly referred to as deeplearning. Among the various deep learning paradigms, ConvolutionalNeural Networks (CNNs) have proved especially /uniFB03cient in imageclassi /uniFB01cation and analysis. In case of CNNs no features need to be pre-calculated as the feature extraction operation is performed by theconvolutional layers of the network and optimal features are obtainedin the course of training. Due to this kind of structure, CNNs requirelarge amounts of training data to converge. The advantage of CNNscompared to traditional machine learning methods in crop yield pre-diction is discussed, for example, in You et al. (2017) CNNs have beensuccessfully applied to crop classi /uniFB01cation Chunjing et al., 2017 andweed detection Sa et al., 2017; Milioto et al., 2017 ).In working towards an /uniFB00ective in-season crop yield predictormodel for the northern climate, our /uniFB00ort in this preliminary study is todevelop CNN based deep learning framework using UAV-acquiredmultispectral data. RGB and NDVI images, representing patches ofwheat and barley /uniFB01elds, are fed as input data to CNN and training isperformed to tune the network parameters. In addition to testing theusefulness of deep learning models for crop yield prediction in general,we also experiment with various setups and training schemes of theCNN model. Training deep learning network is typically an iterativeprocess as there is substantial number of cross-related parameters totune. We /uniFB01rst select the most promising training algorithm from threecandidates (see Section 3.1) and determine the optimal number ofconvolutional layers of the CNN. After that, we optimize the perfor-mance of the network in terms of regularization and parameters of thetraining algorithm. The optimized framework is evaluated using twotypes of input data (RGB and NDVI) and three patch sizes (10, 20 and40 m).2. Data and methods2.1. Data acquisitionThe nine crop /uniFB01elds selected for this study are located in the vicinityof the city of Pori (61°29 ′6.5″N, 21°47 ′50.7″E). The total area of the/uniFB01elds was approximately 90 ha. The main crops grown in the /uniFB01eldswere wheat and malting barley, however the model was trained overthe/uniFB01elds without making distinction between the crop type.Multispectral data were acquired from these /uniFB01elds during thegrowing season of 2017 (i.e., from June to August; see Table ). Thedata were collected with single Airinov Solo 3DR UAV equipped withParrot ’s NIR-capable SEQUIOA-sensor. The images of individual spec-tral bands were stitched together to form complete orthogonal RGB andNDVI rasters of distinct /uniFB01eldsusing the Pix4D software.The UAV data were organized into two sets according to the time ofdata acquisition to see if the phase of the growing season had an /uniFB00ecton predicting the yield from the input image. Growing phase here isde/uniFB01ned as the percentage of total thermal time on the day of imaging.Thermal time for each day was calculated as the magnitude of dailyaverage temperature above °5C. The temperature readings weredownloaded from the Finnish Meteorological Institute. Beginning ofJuly 2017 was chosen as the separating time point between the twodata sets as the UAV data dispersed equally enough around that date.The data sets containing images only prior to July 2017 were labeled asearly (growth phase 25% of the total thermal time) and the re-maining data as late(growth phase 25% of the total thermal time).Details of the /uniFB01elds, crops, imaging dates and corresponding growthphases are listed in Table .The /uniFB01eld-wise image data were then processed using slidingwindow to extract geolocationally matched pairs of input image frames(UAV data) and targets (yield data) of prede /uniFB01ned size from all the/uniFB01elds. The step of the applied sliding window was chosen to be 10 maccording to the resolution of Sentinel-2A satellite data considering thepossibility of using satellite data as an additional input to the networkin future studies. Image frames of sizes 10 ×10 m, 20 ×20 and40 ×40 were considered. The resolution of the UAV data was0.3125 or 32 pixels per 10 m. The overall number of extracted framesaccording to crop /uniFB01elds is given in Table . The individual data frameswere treated as independent inputs fed to the CNN models. The processof data preparation prior to and during training is illustrated in Fig. .The harvest yield data was acquired during September 2017 usingtwo distinct setups attached to the harvesters: Trimble CFX 750 andJohn Deere Greenstar 1. As the yield measurement devices produce anirregular set of data points with multiple attributes, the data had to beprocessed to be handled as rasters of /uniFB01eld-wise yield from the viewpointof the trainable network. The data points were /uniFB01rst/uniFB01ltered according to(Tiusanen, 2017 to preserve only points corresponding to harvesterspeed between and km/h and yield between 1500 and 15,000 kg/ha. The /uniFB01ltering and generation of rasterizable vector /uniFB01les was doneusing the FarmWorks software. The /uniFB01eld-wise vector data /uniFB01les werethen rasterized by interpolating them using an exponential point-wiseinverse distance algorithm. Yield values constitute targets the model istrying to predict during the training of the CNNs. Thus, yield valueswere also extracted using sliding windows similar to the UAV images tohave geolocationally matching pairs of inputs and targets. Yield valueswere then averaged over the analysis window to obtain scalar targetvalues. The histograms and statistics of yield values for point data aswell as window-averaged data using three sample area window sizes(10 m, 20 and 40 m) over all crop /uniFB01elds are given in Fig. . As can beexpected, the larger the window, the more concentrated the yield va-lues are around the mean.For clarity, we also visualize several NDVI and RGB input images ofthe largest sample area window size (40 m) with their correspondingyields in Fig. with the color bar corresponding to yield image valuerange. The images with similar identi /uniFB01ers are from the same location.However, the target for the network will be the mean of the yield valuesover the analysis window corresponding to the input area. It is alsoimportant to note that the network was trained separately for RGB andNDVI input images so that the possible misalignment between the twoimage sources does not /uniFB00ect prediction results. This kind of approachenables us to evaluate which one of the two input sources, RGB orNDVI, gives better prediction results.2.2. Building the convolutional neural networkConvolutional neural networks, or CNNs, are deep learning modelsspecialized in handling grid-like data. Such data can be images or rowsof multi-column data. Deep learning refers to models composed ofmultiple layers. Generally, model is viewed as deep if it has at least aninput layer, one hidden layer and an output layer. The term neural onthe other hand refers to the fact that originally the operation principleof arti /uniFB01cial neural networks was taken from that of the brain, con-taining neurons as its basic building blocks. Compared to traditionalfeedforward neural networks, CNNs possess some special featuresmaking them extremely /uniFB03cient in /uniFB01nding salient features within thedata. Some of these features are:1. exploitation of the convolution operation2. post-convolution pooling3. speci /uniFB01c non-linear activation functions.In the following we provide brief description of these elementsP. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHwith additional information on other key elements of CNNs such asbatch normalization and regularization. We evaluated various setups ofthese CNN elements to /uniFB01nd the best-performing algorithm and assess itsperformance in crop yield prediction.2.2.1. Convolution operationThe convolution operation is the /uniFB01rst of multiple transformationsperformed in convolutional layer of CNNs. Generally, the convolutionoperation can be described as calculating the sum of products betweena set of input values and values of convolutional kernel also called a/uniFB01lter. In CNN, the kernel values are trained to /uniFB01nd optimal features fromthe point of view of the task to be solved (in our case, predicting cropyield). The operating principle of the kernel is depicted in Fig. and theposition of convolutional layers in the overall structure of the CNN usedin this study can be seen from Fig. .2.2.2. Batch normalizationWhile not requirement for CNNs, the state-of-the-art is to applybatch normalization Io/uniFB00e and Szegedy, 2015 as constituent of deeplearning model layers. Batch normalization is an optimization strategyTable 1Details of crops and their varieties sown in each of the /uniFB01elds in 2017. Thermal times for each crop variety are taken from report published by Laine et al. (2017) .Sowing dates and imaging dates are used to calculate the growth phase as fraction of the total thermal time for the crop variety. Images with dates prior to 1st ofJuly form the early data set and the remaining images the late one.Field Size (ha) Mean yield (kg/ha) Crop Variety Thermal time Sowing date Imaging date Growth phase1 5.96 5098 Wheat Zebra 1052 10 May 17 Aug 83%2 10.26 6054 Barley Trekker 979.7 16 May Jun 15%27 Jul 64%3 2.97 8971 Barley Trekker 979.7 17 May Jun 15%27 Jul 64%4 13.05 4673 Barley RGT Planet 982.2 15 May Jul 42%5 4.66 6482 Barley Propino 981.4 15 May 15 Jun 22%6 7.29 6884 Barley Propino 981.4 15 May 15 Jun 22%7 10.92 7568 Barley Harbinger 976.3 24 May Jul 36%8 15.28 7585 Barley Trekker 979.7 18 May Jun 10%13 Jul 49%9 18.86 6991 Wheat KWS Solanus 1065 13 May 15 Jun 21%6 Jul 72%Table 2Number of data frames extracted from each /uniFB01eld using frame sizes of 10 m,20 and 40 m. The number of frames decreases slightly with increasing framesize due to /uniFB01eld edge /uniFB00ects.Field 10 ×10 dataframes20 ×20 dataframes40 ×40 dataframesMean dataframe count1 761 745 735 7472 1102 1159 1150 11373 783 731 691 7354 1494 1486 1454 14785 610 586 590 5956 942 931 916 9307 1240 1247 1224 12378 3736 3786 3812 37789 4556 4548 4520 4541/uni2211 15224 15219 15092 15178Fig. 1. All nine /uniFB01elds were /uniFB01rst split to overlapping data frames of sizes 10 m, 20 and 40 m. dedicated holdout test data set was then built from 15% of shu /uniFB04eddata frames; these data were never presented to the model during training. The remaining 85% of data frames were then used for training the models with k-FoldCross Validation. After the training phase of each model was completed, the test errors were calculated using the holdout test data set to validate the performance ofthe trained model.P. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHfor training deep models more /uniFB03ciently. Batch refers to subset oftraining data used for updating the model parameters (including kernelvalues) at single iteration, albeit the term mini-batch is generally usedto distinguish the whole data set (batch) from it ’s subset (mini-batch). Ithas been shown that normalizing the network layers for each batch (ormini-batch) of data stabilizes the learning, allowing to use higherlearning rates and thus resulting in faster learning Goodfellow et al.,2016 ). There are di /uniFB00erent implementations of batch normalization; theimplementation used in the CNN of this study follows Eq. (1), where xisa mini-batch of activations,/uni220Aa non-signi /uniFB01cant constant to preventnumerical under /uniFB02ow,γis the momentum and bis layer-wise bias:=/uni2212+/uni220A/uni2217+ yxµσγb.xx (1)2.2.3. Max poolingThe convolution operation is usually followed by pooling. Poolingmeans grouping of adjacent values using selected aggregation func-tion, which in our case was taking the maximum (hence max pooling)over the neighboring values within prede /uniFB01ned window. The step sizeof moving this window along the feature map is called stride Poolinge/uniFB00ectively diminishes the input image dimensions making the detectedfeatures more coarse and thus more robust to small variations(Goodfellow et al., 2016 ). The amount of dimension reduction is con-trolled by the stride parameter. The stride dictates how many applica-tions of the pooling window are performed. An example of max poolingis given in Fig. and the position of pooling in the overall structure ofthe CNN used in this study can be seen from Fig. .2.2.4. Recti /uniFB01ed linear unitsA key element in any neural network is the layer-wise activationfunction of the neurons. variety of activation functions have beendesigned, but the use of the recti /uniFB01ed linear function in the activationunits is the current standard for CNNs He et al., 2015; Goodfellowet al., 2016 ). Activation units employing recti /uniFB01ed linear functions arecommonly referred to as ReLUs. The operating principle of this acti-vation function is to allow only positive inputs to proceed linearly andis depicted in Fig. . We too use ReLUs as the activation functions inboth the convolutional as well as the fully connected layers (see Fig. ).2.2.5. Fully connected layersThe convolutional layers of CNN extract salient features frominput images, i.e., factors with highest descriptive power regarding thedata producing process. To utilize the learned features in regression ora classi /uniFB01cation task, they have to be successfully mapped to targetvalue. This is performed typically by adding fully connected (FC) layersafter the convolutional layers. The term fully connected refers to theprinciple that in these layers, each neuron (or unit) of the previous layerhas connection to each unit of the layer in question. Increasing thenumber of FC layers increases the capacity of the network to learn themapping between the features and the target. It also increases theburden of optimization, as in FC layers the number of connectionsgrows exponentially with the number of layers.Fig. 2. Histograms and statistics of point-wise and window-averaged yield data. The histograms are normalized to probability densities to make point-wise graphsalign with sliding window histograms count-wise. While sliding windows contain no-data points near /uniFB01eld edges, only points containing data were taken intoaccount.Fig. 3. Visualizations of NDVI and RGB input images and yield targets. The identi /uniFB01cation numbers above the images denote the distinct area from which the imageswere extracted.P. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH2.2.6. Regularization strategiesIncreasing the depth of deep learning model allows it to learnmore complex functions. This is also known as increased model ’s ca-pacity Goodfellow et al., 2016 ). When model ’s capacity increases, itbecomes more prone to over /uniFB01tting to the training data in which case itsability to generalize (and, therefore, its performance on test data) de-teriorates. This can be avoided with regularization, which /uniFB00ectivelyreduces the model ’s capacity diminishing the gap between training andtest errors. Regularization is comprehensive term for methods inmachine learning that are used to lower the test error without focusingon training error.In our model we make use of two distinct regularization strategies.First of the two is the L2-penalty, also known as the weight decay. Itdiminishes the model ’s layer-wise parameters with each trainingiteration. When applied in conjunction with training by error back-propagation, the most relevant of the model ’s parameters retain theirmagnitude while non-relevant ones diminish. The second implementedregularization strategy is called early stopping. It is robust meta-al-gorithm integrated into the training process to halt the training after nnon-improving iterations. The hyperparameter nis called patience(Goodfellow et al., 2016 ).2.2.7. Overall architectureThe basic architecture of the CNN implemented in this study followsclosely the one reported by Krizhevsky et al. (2017) Their model per-formed extremely well in ImageNet Large Scale Visual RecognitionCompetition Russakovsky et al., 2015 attaining top classi /uniFB01cation re-sults in multiple categories. The general topology of our network isdepicted in Fig. . The network was implemented using the PyTorchframework Paszke et al., 2017 ). In our network we use non-over-lapping pooling windows with pooling window size of and poolingstride matching the pooling window size. We also include the poolingfunction only in the /uniFB01rst and the last convolutional layer. The reason forthis is that at the lowest (i.e., in the case of 10 ground resolution) ourimage size is 32×32 pixels and too many pooling operations wouldcause the data representation to collapse. This way our network is alsoscalable with respect to the number of layers. Regardless of the numberof source image bands, our convolutional layers contain 64 kernelsexcept for the last layer containing 128 kernels. Krizhevsky et al. (2017)incorporated two FC layers to the model with 2048 neurons per layer.We used similar number of layers with half the width, i.e., 1024 neu-rons per layer.2.3. Optimizing the networkFinding the optimal con /uniFB01guration of any deep learning network isan iterative process, where the model ’s parameters are initialized andtuned multiple times. The goal is to /uniFB01nd set of model ’s parameters(weights, biases, etc.) and hyperparameters (learning rate, optimizercoe/uniFB03cients, etc.) that in conjunction produce the best performance.The output of the iterative process is single model usually performingbest when compared to other models produced within the process. Weused absolute error between the network output and the target value(i.e., crop yield values) as the performance measure. In machinelearning, the best performing model is considered to be the one thatgeneralizes well to previously unseen data. To measure the general-ization performance across training instances, we extracted and re-served subset of data as holdout test set. This test data set was usedoutside of the training loop to ensure that the model never learned fromit. With the rest of the data we performed k-fold cross validation usingthree folds per epoch. An epoch is single complete iteration over thefull training data set consisting of windowed image samples of all 9Fig. 4. The kernel Kis applied to the input image Iin sliding window fashion. With each application, sum of element-wise products is calculated and stored. Afterthe kernel has been applied to the whole image, complete feature map Fis produced. feature map indicates the result of detecting kernel-speci /uniFB01c feature in theinput image.Fig. 5. An example of simple application of maxpooling, where the pooling is applied to featuremap Fwith pooling window size of ×2 and astride equaling the kernel size.Fig. 6. An illustration of the /uniFB00ect of applying the recti /uniFB01ed linear activationfunction to pooled feature map.P. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH/uniFB01elds.The best training algorithm was evaluated among three options:Stochastic Gradient Descent with momentum (SGD-momentum)(Bottou, 1998 ), RMSprop Hinton et al., 2014 and Adadelta Zeiler,2012 ). These training algorithms are suggested in Goodfellow et al.(2016) and they are also among the ones compared in Karpathy andFei-Fei (2017) In preliminary test, the three algorithms were testedfor convergence by training the network for three epochs. Training wasperformed for each of the three data window sizes and each of the foursets of input data. The batch size was varied from 25to210. The worstperforming algorithm was excluded and second test performed on theremaining two by /uniFB01xing the batch size to 128 27) and training for 50epochs, number consistent across the training of almost every model.The /uniFB00ect of the depth of the network on the performance wasevaluated by training models with 4, 6, 8, 10 and 12 convolutionallayers over 50 epochs per training session. The training was conductedfor the NDVI and RGB images from early and late data sets and with allthree input image dimensions using the previously selected trainingalgorithm. At this stage, the best performing combination of networkdepth, image type (NDVI or RGB) and window size was selected basedon error performance over the test data.In the next step, the chosen training algorithm ’s hyperparameters(i.e., the learning rate and the past iterations ’error correction adjust-ment) were tuned. In order to evaluate performance, benchmarkmodels were created by initializing model for each of the four datasets (i.e., early and late, RGB and NDVI). The hyperparameter valueswere searched over coarse grid for values producing lowest test errors,followed by more re /uniFB01ned random search in the vicinity of the coarseminimum. Sensitivity of the network performance to initial values ofthe CNN parameters was also assessed.In the last step, the hyperparameter combinations producing thebest performance were used to test and tune the /uniFB00ect of regularizationalgorithms. Tuning of the weight decay coe /uniFB03cient L2regularization)for early and late data sets was performed by searching over coarsegrid of values followed by re /uniFB01ned search. Subsequently, the /uniFB00ect ofearly stopping was tested using values 10, 20, 30, 40 and 50 for thepatience parameter (see Section 2.2.6 ).3. ResultsWe measure the performance of the CNN by mean absolute error i.e.,the mean absolute di /uniFB00erence between the true yield value and the CNNoutput (predicted value). This can also be called loss. We consider twodi/uniFB00erent errors: the training error, obtained for the same data thenetwork is trained with, and the test error, obtained for the data setaside for testing. The former one indicates how well the model is able to/uniFB01t to the data, i.e., what is its capacity, while the latter one indicateshow well the network is able to generalize to unseen data samples.3.1. Selection of the training algorithmOf the three training algorithms –Adadelta, SGD-momentum andRMSprop –the RMSprop showed poor convergence and was thereforeruled out from subsequent tests. Between the two remaining algorithms,Adadelta outperformed SGD-momentum and was chosen as the trainingalgorithm for further experiments (see Table ).3.2. Depth of the networkInFig. the test and training errors for the three window sizes andfor various networks depths are shown for the RGB data of earliergrowth phase. The largest window (size 40×40 m) produced lowesttest errors in majority of cases regardless of the network depth. Thecolored areas indicate gaps between training (lower bound of the area)and test (upper bound of the area) errors, also referred to asFig. 7. The overall topology of the implemented CNN. Network ’s inputs can be single-band or multi-band images B) with varying dimensions D). The network has atleast two convolutional layers accompanied with two fully connected layers. The depth of the network is controlled by the number of intermediary convolutionallayers. The last convolutional layer has 128 kernels while the intermediary layers have 64 kernels. Max pooling is applied only in the /uniFB01rst and last convolutionallayers so that the size of the data representation stays consistent when network depth is varied.Table 3Lowest mean absolute test errors (kg/ha) observed among the three datawindow size con /uniFB01gurations (10 m, 20 and 40 m) with 50 epochs of trainingand batch size of 128 samples for each source image type. Adadelta performedbest with almost every source image con /uniFB01guration.Optimizer NDVI early NDVI late RGB early RGB lateSGD with Momentum 1751.2 1183.7 1231.5 985.0Adadelta 842.8 1165.1 836.2 989.5P. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHgeneralization gaps. The lowest test and training error combination isobtained with convolutional layers. Also, the 40 window and net-work depth of convolutional layers result in the narrowest general-ization gap.3.3. Optimization of Adadelta hyperparametersThe hyperparameters of the chosen training algorithm, Adadelta,were tuned by considering the /uniFB00ects of the adaptive learning rate andthe coe /uniFB03cient adjusting the /uniFB00ect of past iterations ’error corrections(in the form of squared gradients) on learning. The latter is /uniFB00ectivelysimilar to momentum, de /uniFB01ning the magnitude by which the past /uniFB00ectsthe current learning process. The previous experiments were performedusing default values for these hyperparameters, i.e., 1.0 for the learningrate, 0.9 for the coe /uniFB03cient for computing running average of squaredgradients, and for the weight decay (see Table ).The initial grid search was conducted with hyperparameter valuessimilar to those found in the original Adadelta research paper with anepoch limit of 50 compared to the original study ’s epochs Zeiler,2012 ). For the early RGB data set, the optimal values were approxi-mately ×/uni2212103for the learning rate and 0.58 for the coe /uniFB03cient ad-justing the /uniFB00ect of past iterations ’error. For the late data set the re-spective values were/uni2212104and 0.9. The /uniFB00ect of hyperparameter tuningon the performance of the network can be seen from the results inTable .3.4. Optimization of the regularization parametersThe CNN models using optimal hyperparameters for the Adadeltatraining algorithm were trained next with early and late RGB data setsof 40×40 window to determine the /uniFB00ect of regularization on theprediction error and to tune the regularization parameters. The tuningof weight decay coe /uniFB03cient with grid search /uniFB01rst and zoomed-in randomsearch after that resulted in the optimal coe /uniFB03cient value of/uni2212103forboth data sets. The optimal patience values were around 50, again forboth data sets. It was observed that the increase in patience increasedthe training time signi /uniFB01cantly. The selected patience value allowed themodels for both data sets to converge in approximately 250 epochs. Thee/uniFB00ect of using the L2-regularization alone and combined with earlystopping can be seen from Table .4. Discussion and conclusionsThis study presents training paradigm of CNN based deeplearning model for predicting wheat and barley yield. The results in-dicate that the best performing model can predict within- /uniFB01eld yieldwith mean absolute error of 484 kg/ha (MAPE: 8.8%) based only onRGB images in the early stages of growth (< 25% total thermal time).The model for RGB images at later growth stage returned higher errorvalues (MAE: 680 kg/ha; MAPE: 12.6%). In searching for optimal per-formance, the input data window size (10 m, 20 m, 40 m), the dataacquisition time (early vs. late) and data modality (RGB vs. NDVI) werevaried. The /uniFB01elds included in the study were imaged by cameramounted to UAV and together taken as source of 10,000 inputimage frames covering total area of 90 hectares. Network depth (i.e.,the number of convolutional layers), the training algorithm and itshyperparameters as well as the CNN regularization scheme were alsooptimized. The lowest error was achieved using network consisting 6convolutional layers followed by two fully connected layers regularizedwith L2-regularization coe /uniFB03cient of/uni2212103and early stopping patience of50. The optimizer was also tuned for the optimal value of the learningrate ×/uni221281 03) and the coe /uniFB03cient adjusting the /uniFB00ect of past iterations ’error corrections (0.58). The results show that the lowest test errorswere achieved with the largest data window size tested (40 m).The training of any neural network is always in /uniFB02uenced by theFig. 8. The generalization gaps with early RGB images. The generalization gap is depicted as the di /uniFB00erence between the training and the test errors. It shows howclose the test error is to the /uniFB00ective capacity of the model, the training error. The lowest test errors (upper bound of the area) were achieved rather consistently withthe source image window size of 40 ×40 m.Table 4The total improvement in test error compared to the benchmark model whenusing regularization and optimization of training algorithm hyperparameters.The benchmark models were trained with the early and late RGB image datawith default parameters. Window size was 40 ×40 m. Errors are reported asmean absolute error (MAE) and mean absolute percentage error (MAPE). Thebest results are formatted in bold.RGB early RGB lateMAE [kg/ha]MAPE MAE [kg/ha]MAPEBenchmark 997.8 18.3% 1021.5 19.5%learning rate: 1.0past err. coe /uniFB00.: 0.9weight decay: 0patience:/uni221Ewith Optimized Adadelta params. 546.2 9.6% 624.3 11.4%learning rate (early/late): 0.008/0.0001past err. coe /uniFB00. (early/late): 0.58/0.9and with L2-regularization 558.4 9.4% 700.4 13.1%weight decay: 0.001and with Early Stopping 484.3 8.8% 680.4 12.6%patience: 50P. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHcombined randomness resulting from how the data is shu /uniFB04ed betweencross validation folds, the optimization process and other factors. Thisin turn means that, while discrete error metrics produce rankingacross hyperparameter setups, slight variations between test errors canbe attributed to the random nature of the optimization process as awhole. We optimized distinct models for early and late RGB data sets.The best performing model used RGB images from the early growingseason and bene /uniFB01ted from regularization. The model using the late RGBimages didn ’t gain from added regularization, as the best performancewas achieved during the tuning of the training algorithm (see Table ).In yield prediction, the shift from using traditional regressionmethods Ruß, 2009 towards arti /uniFB01cial neural network based methods(Chlingaryan et al., 2018 has resulted in improved performance Jianget al., 2004; Kaul et al., 2005 ). Among these ANN based studies, thoseusing remote sensing image data to train their prediction models haveachieved low prediction errors (/uni22485%). These models are speci /uniFB01c to thecrop types whose images they are trained with (e.g., soybean, wheat,rice). Jiang et al. (2004) working with satellite images reported anaverage relative winter wheat prediction error of 3.5%. The indicesused for training the model were: NDVI, surface temperature, absorbedphotosynthesis active radiation, water stress index and 10-year averagecrop yield. Bose et al. (2016) employed spiking neural networks toestimate winter wheat yield from satellite based NDVI images at theregion level, achieving best average relative error of 4.35%. In theirrecent work, You et al. (2017) leveraged advanced hybrid machinelearning algorithms to achieve very low soybean yield prediction errors(3.19 –5.65%) using only satellite images.A commonality among these studies is the use of satellite imageryand large spatial scales of their analyses (region or county level pre-dictions). Our study, in contrast, seeks to perform predictions at theintra- /uniFB01eld scale using UAV based images in order to spatially analyzeyield within the /uniFB01eld. In one of the earliest studies on this topic, Davisand Wilkinson (2006) used satellite imagery of wheat crop (visible,infrared and radar) and an ANN model showing promising results (errorslightly above 10%) for single /uniFB01eld (/uni224836 ha). Khanal et al. (2018)employed various machine learning algorithms (including neural net-works) and aircraft based multi-spectral images to predict corn yield ona single /uniFB01eld (17.5 ha). few studies have applied ANN ’s for classifyingcrops Rebetez et al., 2016 and yield Pantazi et al., 2016 at the intra-/uniFB01eld scale. However, rather than classifying within yield categories thisstudy aims at quantitative predictions. Models at intra- /uniFB01eld scale wouldo/uniFB00er the individual farmer the possibility of in-season monitoring ofcrop, which would enable decision support systems for interventionsnecessary to achieve higher yields. Models trained at large regionalscales rarely extrapolate to /uniFB01ner scales, though /uniFB00orts are underway todevelop scalable models Donohue et al., 2018 ). The methodology in-troduced by You et al. (2017) shows great potential and as authorsclaim its scalability, it would certainly be of interest in testing at theintra- /uniFB01eld scale.One important aspect of remote sensing based yield prediction hasbeen /uniFB01nding image channels or indices containing the most dis-criminating features necessary for analysis Panda et al., 2010 ). Con-sequently, the /uniFB01nding in this study that the RGB images perform betterthan NDVI, assumes signi /uniFB01cance and aligns with the study for esti-mating biomass and crop height Näsi et al., 2017 ). This indicates thatmultiple spectral bands increase the information content in comparisonto the condensed NDVI image. From utility perspective, RGB camerasare cheaper with most commercially available UAVs already /uniFB01tted withdecent cameras able to produce images of high resolution. Models thatcan perform well without the need for expensive specialized equipmentwill make the analyses accessible to an individual farmer.The relationship between crop yield and its environment is non-linear and may not be su /uniFB03ciently contained in the features captured byimages. As shown by the studies reporting low prediction error levels,by adding multi/hyper spectral data, temporal image data, soil andenvironmental features in the feature matrix, it is possible to constrainthe resulting model error /uniFB00ectively. Considering that this study modelsthe yield based only on images, the resulting prediction error of 8.8% ispromising. Additionally, collection of multi-year yield maps fromsensor-equipped harvesters would add valuable information to act asground truth. More than 90 hectares of /uniFB01elds were mapped in this study(2017 season). In 2018 similar set of data has been acquired while thedata acquisition will be continued in 2019. This valuable database willserve to further train, tune and verify the current model for greateraccuracy. An additional limitation of this study is that only minimalpreprocessing was applied to the source data. Developing automatederror correction methods for data preprocessing would be another im-portant task when developing remote sensing based crop yield models.Careful artifact rejection and preprocessing would probably bene /uniFB01t themodeling considerably.In conclusion, this study is an important step towards establishing acombined model for wheat and barley yield prediction in the Finnishcontinental subarctic climate. The long summer growing days in thisregion presents unique pro /uniFB01le of temperature and photoperiod, jus-tifying region speci /uniFB01c deep learning model for these crops. By col-lecting data using commercial /uniFB00-the-shelf UAV and camera packages,wefocus our attention on spatial scale that enables us to predict intra-/uniFB01eld yield distribution within the context of individual farm cropmonitoring. The results indicate that the CNN models are capable ofreasonable accurate yield estimates based on RGB images. It is worthnoting that the CNN architecture seemed to be performing better withRGB images than NDVI images. In the future, the developed model willbe trained on larger set of features (climate and soil) along with timeseries image data to tune the trained model for accuracy.AcknowledgmentsWe would like to give special acknowledgments to Mtech DigitalSolutions Oy for partly funding this research. We also want to thank theMIKÄ DATA project ’s research group of Tampere University ofTechnology for providing the data and additional knowledge requiredto use the data appropriately. special thanks to Mikko Hakojärvi fromMtech Digital Solutions Oy for providing insight into the agriculturalknowledge domain.Appendix A. Supplementary materialSupplementary data associated with this article can be found, in theonline version, at https://doi.org/10.1016/j.compag.2019.104859 .ReferencesBose, P., Kasabov, N.K., Bruzzone, L., Hartono, R.N., 2016. Spiking neural networks forcrop yield estimation based on spatiotemporal analysis of image time series. IEEETrans. Geosci. Remote Sens. 54 (11), 6563 –6573 .Bottou, L., 1998. On-line Learning in Neural Networks. Cambridge University Press, NewYork, NY, USA Ch. On-line Le, pp. –42.Chlingaryan, A., Sukkarieh, S., Whelan, B., 2018. Machine learning approaches for cropyield prediction and nitrogen status estimation in precision agriculture: review.Comput. Electron. Agric. 151 (November 2017), 61 –69.Chunjing, Y., Yueyao, Z., Yaxuan, Z., Liu, H., 2017. Application of convolutional neuralnetwork in classi /uniFB01cation of high resolution agricultural remote sensing images. ISPRS–Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. XLII-2/W7, 989 –992.Davis, I.C., Wilkinson, G.G., 2006. Crop yield prediction using multipolarization radarand multitemporal visible/infrared imagery. In: Proc.SPIE 6359, 6359 –6359 –12.Donohue, R.J., Lawes, R.A., Mata, G., Gobbett, D., Ouzman, J., 2018. Towards national,remote-sensing-based model for predicting /uniFB01eld-scale crop yield. Field Crops Res.227, 79 –90.Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. MIT Press .He, K., Zhang, X., Ren, S., Sun, J., 2015. Delving deep into recti /uniFB01ers: Surpassing human-level performance on imagenet classi /uniFB01cation. In: Proceedings of the IEEEInternational Conference on Computer Vision 2015 Inter, pp. 1026 –1034.Hinton, G., Srivastava, N., Swersky, K., 2014. Neural Networks for Machine LearningLecture 6a: Overview of minibatch gradient descent.Io/uniFB00e, S., Szegedy, C., 2015. Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift.Jiang, D., Yang, X., Clinton, N., Wang, N., 2004. An arti /uniFB01cial neural network model forP. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHestimating crop yields using remotely sensed information. Int. J. Remote Sens. 25 (9),1723 –1732 .Kamilaris, A., Kartakoullis, A., Prenafeta-Boldú, F.X., 2017. review on the practice ofbig data analysis in agriculture. Comput. Electron. Agric. 143, 23 –37.Karpathy, A., Fei-Fei, L., 2017. Deep visual-semantic alignments for generating imagedescriptions. IEEE Trans. Pattern Anal. Mach. Intell. 39 (4), 664 –676.Kaul, M., Hill, R.L., Walthall, C., 2005. Arti /uniFB01cial neural networks for corn and soybeanyield prediction. Agric. Syst. 85 (1), –18.Khanal, S., Fulton, J., Klopfenstein, A., Douridas, N., Shearer, S., 2018. Integration of highresolution remotely sensed data and machine learning techniques for spatial pre-diction of soil properties and corn yield. Comput. Electron. Agric. 153 (August),213–225.Krizhevsky, A., Sutskever, I., Hinton, G.E., 2017. ImageNet classi /uniFB01cation with deepconvolutional neural networks. Commun. ACM 60 (6), 84 –90.Laine, A., Högnäsbacka, M., Niskanen, M., Ohralahti, K., Jauhiainen, L., Kaseva, J.,Nikander, H., 2017. Virallisten lajikekokeiden tulokset 2009-2016, 262.Matikainen, L., Karila, K., Hyyppä, J., Puttonen, E., Litkey, P., Ahokas, E., 2017.Feasibility of multispectral airborne laser scanning for land cover classi /uniFB01cation, roadmapping and map updating. ISPRS Int. Arch. Photogramm. Remote Sens. Spatial Inf.Sci. XLII-3/W3, 119 –122.Milioto, A., Lottes, P., Stachniss, C., 2017. Real-time semantic segmentation of crop andweed for precision agriculture robots leveraging background knowledge in CNNs.Adv. Intell. Syst. Comput. 531, 105 –121.Miyoshi, G.T., Imai, N.N., de Moraes, M.V.A., Tommaselli, A.M.G., Näsi, R., 2017. Timeseries of images to improve tree species classi /uniFB01cation. ISPRS –Int. Arch.Photogramm. Remote Sens. Spatial Inf. Sci. XLII-3/W3, 123 –128.Näsi, R., Viljanen, N., Kaivosoja, J., Hakala, T., Pand žić, M., Markelin, L., Honkavaara, E.,2017. Assessment of various remote sensing technologies in biomass and nitrogencontent estimation using an agricultural test /uniFB01eld. ISPRS –Int. Arch. Photogramm.Remote Sens. Spatial Inf. Sci. XLII-3/W3, 137 –141.Panda, S.S., Ames, D.P., Panigrahi, S., 2010. Application of vegetation indices for agri-cultural crop yield prediction using neural network techniques. Remote Sens. (3),673–696.Pantazi, X.E., Moshou, D., Alexandridis, T., Whetton, R.L., Mouazen, A.M., 2016. Wheatyield prediction using machine learning and advanced sensing techniques. Comput.Electron. Agric. 121, 57 –65.Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A.,Antiga, L., Lerer, A., 2017. Automatic di /uniFB00erentiation in PyTorch. In: NIPS-W.Rebetez, J., Satizábal, H.F., Mota, M., Noll, D., Büchi, L., Wendling, M., Cannelle, B.,Pérez-Uribe, A., Burgos, S., 2016. Augmenting convolutional neural network withlocal histograms a case study in crop classi /uniFB01cation from high-resolution uav ima-gery. In: 24th European Symposium on Arti /uniFB01cial Neural Networks, ESANN 2016,Bruges, Belgium, April 27 –29,2016.Ruß, G., 2009. Data mining of agricultural yield data: comparison of regression models.Lecture Notes in Computer Science (including subseries Lecture Notes in Arti /uniFB01cialIntelligence and Lecture Notes in Bioinformatics) 5633 LNAI, 24 –37.Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A.,Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L., 2015. ImageNet large scale visualrecognition challenge. Int. J. Comput. Vision 115 (3), 211 –252.Sa, I., Chen, Z., Popovic, M., Khanna, R., Liebisch, F., Nieto, J., Siegwart, R., 2017.weedNet: Dense Semantic Weed Classi /uniFB01cation Using Multispectral Images and MAVfor Smart Farming.Tiusanen, J., 2017. Aineiston käsittely ja muotoilu. Käytännön Maamies.Wolfert, S., Ge, L., Verdouw, C., Bogaardt, M.J., 2017. Big data in smart farming –areview. Agric. Syst. 153, 69 –80.You, J., Li, X., Low, M., Lobell, D., Ermon, S., 2017. Deep Gaussian process for crop yieldprediction based on remote sensing data. In: 31th AAAI Conference on Arti /uniFB01cialIntelligence, pp. 4559 –4565 .Zeiler, M.D., 2012. ADADELTA: An Adaptive Learning Rate Method. unde /uniFB01ned.P. Nevavuori, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH