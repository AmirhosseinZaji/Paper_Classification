Journal of Integrative Agriculture 2020, 19(8): 1998–2008RESEARCH ARTICLEAvailable online at www.sciencedirect.comScienceDirectDetection and enumeration of wheat grains based on deep learning method under various scenarios and scalesWU Wei, YANG Tian-le, LI Rui, CHEN Chen, LIU Tao, ZHOU Kai, SUN Cheng-ming, LI Chun-yan, ZHU Xin-kai, GUO Wen-shanJiangsu Key Laboratory of Crop Genetics and Physiology/Co-Innovation Center for Modern Production Technology of Grain Crops, Yangzhou University, Yangzhou 225009, P.R.China Abstract Grain number is crucial for analysis of yield components and assessment of effects of cultivation measures. The grain number per spike and thousand-grain weight can be measured by counting grains manually, but it is time-consuming, tedious and error-prone. Previous image processing algorithms cannot work well with different backgrounds and different sizes. This study used deep learning methods to resolve the limitations of traditional image processing algorithms. Wheat grain image datasets were collected in the scenarios of three varieties, six background and two image acquisition devices with different heights, angles and grain numbers, 748 images in total. All images were processed through color space conversion, image flipping and rotation. The grain was manually annotated, and the datasets were divided into training set, validation set and test set. We used the TensorFlow framework to construct the Faster Region-based Convolutional Neural Network Model. Using the transfer learning method, we optimized the wheat grain detection and enumeration model. The total loss of the model was less than 0.5 and the mean average precision was 0.91. Compared with previous grain counting algorithms, the grain counting error rate of this model was less than 3% and the running time was less than s. The model can be effectively applied under variety of backgrounds, image sizes, grain sizes, shooting angles, and shooting heights, as well as different levels of grain crowding. It constitutes an effective detection and enumeration tool for wheat grain. This study provides reference for further grain testing and enumeration applications.Keywords: wheat grain, deep learning, Faster R-CNN, object detection, counting1. IntroductionThe number of grains per spike and thousand-grain weight are important factors that reflect the grain yield of crops, and are necessary indicators in crop breeding and cultivation (Prystupa et al. 2004; Peltonen-Sainio et al. 2007; Slafer et al. 2014; García et al. 2016). Enumerating the grains rapidly will render scientific research more efficient. During crop breeding and cultivation, the number of grains per panicle, individual plant, plot, and lineage group is used as the basic information for examining the yield potential Received 24 April, 2019 Accepted 29 July, 2019WU Wei, E-mail: 435208450@qq.com; Correspondence SUN Cheng-ming, Tel: +86-514-879793381, E-mail: cmsun@yzu.edu.cn; GUO Wen-shan, Tel: +86-514-87979339, E-mail: guows@yzu.edu.cn© 2020 CAAS. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).doi: 10.1016/S2095-3119(19)62803-01999WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008of the line (Ferrante et al. 2017), and also constitutes the basic data for examining the effectiveness and practicality of cultivation measures (Li et al. 2004). In practice, the number of grains is often obtained by manual enumeration (Liu et al. 2016). This method is time-consuming, tedious and error-prone. It is thus necessary that convenient and efficient grain counting method is developed.At present, commonly used rapid grain detection and counting methods rely on image processing techniques. They have two key processes. The first one is to separate the object from the background, and the other is to resolve the difficulty in separating multiple grains that are adjacent. The typical approach for the first process is to select single color plate as the background, and then easily separate the object from the background using color extraction algorithm. The second process focuses on grain counting. Numerous existing segmentation algorithms for adjacent grains have been proposed, including expansion corrosion algorithms (Shatadal et al. 1995), watershed algorithms (Bleau and Leon 2000) and feature point matching algorithms (Hobson et al. 2009; Kiratiratanapruk and Sinthupinyo 2010; Lin et al. 2014). The threshold of expansion and corrosion is mainly determined by human experience. This is invalid when the grains are adjacent on large scale, and the non-reproducibility of corrosion and expansion also causes errors. The watershed algorithm is widely-used segmentation method that uses the local gradient differences in the overlapping regions and identifies the segmentation lines through the canal simulation process. However, this algorithm is prone to over-separation. The feature-point-matching algorithm uses the methods of curvature detection and corner detection to detect the characteristic points of the adjacent grain area, and achieves grain separation through feature point matching (Bai et al. 2009). This method generally has high separation accuracy, but experiences matching errors in complex adhesion areas. Although the accuracy of the existing image processing algorithms is continuously improving, their actual adaptability is still poor because they all require specific environment, such as customized background board, fixed camera, complex equipment, and fixed angle.With the rapid development of the theory and practice of deep learning, object detection and classification based on machine learning have entered new phase. In contrast to the traditional feature extraction algorithms that rely on prior knowledge, deep convolutional neural networks have certain degree of invariance to geometric transformation, deformation and illumination, which effectively overcomes the difficulty of recognition caused by changes in object appearance. Furthermore, they can also construct feature descriptions that are adaptively driven by training data, thereby exhibiting greater flexibility and generalization ability. Since the inception of the Region-based Convolutional Neural Network Method (R-CNN) in 2013, deep learning has progressed rapidly. R-CNN, proposed by Girshick et al. (2014, 2015), has made major advances in the field of object detection, and the object detection accuracy has increasingly progressed. Following this, spatial pyramid pooling in deep convolutional networks (SPP-net) (He et al. 2014), Fast Region-based Convolutional Neural Network (Fast R-CNN) (Girshick 2015), Faster Region-based Convolutional Neural Network (Faster R-CNN) (Dechant et al. 2017; Lu et al. 2017; Xiong et al. 2017), region-based fully convolutional networks (R-FCN) (Dai et al. 2016), You Only Look Once (YOLO) (Redmon et al. 2015), Single Shot MultiBox Detector (SSD) (Liu et al. 2015), and other algorithms emerged. These innovative algorithms combined the traditional field of computer vision with deep learning, achieving significant outcomes, such as selective search (SS) and image pyramid. Object detection algorithms based on deep learning have attracted the attention of researchers and have become an important research topic in the field of artificial intelligence. There are more and more studies in the field of agriculture (Liu et al. 2015; Sharada et al. 2016; Chen et al. 2019).The results of the previous grain counting methods have predominantly been achieved through image processing methods, leaving significant room for improvement in terms of accuracy, scene recognition, calculation speed, and ease of use. Deep learning technology has great advantages in object detection in complex backgrounds, and can improve the deficiencies in past grain enumeration methods. However, wheat grains differ from other objects. There are four or five effective tillers in single wheat plant, and the number of grains after each tillering is about 40 grains. Thus the scenario of 200 grains appearing on one image at the same time must be considered. This then leads to other issues, e.g., multiple objects and small dimensions on the image. In such scenario, current deep learning methods cannot be directly used for grain detection and enumeration. Therefore, this study aimed to construct deep learning-based grain-counting model for better grain counting.2. Materials and methodsWe firstly manually collected wheat grain image datasets in different environments, manually annotated the grain, and then divided the dataset into three sub-datasets for model training, verification, and testing, respectively. Model training used the transfer learning method, which can significantly reduce training time and accelerate the reduction of loss. The mean average precision (mAP) is good and smooth during model validation, so the model can be used for 2000WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008testing. The main procedure is shown in Fig. 1.2.1. Image collectionPublicly available datasets, including ImageNet (http://www.image-net.org/), COCO (http://cocodataset.org/) and PASCAL VOC (http://cvlab.postech.ac.kr/~Mooyeol/pascal_voc_2012/), do not contain the object of this study. Therefore, we manually collected the object dataset. Considering the variability between different wheat varieties, three varieties, namely Ningmai 13, Yangfumai and Yangmai 23, were selected as research objects. In order to adapt the method to complex scene, six backgrounds were selected: red written, ivory white tablecloth, white A4 paper, black mouse pad, pan-yellow paper, and blue drawing mouse pad, and the number of seeds in each background was 1–220. Differences exist between the front and back images of wheat grains, such as changes in directions of the ventral grooves, or changes in the crowdedness. Different grain spatial arrangements are involved. However, this does not account for changes in grain overlap. Two different image acquisition devices are used including Huawei mobile phone camera (type: KIW-AL10, camera lens: 13 million pixels 2.0, China) and Sony cameras (type: ILCE-6300, lens: PZ 16–50 mm 3.5–5.6, Japan). In order to render the method more applicable, three heights (10, 15 and 20 cm) and four shooting angles (45°, 60°, 75°, and 90°) were set for image acquisition. total of 432 treatments were combined by five treatments: variety, background, equipment, shooting height, and shooting angle. Each treatment randomly placed 1–200 seeds and captured images. total of 748 images were obtained, including 29 175 grain objects.2.2. Object annotationObject detection determines the location of the object. The collected images can be targeted by the object annotation tool. Among the commonly used tools like RectLabel, LabelImg and LabelMe, we selected the LabelImg (Tzutalin, https://github.com/tzutalin/labelImg) tool under the Ubuntu System, ultimately generating an .xml file that constitutes PASCAL VOC format annotation file.2.3. Data standardizationPicture and annotation data were input into the model after being converted into binary data. We used the Tensorflow deep learning framework, and unified the image and annotation data into the TFRecord data type. We divided 70% of the dataset into training sets, 20% into validation sets and 10% into test sets.2.4. PreprocessingThe input images were pre-processed, including the conversion of color space, image turnover, contrast change, Pre-trainCNNImageImageImageModel parametersInference graph DatasetCollectingLabelingTrainingValidationTFRecord1223350175TestFaster R-CNN modelFrozenFeaturemapRPNRoIpoolingClassificationlossBounding-boxregression lossTotal lossPrecision/mAPResultNumber: 10 grainsProbability:Grain 1: 99%Grain 2: 99%Grain 3: 99%…Fig. Grain detection flowchart. RPN, Region Proposal Network; CNN, Convolutional Neural Network.2001WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008and so on. The input of the model is not limited, but different image sizes occupy different amounts of memory or video memory. The maximum image preprocessing size was 200×1 600 pixels, while the minimum was 500×500 pixels.2.5. Network modelThe Faster R-CNN Model was used for object detection. Faster R-CNN is developed from R-CNN and Fast R-CNN. Its primary function is to run convolutional neural network on the original map to extract basic features and introduce region proposal network (RPN) to create candidate areas, followed by Softmax classification layer and bounding box return.Conventional feature extraction networks include VGGNet, Inception Net and ResNet. Deep learning often faces the degradation problem (He and Sun 2014; Srivastava et al. 2015), i.e., the error rate increases as the model level deepens. One of the reasons for this is that the deeper the network is, the more the gradient disappears. Therefore, the developers of ResNet (He et al. 2016) proposed residual structure. As the number of layers in the model deepens, this simple structure can resolve the problem of degradation. Fig. shows the ResNet 101 network structure used in this paper.The Faster R-CNN Model uses RPN networks to extract grain object. The input feature map receives the regional proposal and the regional score through the RPN, uses the non-maximum suppression (threshold 0.7) for the regional score, and then output the Top-N (500 in this article) score region to the region of interest (RoI) pooling layer. The RPN considers nine possible reference windows at each sliding window position (Fig. 3). Anchors are fixed set of reference windows with three scales (1 282, 562 and 122)×three aspect ratios (1:1, 1:2 and 2:1). Using the anchor pyramid to rely solely on single-scale images and feature maps and single-sized convolution kernels, multi-scale problem and multi-length-breadth ratio problem can be solved. This model for the recommended sampling area can achieve good performance in terms of both speed and accuracy.2.6. The assessment methodThe effect of model fitting is represented by loss function or cost function. The smaller the loss function is, the better the model fits. The loss function proposed by Faster R-CNN is as follows: {} }()()()***11L,,,tiiclsiiiregiiiiclsregptLp ppL tNN=+Ȝ (1)where represents the index of an anchor in batch, pi e r s n s h p o a i i y f n h r predicted as an object; when the anchor is positive sample, pi*=1, and when it is negative sample pi*=0. It can be seen that the regression loss term is only activated if the anchor is positive; ti represents the four panning scaling parameters of the positive sample anchor to the prediction region; ti* represents the four panning scaling parameters of the positive sample anchor to the ground truth; is used to weigh classification loss Lcls and regression loss Lreg, and the default value of Ȝ=10; Ncls and Nreg are used to normalize classification loss 7×7 conv, 64, /21×1 conv, 643×3 conv, 641×1 conv, 256Pool, /21×1 conv, 643×3 conv, 641×1 conv, 2561×1 conv, 643×3 conv, 641×1 conv, 2561×1 conv, 1283×3 conv, 1281×1 conv, 5121×1 conv, 1283×3 conv, 1281×1 conv, 5121×1 conv, 1283×3 conv, 1281×1 conv, 5121×1 conv, 1283×3 conv, 1281×1 conv, 5121×1 conv, 5123×3 conv, 5121×1 conv, 0481×1 conv, 5123×3 conv, 5121×1 conv, 0481×1 conv, 5123×3 conv, 5121×1 conv, 0481×1 conv, 2563×3 conv, 2561×1 conv, 0241×1 conv, 2563×3 conv, 2561×1 conv, 0241×1 conv, 2563×3 conv, 2561×1 conv, 024fc 1000Avg, poolImage×21Fig. The ResNet 101 network structure. conv, convolutional. 2002WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008item Lcls and regression loss item Lreg, respectively.Classification loss function Lcls is Boolean classifier (object or not), and the formula is as follows:() [(1 )(1 )]clsiiiiiiLp plogpp p***=í+íí, (2)Regression loss function Lreg is used to calculate the difference between the two transformations, and the formula is as follows:(, ( )regiiiiLtt t t**=í (3)R function is defined as: (4)()=*10.5 ,LxifxSmoothxxotherwise<1°®í°¯mAP is an indicator of the accuracy in the object detection algorithm and involves two concepts: precision and recall. For an object, precision and recall can be calculated. P–R curve can be retrieved through multiple calculations and trials, and the area under the curve is the AP value. The average AP value of each class is mAP. This article only identifies one class, so that the value of mAP lies in the [0, 1] range. The formula is as follows:1RqQRmAP=AP(q)Q (5)where is the number of queries.Error rates and runtimes are often used as important indicators for measuring count problems. The formula for error rate is:Error ratio=PNíTNTN (6)where PN is predicted number, TN is true number.&RPSXWHUFRQ¿JXUDWLRQThe software and hardware of the computer and network configurations are shown in Table 1.3. Results 3.1. Image processingAlthough the experimental design includes five kinds of treatments of variety, background, equipment, height and angle, the number of images is still small. In order to make the model more adaptable, this study deals with the acquired images, which is beneficial to the expansion of the data set and the increase of sample diversity. The result is shown in Fig. 4, including image rotation, color space conversion and image flipping.3.2. Model trainingThe model training and verification process need to be monitored. If the training is poor, the parameters should be adjusted accordingly and timeously. The model in this article used transfer learning (Pan and Yang 2010) which greatly reduced the training time. The model converged quickly and began to stabilize at 000 iterations. The classification loss (Fig. 5-A) and the border regression loss (Fig. 5-B) both dropped to very low level, and the total loss (Fig. 5-C) slowly declined after 000–10 000 iterations. The mAP (Fig. 5-D) reached above 0.8 after 000–10 000 iterations, and reached 0.91 at 30 000 iterations.3.3. Model accuracyThe model counting method was tested using samples with different grain numbers. The results are shown in Table 2. When the number of grains in the sample is less than 50, the recognition accuracy reaches 100%. When 36 coordinates18 scores256-dConv feature mapSliding windowsIntermediate layerCls layerReg layer1:11:22:1{1282, 2562, 5122}Fig. Region Proposal Network (RPN). Cls, classification; Reg, regression; Conv, convolutional. 2003WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008Table Computer and network configurationProject1)ContentCPUIntel Xeon E5-2682v4RAM16 GGPUNvidia Tesla P4Operating systemUbuntu 16.04 LTSCudaCuda8.0 with Cudnn v6Data processingPython2.7, OpenCV, LabelImg, etc.Deep learning frameworkTensorFlowBatch size1Initial learning rate0.0003Learning rate0.0003Iteration steps30 0001) CPU, central processing unit; RAM, random access memory; GPU, graphics processing unit. the number of grains is less than 160, the error rate of most samples does not exceed 1%. When the number of grains reached 200, the error rate increased, but none exceeded 3.5%. Among them, the model is not affected by the type of wheat varieties and different backgrounds, and the robustness is strong.0RGHOYHUL¿FDWLRQThe accuracy of the model was further tested under different backgrounds and different grain numbers. The results are shown in Fig. 6. The results showed that different backgrounds have less impact on the model. The model has strong robust. When the number of seeds is too high, the accuracy of the model will decrease. Fig. is 1:1 line graph of estimated and measured values. The model performs well. When the number of grains is below 200, the verification model R2 is 0.9996 and the RMSE is 2.63. 4. Discussion4.1. Number of iterations analysisThe number of iterations had great influence on the training effect of the model. As shown in Fig. 8, 000-step iterations were very poor in candidate region selection and category score performance, whereas the results were significantly improved after 30 000 iterations.4.2. Grain scale analysis Traditional image processing algorithms generally preprocess the image first. When the scale of the grain changes, it is challenging to adaptively adjust the threshold to obtain better pretreatment effect. As shown in Fig. 9, this research method is barely influenced by the grain scale. Only when the grain scale is extremely small (the grain occupying less than 3% of the entire map) and loses its main features, will the detection results be affected.4.3. Comparison of grain counting algorithmsFrom the Table 3, it can be seen that in the traditional grain counting method, the feature point matching algorithm performs best, and the error rate is below 5%. The watershed algorithm is the second, and the morphology erosion algorithm has the highest error rate. Compared with the traditional algorithm, the Faster R-CNN Model has certain advantages in accuracy. When the number of seeds is less than 150, the error rate is less than 1%, and ABCDHGFEKLMAIJ018027090Fig. Image processing results. A, the original image. B, rotation 45°. C, rotation 90°. D, rotation 135°. E, rotation 180°. F, rotation 225°. G, rotation 270°. H, rotation 315°. I, YUV color space. J, HIS color space. K, vertical flip. L, horizontal flip. M, horizontal vertical flip.2004WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008Table Model counting results under different grain numbersVarietyGrainnumberError rate (%)1)Background Background Background Background Background Background 6Ningmai 13100000002000000050000000800.020.020.030.010.010.021200.640.460.810.860.910.671600.900.860.950.670.850.892002.672.693.052.873.113.08Yangfumai 100000002000000050000000800.010.020.010.020.020.021200.540.660.740.690.890.781600.860.880.960.941.040.992003.052.962.582.672.312.77Yangmai 23100000002000000050000000800.030.020.010.010.020.021200.560.840.890.710.850.591600.780.930.820.980.970.922002.593.103.042.853.062.87 1) Background is red letter paper, Background is ivory white tablecloth, Background is white A4 paper, Background is black mouse pad, Background is pan-yellow paper, and Background is blue drawing mouse pad.Fig. Model training and verification results. A, the category prediction loss. B, the border regression loss. C, the total loss. D, mean average precision.05 000 10 000 15 000 20 000 25 000 30 00000.20.40.60.81.0ValueIteration steps05 000 10 000 15 000 20 000 25 000 30 00000.20.40.60.81.0ValueIteration steps05 000 10 000 15 000 20 000 25 000 30 00000.20.40.60.81.0ValueIteration steps05 000 10 000 15 000 20 000 25 000 30 00000.20.40.60.81.0ValueIteration stepsDCAB2005WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008the highest is only about 3%. In the field of deep learning, there are simpler and faster models such as R-FCN, YOLO, SSD, etc. These models perform poorly in the detection of large number of targets and small targets. The test results represented by SSD are shown in the Table 3. The Faster R-CNN Model is significantly better than them.4.4. RuntimeObject detection and enumeration have certain runtime requirements. The optimum is to achieve real-time detection of each frame of the video. Fig. 10 shows comparison of several common kernel inspection runtimes. Runtime less than second can be applied in mobile terminal (Liu et al. 2017). The runtime of the expansion corrosion algorithm was less than s, but was less accurate in the case of multi-grain overlapping or adjacency. The watershed algorithm ran for up to 2.6 s. The runtime of the feature-point-matching algorithm increased with the increase in grain number. None of these three algorithms could segment and recognize O/E=199/193AO/E=158/157 BO/E=116/115 CO/E=80/80 DO/E=50/50EO/E=20/20 FFig. Test results. A, 199 grains in blue drawing mouse pad. B, 158 grains in red letter paper. C, 116 grains in desktop. D, 80 grains in white A4 paper. E, 50 grains in black mouse pad. F, 20 grains in ivory white tablecloth. O, observed value; E, estimated value.030609012015018021024030 60 90 120 150 180 210 240Estimated valueObserved valueR2 =0.9996RMSE=2.63Fig. Grain counting verification result.Iteration=3 000Iteration=30 000Fig. Results of different numbers of iterations. 2006WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008other background and size images. The SSD Model was insensitive to changes in grain number and image size (Figs. 10 and 11), and the runtime was less than s. The runtime of the Faster R-CNN Model in this paper increased with the increase in grain number, and was within s. The larger image size also affected the runtime of the Faster R-CNN Model. Once the image size was larger than 800×800 pixels, it had no effect on the detection accuracy of the model. Therefore, it is possible to resize the image to 800×800 pixels to resolve this issue. Below 500×500 pixels, too many object features will be lost, and the accuracy of both models will be impaired. The accuracy of the Faster R-CNN Model was about 0.2 higher than that of the SSD Model. Overall, the Faster R-CNN model was superior. Future work should be focused on optimizing the model structure and reducing the model parameters, which will help reduce the runtime.5. ConclusionThis paper collected wheat grain dataset and used the migration learning method based on the Faster R-CNN deep learning network to train the wheat grain detection and enumeration depth model. The model can be applied to the detection and enumeration of images in variety of complex backgrounds at multiple scales, and in multi-angle Table Comparison of error rates in the different grain counting methods1)Grain number Morphology erosion (%) Watershed algorithm (%) Corner matching (%)SSD (%)Faster R-CNN in this paper (%)10 10.32 18.62 00020 17.65 23.46 00050 32.23 35.64 1.2410.56080 36.26 38.26 3.5625.430120 36.56 39.35 2.4633.340.84160 42.48 38.46 2.7837.500.63200 45.56 37.59 4.3248.363.021) SSD, single shot multibox detector; Faster R-CNN, Faster Region-based Convolutional Neural Network.O/E=5/5O/E=9/9O/E=21/21Big scaleMiddle scaleSmall scaleFig. Test results of different grain scales. O, observed value; E, estimated value.00.51.01.52.02.53.03.54.050 100 150 200Runtime (s)The number of grainsMorphology erosionWatershed algorithmCorner matchingSSD (800)Faster R-CNN in this paper (800)Fig. 10 Comparison of the runtime of the different methods. SSD, single shot multibox detector; Faster R-CNN, Faster Region-based Convolutional Neural Network. 800, the size of the image is 800×800 pixels.2007WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–200800.10.20.30.40.50.60.70.80.91.000.51.01.52.02.53.03.54.04.55.0300×300500×500800×8001 200×1 2001 500×1 5001 800×1 8002 100×2 1002 400×2 400mAPRuntime (s)Image scales (px)Faster R-CNN in this paperSSDFaster R-CNN in this paperSSDFig. 11 Effect of different image scales on runtime. SSD, single shot multibox detector; Faster R-CNN, Faster Region-based Convolutional Neural Network. px, pixels.environments. With an error rate of less than 3% and running time less than s, the model constitutes an effective tool for the detection and enumeration of wheat grains.AcknowledgementsThis research was mainly supported by the National Key Research and Development Program of China (2017YFD0301205), the Postgraduate Research Practice Innovation Program of Jiangsu Province, China (KYCX18_2371), the National Natural Science Foundation of China (31701355 and 31671615), the China Postdoctoral Science Foundation, China (2016M600448), the Priority Academic Program Development of Jiangsu Higher Education Institutions, China (PAPD), the Yangzhou Science Foundation for Excellent Youths, China (YZ2017098) and the Science and Technology Plan Projects of Yangzhou, China (YZ2016251).ReferencesBai X, Sun C, Zhou F. 2009. Splitting touching cells based on concave points and ellipse fitting. Pattern Recognition, 42, 2434–2446.Bleau A, Leon J. 2000. Watershed-based segmentation and region merging. Computer Vision Image Understanding, 77, 317–370.Chen J, Liu Q, Gao W. 2019. Visual tea leaf disease recognition using convolutional neural network model. Symmetry, 11, 343–356.Dai J, Li Y, He K, Sun J. 2016. R-FCN: Object detection via region-based fully convolutional networks. Neural Information Processing Systems (NIPS). Barcelona, Spain. pp. 1–9.Dechant C, Wiesnerhanks T, Chen S, Stewart L, Yosinski J, Gore A, Nelson J, Lipson H. 2017. Automated identification of northern leaf blight-infected maize plants from field imagery using deep learning. Phytopathology, 107, 1426–1432.Ferrante A, Cartelle J, Savin R, Slafer A. 2017. Yield determination, interplay between major components and yield stability in traditional and contemporary wheat across wide range of environments. Field Crops Research, 203, 114–127.García A, Serrago A, Dreccer F, Miralles J. 2016. Post-anthesis warm nights reduce grain weight in field-grown wheat and barley. Field Crops Research, 195, 50–59.Girshick R. 2015. Fast R-CNN. In: IEEE International Conference on Computer Vision. IEEE Computer Society, USA. pp. 1440–1448.Girshick R, Donahue J, Darrell T, Malik J. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, USA. pp. 580–587.Girshick R, Donahue J, Darrell T, Malik J. 2015. Region-based convolutional networks for accurate object detection and segmentation. IEEE Transactions on Pattern Analysis Machine Intelligence, 38, 142–158.He K, Sun J. 2015. Convolutional neural networks at constrained time cost. In: Computer Vision and Pattern Recognition. IEEE Computer Society, USA. pp. 5353–5360.He K, Zhang X, Ren S, Sun J. 2014. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE Transactions on Pattern Analysis Machine Intelligence, 37, 1904–1916.He K, Zhang X, Ren S, Sun J. 2016. Deep residual learning for image recognition. In: Computer Vision and Pattern Recognition. IEEE Computer Society, USA. pp. 770–778.Hobson M, Carter M, Yan Y. 2009. Rule based concave curvature segmentation for touching rice grains in binary digital images. In: Instrumentation and Measurement Technology Conference, 2009. I2MTC 09. IEEE Computer Society, USA. pp. 1685–1689.Kiratiratanapruk K, Sinthupinyo W. 2010. Segmentation algoritm for touching round grain image. In: International Conference on Electronics and Information Engineering. IEEE, Japan. pp. V1-263–V1-266.Li J, Thomson M, McCouch R. 2004. Fine mapping of grain-weight quantitative trait locus in the pericentromeric region of rice chromosome 3. Genetics, 168, 2187–2195.Lin P, Chen M, He Y, Hu W. 2014. novel matching algorithm for splitting touching rice kernels based on contour curvature analysis. Computers Electronics in Agriculture, 109, 124–133.Liu T, Chen W, Wang Y, Wu W, Sun C, Ding J, Guo W. 2017. Rice and wheat grain counting method and software development based on Android system. Computers and Electronics in Agriculture, 141, 302–309.Liu T, Wu W, Chen W, Sun C, Chen C, Wang R, Zhu X, Guo W. 2016. shadow-based method to calculate the percentage 2008WU Wei et al. Journal of Integrative Agriculture 2020, 19(8): 1998–2008of filled rice grains. Biosystems Engineering, 150, 79–88.Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu Y, Berg C. 2015. SSD: Single Shot MultiBox Detector. ECCV, Netherlands. pp. 21–37.Lu J, Hu J, Zhao G, Mei F, Zhang C. 2017. An in-field automatic wheat disease diagnosis system. Computers Electronics in Agriculture, 142, 369–379.Pan J, Yang Q. 2010. survey on transfer learning. IEEE Transactions on Knowledge Data Engineering, 22, 1345–1359.Peltonen-Sainio P, Kangas A, Salo Y, Jauhiainen L. 2007. Grain number dominates grain weight in temperate cereal yield determination: Evidence based on 30 years of multi-location trials. Field Crops Research, 100, 179–188.Prystupa P, Savin R, Slafer A. 2004. Grain number and its relationship with dry matter, and in the spikes at heading in response to N×P fertilization in barley. Field Crops Research, 90, 245–254.Redmon J, Divvala S, Girshick R, Farhadi A. 2015. You only look once: Unified, real-time object detection. In: IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, USA. pp. 779–788.Sharada M, David H, Marcel S. 2016. Using deep learning for image-based plant disease detection. Frontiers in Plant Science, 7, 1419–1429.Shatadal P, Jayas S, Bulley R. 1995. Digital image analysis for software separation and classification of touching grains. I. Disconnect algorithm. Transactions of the ASAE, 38, 645–649.Slafer A, Savin R, Sadras O. 2014. Coarse and fine regulation of wheat yield components in response to genotype and environment. Field Crops Research, 157, 71–83.Srivastava K, Greff K, Schmidhuber J. 2015. Highway networks. In: International Conference on Machine Learning. ICML, France. pp. 1–6.Xiong X, Duan L, Liu L, Tu H, Yang P, Wu D, Chen G, Xiong L, Yang W, Liu Q. 2017. Panicle-SEG: robust image segmentation method for rice panicles in the field based on deep learning and superpixel optimization. Plant Methods, 13, 104–119.Executive Editor-in-Chief LI Shao-kunManaging editor WANG Ning