Contents lists available at ScienceDirectComputers and Electronics in Agriculturejournal homepage: www.elsevier.com/locate/compagSegmenting ears of winter wheat at /uniFB02owering stage using digital images anddeep learningJuncheng Maa, Yunxia Lib, Keming Dua, Feixiang Zhenga, Lingxian Zhangb,/uni204E, Zhihong Gongc,Weihua JiaodaInstitute of Environment and Sustainable Development in Agriculture, Chinese Academy of Agricultural Sciences, Beijing 100081, ChinabCollege of Information and Electrical Engineering, China Agricultural University, Beijing 100083, ChinacTianjin Climate Center, Tianjin 300074, ChinadCenter for Agricultural and Rural Economic Research, Shandong University of Finance and Economics, Jinan 250014, ChinaARTICLE INFOKeywords:Ears of winter wheatRGB imagesDeep convolutional neural networkFully convolutional networkSegmentationABSTRACTSegmenting ears of winter wheat from canopy images was considered to be an important procedure prior to theextraction of related traits. Current segmentation method based on computer vision was susceptible to noise,which is limited in practical applications. In this study, two-stage segmentation method for ears of winterwheat based on digital images of unit ground area and the state-of-the-art deep learning techniques was pro-posed. In the coarse segmentation stage, deep convolutional neural network (DCNN) was constructed toclassify the superpixels generated by entropy rate superpixel algorithm, achieving the coarse results. In the /uniFB01nesegmentation stage, fully convolutional network (FCN) allowing pixel-wise semantic segmentation was con-structed to eliminate the non-ear pixels in the coarse results. To compare the results of the proposed two-stagesegmentation method, conventionally adopted methods for image segmentation were used. Results showed thatthe proposed two-stage segmentation method was able to accurately segmenting ears of winter wheat fromcanopy images captured at /uniFB02owering stage (Qseg 0.7197, F1 score 83.70%, SSIM 0.8605), out-performing the other compared methods. Generalization tests were conducted to evaluate the utility of theproposed two-stage segmentation method. Results showed that the two-stage segmentation method was stillcapable of accurately segmenting ears of winter wheat, even though the performance slightly decreased. Changeof winter wheat cultivar and lack of descriptive information were two factors that could degrade the perfor-mance of the two-stage segmentation method. Tests of the methods on Unmanned Aerial Vehicle (UAV) basedRGB images showed the Fully Convolutional Network stride predictions (FCN-8s) had good chance to achievesatisfactory performances on UAV based canopy images.1. IntroductionTraits associated with ears of winter wheat, such as size and shape,are critical in phenotyping tasks. However, conventional methods for/uniFB01eld phenotyping of ears are tedious and costly of human /uniFB00orts, whichare limited in the large-scale and high-throughput applications. Withthe development of computer vision technology, image-based techni-ques provide promising way to extract traits related to ears of winterwheat Fernandez-Gallego et al., 2018; Madec et al., 2018; Zhou et al.,2018; Lu et al., 2017; Xiong et al., 2017; Zhu et al., 2016 ). Extractingears from canopy images captured by handheld mobile device or au-tomatic phenotyping platforms enabled to measure the traits associatedwith ears under /uniFB01eld conditions, thus increasing the throughput for/uniFB01eld phenotyping Rico-Fernández et al., 2019; Fernandez-Gallegoet al., 2018; Zhou et al., 2018; Hu et al., 2017; Xiong et al., 2017; Duanet al., 2015 ). Therefore, segmenting ears of winter wheat from canopyimages was considered to be an important procedure prior to the ex-traction of the traits. Zhu et al. (2016) proposed wheat ear detectionmethod to observe the heading stage of wheat. The images of wheatcanopy were captured by in- /uniFB01eld image acquisition devices and pro-cessed by collection of image processing techniques, generating afeature set for classi /uniFB01cation by shallow machine learning method.Fernandez-Gallego et al., (2018) proposed an ear /uni2011counting algorithm toestimate ear density under /uniFB01eld conditions based on digital imagehttps://doi.org/10.1016/j.compag.2019.105159Received 22 July 2019; Received in revised form 29 November 2019; Accepted 15 December 2019/uni204ECorresponding author at: College of Information and Electrical Engineering, China Agricultural University. 17, Qinghua East Road, Haidian District, Beijing100083, China.E-mail address: zhanglx@cau.edu.cn (L. Zhang).&RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH$YDLODEOHRQOLQH'HFHPEHU(OVHYLHU%9$OOULJKWVUHVHUYHG7processing. The method can automatically process the images capturedin/uniFB01eld conditions and count the number of ears per unit ground areawithout human intervention. Zhou et al. (2018) proposed computervision based method to recognize wheat ears from images capturedfrom the side view at 45 ◦above the horizontal. Following the ap-proximately same pipelines in Zhu et al., (2016) the method extractedlow-level image features and adopted shallow machine learning, twin-support-vector-machine, to classify the image pixels into two cate-gories. Although good accuracy was reported in the above work, it wasknown that noises from /uniFB01eld conditions, such as strong illuminationand clutter background, would severely in /uniFB02uence the accuracy of imageprocessing techniques Ma et al., 2018, 2017; Hamuda et al., 2017 ),moreover, the manually designed image features had weak general-ization ability Ma et al., 2019; Uzal et al., 2018; Tang et al., 2017 ),thus hindering the practical application of these methods.Deep Learning is currently the state-of-the-art technique for imageprocessing, which has been proved to be powerful tool for /uniFB01eldphenotyping of ears Ma et al., 2019; Ghosal et al., 2018; Madec et al.,2018; Ferreira et al., 2017; Lu et al., 2017; Xiong et al., 2017 ).Xionget al. (2017) proposed segmentation method for rice panicle in /uniFB01eldconditions. The authors used SLIC superpixel to build the input datasetfor Convolutional Neural Network (CNN). In the test stage, the methodadopted entropy rate superpixel for optimization, achieving bettersegmentation results than the compared approaches. However, theimage collection of the method required an imaging bracket with blackbackground to screen the excessive light and unwanted image pixels,which limited the practical application in /uniFB01eld conditions. Madec et al.(2018) proposed an ear density estimating method by using Faster-RCNN Ren et al., 2015 and RGB images of high spatial resolution. Inorder to show the validity of the proposed estimating method, the au-thors adopted TasselNet proposed by Lu et al. (2017) for comparison.TasselNet was local count regression network based on CNN, whichwas constructed for in- /uniFB01eld counting of maize tassels. Both of the twoapproaches achieved good accuracy, however, these two methods werefocusing on counting the number of ears. The TasselNet directly esti-mated the value of the estimated ear density from single canopyimage while the Faster-RCNN marked the position of all the ears in asingle canopy image, both of which were not able to extract ears from asingle canopy image precisely. Therefore, it was necessary to explore amore feasible and precise approach.In this paper, two-stage segmentation method for ears of winterwheat based on digital images of unit ground area and the state-of-the-art deep learning techniques was proposed. In the coarse segmentationstage, the entropy rate superpixel algorithm was performed to generatepatches of ear and non-ear, aiming to highlight the ears from canopyimage and build the input dataset for CNN classi /uniFB01cation. Based on theinput dataset, deep convolutional neural network (DCNN) was con-structed to perform coarse segmentation. In the /uniFB01ne segmentationstage, fully convolutional network (FCN) which allowed pixel-wisesemantic segmentation was constructed to eliminate the non-ear pixels,Fig. 1. Pipeline of the two-stage wheat ear segmentation method.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHthus optimizing the coarse segmentation results.2. Methods2.1. Experiment setup and image collectionThe experiment /uniFB01eld was located in the /uniFB01eld station of ShangqiuAcademy of Agriculture and Forestry Sciences in Shuangba, Henan,China (Lat: 34°32 ′21.1884 ″: N, Long: 115°43 ′8.0868 ″: E). Twelve plotsof winter wheat, whose size was m in length and 2.4 in width, weresown on 14th October 2017. Three target plant densities, i.e., 120, 270,and 420 plants/m2, were adopted, which meant that each plant densitywas applied to four replicates. The same amount of fertilizers was ap-plied to all the plots. Within each plot, three white markers of size1 × m were laid down on the canopy with certain distances be-tween every two of them. For more details about the Experiment setup,readers are referred to Ma et al. (2019) .The images were captured by Canon EOS 600D digital camerawhich was mounted on tripod. The camera oriented verticallydownwards over the canopy at distance of 1.5 m. Images were takenat focal length of 18 mm with an aperture of f/4, resulting in /uniFB01eld ofview of approximately seven rows of each plot. The canopy imageswere captured on May 2, 2018, which was the /uniFB02owering stage of winterwheat.According to the experimental setup, dataset for season 2018 wasconstructed, consisting of 36 raw images (12 plots 3 markers). Theimages were captured at 5184 3456 pixel resolution, which was thenmanually cropped to eliminate the pixels outside the markers and re-shaped to 2500 2500 pixel resolution, generating 36 ROI imageswith ground resolution of 0.16 mm Fig. ). Of the three ROI imagesfor each plot, one ROI image was randomly selected, resulting in adataset containing 12 ROI images for constructing the input dataset ofdeep learning methods.2.2. Coarse segmentationDCNN was able to achieve good classi /uniFB01cation with an input datasetcovering as much variability in /uniFB01eld conditions as possible. Many re-searchers have utilized superpixel segmentation to construct the inputdataset for CNN, especially the Simple Linear Iterative Clustering (SLIC)superpixels Zhou et al., 2018; Lu et al., 2017; Achanta et al., 2012 ). Inthis study, the coarse segmentation was expected to accurately detectall the ears in an ROI image, as well as to eliminate as many non-earpixels in the coarse segmentation results as possible. Therefore, entropyrate superpixel Liu et al., 2011 segmentation was adopted to build theinput dataset for the DCNN in coarse segmentation since it was parti-cularly useful in preserving edges and keeping the intrinsic homo-geneity within the superpixels. Taking number of 2000 superpixels foreach ROI image, dataset containing 24,000 images was constructed,which was then interactively labeled. Due to the entropy rate superpixelalgorithm, the generated superpixels were homogeneous without toomany noise pixels belonging to the other class, thus saving /uniFB00orts forthe interactive image labeling. The number of images for the twoclasses in the input dataset, i.e., ear and non-ear, was 6576 and 17,424respectively, which was then divided into training and test datasets in aratio of 8:2 by random selection. The training dataset consisted of19,199 images, 5260 of which belonged to class ear and 13,939 be-longed to class non-ear. The test dataset consisted of 4801 images, 1316of which belonged to class ear and 3485 belonged to class non-ear. Thestatistics of the datasets used for the construction of the DCNN areshown in Table .It is shown in Table that the number of images for the two classesin the training dataset was severely unbalanced, which may cause thesituation that the DCNN performs better on class non-ear than that onclass ear Ma et al., 2018; Ferreira et al., 2017 ). Therefore, it was ne-cessary to balance the number for the two classes in the training datasetprior to construction of the DCNN. For the purpose of improving thenumber of images for class ear, data augmentation was conducted. Theaugmentation method was used to translate the original image hor-izontally and vertically by 10 pixels respectively, which allowed for twoaugmented datasets. As result of the augmentation, the number ofimages for class ear was improved to 15,780, roughly equaling to thenumber of images for class non-ear. Sequentially, an augmentedtraining dataset consisting of 29,719 images was constructed, 23,775 ofwhich were used for training and 5944 for validation Table ).Considering the pixel resolution of the images in the input dataset,the architecture of the DCNN proposed in Ghosal et al. (2018) wasadopted in this paper due to its extraordinary abilities to accuratelyclassify images of small size Fig. ). The DCNN for coarse segmentationconsisted of /uniFB01ve convolution layers, /uniFB01ve batch normalization layers,four max pooling layers, and two fully connected layers. For the de-tailed information about network structure and parameter setting,readers are referred to Ghosal et al. (2018) .Prior to DCNN training, all the images were reshaped to 64 64pixel resolution to suit the input layer. The DCNN was then trainedusing NVIDIA Quadro P4000 (8 GB memory) with CUDA 9.0. Thestochastic gradient descent with momentum (SGDM) was used to op-timize the network weights, whose momentum was set to 0.9 and re-mained constant for the training process. The learning rate was in-itialized as 0.001 and dropped every 10 epochs by drop factor of 0.1.A mini-batch of 128 was used. L2 Regularization with regularizationfactor of 0.0005 was adopted to decrease the chance of over /uniFB01tting. Themaximum number of epochs used for training was set to 200. When thetest /uniFB01nished, the coarse segmentation of test ROI image can beachieved by utilizing the pretrained DCNN to perform classi /uniFB01cation ofthe generated superpixels. As mentioned above, the superpixels weregenerated using the entropy rate superpixels algorithm, however, therewere still pixels representing non-ear elements, such as the stem.Therefore, following the coarse segmentation of the ROI image, it wasnecessary to optimize the segmentation results to eliminate the non-earpixels, i.e., the /uniFB01ne segmentation.2.3. Fine segmentationIt was expected that the method for the /uniFB01ne segmentation should becapable of performing pixel-wise classi /uniFB01cation so that the noise pixelsin the coarse segmentation results can be eliminated. Given the re-quirement of the /uniFB01ne segmentation, the fully convolutional network(FCN) Long et al., 2015 was adopted for its remarkable abilities ofpixel-wise prediction. In order to train an FCN, it was necessary toconstruct the labeled pixel dataset at /uniFB01rst, which also utilized the 12ROI images for method construction Fig. ). However, if all the pixelsof the 12 ROI images were utilized for the labeled pixel dataset, atremendous amount of work would go into image labeling. In order toimprove the /uniFB03ciency of the segmentation method, an image samplingstrategy based on the ear orientation was developed Fig. ). It can beseen in Fig. that an ROI image can be roughly partitioned into 8blocks according to the ear orientation, thus making it possible to useone sub-image from each block to represent the whole ROI image.Based on the above image sampling strategy, eight sub-imagescorresponding to eight blocks were used to represent one ROI image,resulting in labeled pixel dataset containing 96 sub-images (twelveROI images eight blocks). The sub-image had size of 400 400,which was then labeled using the Image Labeler App of Matlab(MathWorks Inc., USA). When the image labeling /uniFB01nished, the labeledpixel dataset was divided into two subsets, i.e., training and test. Thetraining dataset consisted of 76 sub-images and the corresponding pixellabels. The test dataset consisted of 20 sub-images and the corre-sponding pixel labels. Image augmentation was applied to the trainingdataset. The augmentation method was to /uniFB02ip the image vertically andhorizontally, as well as to translate the images vertically and horizon-tally by 10 pixels, resulting in augmented datasets. The architecture ofJ. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHFCN in this paper was constructed using the VGG 16-layer net Longet al., 2015; Simonyan and Zisserman, 2014 ). Speci /uniFB01cally, the FCN-8snet was adopted for its excellent performance in semantic segmenta-tion. The same GPU for DCNN training was also utilized to train theFCN. The network weights were optimized using the SGDM with amomentum of 0.9. The learning rate was initialized as 0.001 and re-mained constant for the training process. mini-batch of was used, aswell as the L2 Regularization with regularization factor of 0.0005. TheTable 1Statistics of the datasets used for the construction of the DCNN.Categories Original dataset Augmentation Augmented datasetTraining Test Training Test Training Validation TestEar 5260 1316 Two datasets 12,624 3156 1316Non-ear 13,939 3485 \ 11,151 2788 3485Total 19,199 4801 10,520 23,775 5944 4801Fig. 2. Architecture of DCNN Ghosal et al., 2018 ).Fig. 3. Pipeline of the image sampling strategy.Fig. 4. Pipelines for /uniFB01ne segmentation.Table 2Confusion matrix of the DCNN for coarse segmentation.Class Ear Non-ear Precision (%) Sensitivity (%) F1 score (%)Ear 1249 108 92.0 94.9 93.4Non-ear 67 3377 98.1 96.9 97.5Accuracy (%) 96.4J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHmaximum number of epochs used for training was set to 30. When thetest of the FCN-8s net /uniFB01nished, the /uniFB01ne segmentation result for an ROIimage can be achieved by following the pipelines Fig. ).(1) Spilt the coarse segmentation result of an ROI image into sub-images according to the number of connected components.(2) For each sub-image, enlarge the image size by padding the edgeswith extra zero pixels. Use sliding window and the FCN-8s net toperform pixel-wise classi /uniFB01cation, obtaining the optimized result ofthe sub-image.(3) Update the coarse segmentation result with the optimized sub-Fig. 5. Coarse segmentation results, (a) Original images, (b) Ground truth images, (c) Coarse segmentation results, the red pixels were the incorrectly classi /uniFB01edpixels, (d) Evaluation metrics for the corresponding image, (e) Mean values of the evaluation metrics for coarse segmentation, (f) Standard deviations of theevaluation metrics for coarse segmentation.Table 3Confusion matrix of the FCN-8s net for /uniFB01ne segmentation.Class Ear Non-ear Precision (%) Sensitivity (%) F1 score(%)Ear 760,733 135,038 84.92 86.75 85.83Non-ear 116,162 2,188,067 94.96 94.19 94.57Accuracy (%) 92.15J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHFig. 6. Fine segmentation results, (a) Original images, (b) Ground truth images, (c) Fine segmentation results, (d) evaluation metrics, (e) Mean values of theevaluation metrics for /uniFB01ne segmentation, (f) Standard deviations of the evaluation metrics for /uniFB01ne segmentation.Table 4The details of the down-sampled image datasets.Dataset Down sampling factor Ground resolution Range of the NOSPs (interval: 500)Dataset for 2018 0.16 mm [500,4000]Down-sampled dataset_1 0.7 0.33 mm [500,3500]Down-sampled dataset_2 0.5 0.64 mm [500,3000]Down-sampled dataset_3 0.4 mm [500,2000]J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHFig. 7. Performances of the two-stage segmentation method over the down-sampled datasets, (a) dataset for 2018 with ground resolution of 0.16 mm (no downsampling), (b) Down-sampled dataset_1 with ground resolution of 0.33 mm, (c) Down-sampled dataset_2 with ground resolution of 0.64 mm, (d) Down-sampleddataset_3 with ground resolution of mm.Fig. 8. NOSPs adopted in the best performances of the two-stage segmentation method over the down-sampled datasets (left axis), the right axis showed theevaluation metrics of the corresponding best performance.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHimages.2.4. Performance evaluationIn order to show the validity of the two-stage segmentation method,as well as to understand the limitations, comparative tests and gen-eralization tests were performed. For the comparative tests, the two-stage segmentation method was compared to other widely used seg-mentation methods. The K-means clustering algorithm was selected asone of the compared methods for its ability to perform automatic seg-mentation Ma et al., 2017 ). For the purpose of segmentation usingautomatic clustering, the ROI images were converted to CIELab colorspace Ma et al., 2017; García-Mateos et al., 2015 ). The ear countingalgorithm proposed by Fernandez-Gallego et al., (2018) was adopted asone of the compared methods. Since the pixel resolution of the imageswas di /uniFB00erent, the original window size used in the ear counting algo-rithm was not applicable to canopy images in this study. Based on thetest results, the window size was /uniFB01nally determined as 32. Anothercompared method was random forest Breiman, 2001 ), which wasconstructed using the color features of the superpixels. The color fea-tures included the minimum, maximum, mean and standard deviationvalues of each channel of RGB, HSV and CIELab color spaces. Asmentioned in 2.3, the FCN-8s net was able to perform pixel-wise clas-si/uniFB01cation, making it feasible method to segment ears of winter wheatfrom digital images. Therefore, in the comparative tests, the FCN-8s netwhich was constructed for the /uniFB01ne segmentation was applied directly tothe ROI images. Given that the input image size for the FCN-8s net was400 400 pixels, the ROI images were divided into sub-images using asliding window. No overlap between the sub-images was necessarysince the FCN-8s net had the advantage of pixel-wise classi /uniFB01cation. Forthe generalization tests, the two-stage segmentation method was gen-eralized to image datasets with new variability. In order to quantitivelyevaluate the performance of the segmentation methods, /uniFB01ve widelyused evaluation metrics, i.e, Qseg, Precision, Sensitivity, F1 score, andstructural similarity index (SSIM), were adopted in this study Zhouet al., 2018; Ma et al., 2018, 2017; Ferreira et al., 2017; Xiong et al.,2017 ). For the four evaluation metrics, i.e, Qseg, Precision, Sensitivityand F1 score, confusion matrices were involved for the calculation(Powers, 2007 ). For the SSIM, the method proposed by Wang et al.(2004) was used.=++QsegTPTP FP FN (1)=+PrecisionTPTP FP (2)=+SensitivityTPTP FN (3)=/uni2217/uni2217+Fs o ePrecision SensitivityPrecision Sensitivity1 2(4)where TP was the number of ear pixels that were correctly classi /uniFB01ed aslass ear and FP was the number of background pixels that were in-correctly classi /uniFB01ed as class ear. FN was the number of ear pixels thatwere incorrectly classi /uniFB01ed as class background and FP was the numberof background pixels that were correctly classi /uniFB01ed as class background.3. Results and discussion3.1. Construction of the two-stage segmentation method3.1.1. Coarse segmentationWhen training /uniFB01nished, the test dataset was used to test the DCNN.The test results were shown in Table .It can be seen from the confusion matrix that 92.0% of the earpredictions were correct, as well as 94.9% of the ear cases, revealingthat the DCNN achieved an F1 score of 93.4% for class ear. Similarresults can be observed for class non-ear. The results showed that DCNNachieved an accuracy of 96.4%, indicating that the DCNN was capableof accurately distinguishing the two classes. In the following step, thepretrained DCNN was used to classify the superpixels generated fromthe ROI image into the two classes, achieving the coarse segmentationresult for the corresponding ROI image. Some coarse segmentation re-sults were shown in Fig. .As expected, non-ear pixels can be observed in the coarse segmen-tation results Fig. c, the red pixels), which agreed with the corre-sponding results of evaluation metrics presented in Fig. d. Quantita-tive evaluation of the coarse segmentation results was performed usingthe evaluation metrics mentioned in 2.4 Fig. e, and f). The resultsshowed that the coarse segmentation did not perform well on the me-trics of Qseg, Precision, F1 score, and SSIM, with the mean values of0.5837, 63.55%, 73.70%, and 0.7742, respectively, however, itachieved high score of sensitivity, with mean value of 87.76%(Fig. e). The detailed results of coarse segmentation on the dataset for2018 were shown in Appendix A, Table S1. Overall, the coarse seg-mentation was far from good segmentation, requiring the /uniFB01ne seg-mentation to further optimize the results. It was revealed form thecoarse segmentation results that the canopy images captured in /uniFB01eldconditions had lot of noise that the brightness was not robust enoughto accurately distinguish ear from the background. Although the en-tropy rate superpixels algorithm which had an advantage of keeping theintrinsic homogeneity was used to generate superpixels for the ROIimages, it was unavoidable that there were noise pixels in the results.Nevertheless, what was signi /uniFB01cant about the coarse segmentation wasthat most of the ear pixels in ROI images, i.e., nearly 90%, were cor-rectly segmented so that the /uniFB01ne segmentation could just focus oneliminating the non-ear pixels in the coarse segmentation results.31×85 56 ×55 45 ×39 57 ×47Fig. 9. Some of the misclassi /uniFB01ed superpixels by the DCNN in Down-sampleddataset_3, the NOSPs was 1000, the bottom row was the size of the corre-sponding superpixel.Table 5The statistics of the evaluation metrics for comparative tests (mean value standard deviation).Methods Qseg Precision (%) Sensitivity (%) F1 score (%) SSIMProposed method 0.7197 0.0124 82.9 1.43 84.54 1.29 83.7 0.84 0.8605 0.0069Auto-clustering 0.3594 0.0222 36.56 2.32 95.49 0.79 52.83 2.42 0.4684 0.0212Ear counting algorithm 0.3650 0.0269 69.36 2.88 43.68 4.12 53.42 2.87 0.7848 0.0136Random forest 0.4097 0.0222 44.84 2.39 82.81 4.72 58.09 2.25 0.6471 0.0223FCN-8s net 0.6917 0.0124 72.32 1.49 94.09 0.78 81.77 0.87 0.8326 0.0075Bold values indicated the best performance by the methods.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH3.1.2. Fine segmentationIn order to test the performance of the FCN-8s net for /uniFB01ne seg-mentation, the confusion matrix was used. The results were shown inTable .It can be seen from the confusion matrix that the FCN-8s netachieved an overall accuracy of 92.15%. In the performance per class,the performance of the FCN-8s net on class non-ear was better than thaton class ear. The precision values for each class were 84.92%, and94.96% respectively, and the sensitivity values for each class were86.75% and 94.19%. The test results showed that the FCN-8s net wasable to eliminate the noise pixels. The /uniFB01ne segmentation results wereshown in Fig. .Signi /uniFB01cant improvement in the segmentation results can be ob-served Fig. c). Quantitative evaluation of the /uniFB01ne segmentation re-sults was also performed using the evaluation metrics Fig. d, e).Comparing the two results, it can be seen that the /uniFB01ne segmentationachieved improved results on the metrics of Qseg, Precision, F1 score,and SSIM, with mean values of 0.7197, 82.90%, 83.70%, and 0.8605respectively, while the result on the sensitivity slightly decreased, witha mean value of 84.54% Fig. e). The detailed results of /uniFB01ne seg-mentation on the dataset for 2018 were shown in Appendix A, Table S2.The observed improvement in the segmentation results could be at-tributed to the good performance of FCN-8s net. With the ability toperform pixel-wise classi /uniFB01cation, FCN-8s net optimized the coarsesegmentation results by correcting the classi /uniFB01cation results of the non-ear pixels. For the results of the FCN-8s net over the test dataset, it wasnot necessary to be concerned about the fact that performance on classbackground was better than that on class ear. Previous studies indicatedthat deep learning techniques can achieve satisfactory results with alarge mass of data Ferreira et al., 2017; Ma et al., 2019, 2018 ). It wasobvious that the data size of the two classes in the training set for FCN-8s net was severely unbalanced. The data size of class ear was heavilyoutnumbered by that of class background, thus making it expectablethat the FCN-8s net might achieve better performance on class back-ground. The results in this section indicated that the proposed two-stagewheat ear segmentation method achieved accurate segmentation resultsof the canopy images captured at /uniFB02owering stage of winter wheat. Inthe following section, the factors that might in /uniFB02uence the performanceof the proposed two-stage segmentation method were discussed.3.1.3. Preferable NOSPs for canopy images of di /uniFB00erent ground resolutionsThe number of superpixels (NOSPs) was an important hyperpara-meter, determining the size of the superpixels. Taking large numberresulted in small-sized superpixels that contained less descriptive in-formation for the DCNN, increasing the chances of misclassi /uniFB01cation. Inthe opposite case, the superpixels might contain noise pixels degradingthe accuracy of FCN-8s net. In /uniFB01eld use of the two-stage segmentationmethod, multiple devices might be involved, resulting in canopy imagesof di/uniFB00erent ground resolutions. It was therefore necessary to determinethe suitable NOSPs for canopy images of di /uniFB00erent ground resolutions.To understand the in /uniFB02uence of NOSPs and ground resolution of canopyimages to the performance of the two-stage segmentation method, asOriginal image Ground truth Proposed method Qseg =0.7340, Precision 85.15%, Sensitivity 84.18%, F1 score 84.66%SSIM 0.8595 Qseg =0.7258, Precision 83.85%, Sensitivity 84.37%, F1 score 84.11%, SSIM 0.8713 Qseg 0.7416, Precision 84.32%, Sensitivity 86.01%, F1 score 85.16%, SSIM 0.8593 Fig. 10. Examples of segmentation results.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHwell as to determine the preferable NOSPs for canopy images of dif-ferent ground resolutions, grid search were conducted.To simulate canopy images of di /uniFB00erent ground resolutions, theimage dataset for 2018 was down sampled by factors of 0.7, 0.5, and0.4 using bicubic interpolation. The down-sampled image datasets hadground resolutions of 0.33 mm, 0.64 mm and mm respectively.Details of the down-sampled image datasets can be found in Table .The performances of the two-stage segmentation method tested overthe down-sampled datasets were shown in Fig. . The detailed resultson the down-sampled image datasets were shown in Appendix B, TableS3-S6.It can be seen from the results that the two-stages segmentationAuto clustering Qseg =0.3884, Precision 39.60%, Sensitivity 95.29%, F1 score 55.95%, SSIM 0.4879 Qseg =0.3456, Precision 35.24%, Sensitivity 95.36%, F1 score 51.47%, SSIM 0.4677 Qseg 0.3808, Precision 38.72%, Sensitivity 95.84%, F1 score 55.16%, SSIM 0.4658 Ear counting algorithm Qseg =0.3542, Precision 74.50%, Sensitivity 40.31%, F1 score 52.31%, SSIM 0.7789 Qseg =0.3774, Precision 72.57%, Sensitivity 44.02%, F1 score 54.80%, SSIM 0.8037 Qseg =0.3650, Precision 69.10%, Sensitivity 43.62%, F1 score 53.48%, SSIM 0.7704 FCN-8s net Qseg 0.7067, Precision 74.13%, Sensitivity 93.80%, F1 score 82.18%, SSIM 0.8302 Qseg =0.6918, Precision 72.50%, Sensitivity 93.80%, F1 score 81.79%, SSIM 0.8411 Qseg =0.7114 Precision 74.25%, Sensitivity 94.45%, F1 score 83.14%, SSIM 0.8303 Randon forest Qseg 0.4374, Precision 47.86%, Sensitivity 83.56%, F1 score 60.86%, SSIM 0.6535 Qseg =0.4112, Precision 44.20%, Sensitivity 85.51%, F1 score =58.27%, SSIM 0.6578 Qseg =0.4415, Precision 48.25%, Sensitivity 83.87%, F1 score =61.26%, SSIM 0.6532 Fig. 10. (continued )J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHmethod achieved the best performance over the dataset for 2018 whenadopting the NOSPs of 2000. Similarly, the best performances over thedown-sampled datasets were achieved using the NOSPs of 2000, 1500,and 1000 respectively. The results derived from the grid search Fig. )showed that the NOSPs had second-order /uniFB00ect on the performancesof the two-stage segmentation method over the down-sampled datasets,thus making it possible to con /uniFB01rm the most suitable NOSPs for canopyimages of ground resolution ranging from 0.16 mm to mm for futurereference Fig. ).It also can be seen from Fig. that the performance of the two-stagesegmentation degraded with the decrease of the ground resolution. Thisdegradation in the performances was mostly explained by the facts thatthe superpixels generated from canopy images of relatively low groundresolution contained less descriptive information than those generatedFig. 11. The comparison results of the two methods on the dataset for season 2019.Fig. 12. The performances of the two methods on the dataset for the two seasons, (a) the proposed two-stage segmentation method, (b) FCN-8s net.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHfrom canopy images of relatively high ground resolution for the DCNN.This can be seen in Fig. . The size of the misclassi /uniFB01ed superpixels bythe DCNN was small, moreover, the percentage of ear pixels was low inthese small-size superpixels.3.2. Comparative tests with widely adopted segmentation methodsIn this section, the results of the comparative tests were presented.The statistics of the evaluation metrics for the comparative tests wereshown in Table . Some of the segmentation results were shown inFig. 10 It can be seen that the proposed two-stage segmentationmethod demonstrated superior results to the compared methods in theevaluation metrics of Qseg, Precision, F1 score, and SSIM. The proposedmethod had the four evaluation metrics equal to 0.7197, 82.90%,83.70%, and 0.8605 respectively. As for the evaluation metric of sen-sitivity, the auto-clustering algorithm outperformed the other segmen-tation methods, with mean value of 95.49%. The sensitivity value ofthe proposed two-stage segmentation method was 84.54%. For thedetailed results of the compared methods on the dataset for season2018, readers were referred to Appendix C, Table S7a and S7b.The results revealed that both of the proposed two-stage segmen-tation method and the FCN-8s net were able to correctly segment thewinter wheat ears, moreover, by comparison, the proposed two-stagesegmentation method performed better than the FCN-8s net. The reasonthat the proposed two-stage segmentation method outperformed theFCN-8s net was rooted in the coarse segmentation stage, which pro-tected the /uniFB01ne segmentation stage from being exposed to the variabilitythat was not covered in the training dataset. large amount of non-earpixels, i.e., leaves and stems, can be observed in the segmentation re-sults of the auto-clustering algorithm and the random forest method,indicating that the method was severely /uniFB00ected by the clutter back-ground Fig. 10 ). possible explanation for the poor performances ofthe two methods might be that they were counting on the color in-formation to discriminate ears from background in ROI images. Asmentioned in Section 3.1 the canopy images captured under /uniFB01eldconditions had lot of noise, which had severe in /uniFB02uences on the color.Besides, from the perspective of color, the ears of winter wheat at/uniFB02owering stage were very similar to leaves. In the case of the earcounting algorithm, the above explanation also applied, moreover, itwas using /uniFB01xed value for the parameters, which cannot cover all thevariability of di /uniFB00erent image datasets. In summary, the results of thecomparative tests suggested the proposed two-stage segmentationmethod was reliable method for canopy images captured at /uniFB02oweringstage of winter wheat.Fig. 13. The performance of the two-stage method over the dataset for season2019 by transfer learning.Original image Ground truthProposed method, Qseg =0.5175, Precision 72.84%, Sensitivity 64.11%, F1 score 68.20%, SSIM 0.8639 FCN-8s net, Qseg =0.6111, Precision 66.59%, Sensitivity 88.12%, F1 score 75.86%, SSIM 0.8625Original image Ground truthProposed method, Qseg =0.4837, Precision 66.89%, Sensitivity 63.61%, F1 score 65.21%, SSIM 0.8725FCN-8s net, Qseg =0.5307, Precision 59.33%, Sensitivity 83.41%, F1 score 69.34%, SSIM 0.8538Original image Ground truthProposed method, Qseg =0.4957, Precision FCN-8s net, Qseg =0.5860, Precision 70.54%, Sensitivity 62.51%, F1 score 66.28%, SSIM 0.836963.28%, Sensitivity 87.74%, F1 score 73.89%, SSIM 0.8292 Fig. 14. The performances of the two methods over the low ground resolution images.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH3.3. Generalization tests using image datasets with new variabilitiesIn order to validate the utility of the two-stage segmentationmethod, experiments of the proposed method on dataset for theseason 2019 were conducted. The dataset for the season 2019 con-taining 36 ROI images was constructed by following the same protocolsof the dataset for season 2018. Since the FCN-8s net demonstrated greatpotential for segmenting ears of winter wheat, it was adopted as thecompared method. It was worth noting that the growth of winter wheatin season 2019 was /uniFB00ected by the frost damage, adding new variabilityto the images in the dataset, such as di /uniFB00erent ear sizes and extra soilpixels due to less dense canopy Fig. 11 a). The comparison results ofthe two methods over the dataset for season 2019 were shown inFig. 11 b. The NOSPs for the two-stage segmentation method was 2000.The results showed that there were four evaluation metrics belowthe 1:1 line, i.e., Qseg, Precision, F1 score, and SSIM, indicating that theproposed two-stage segmentation method achieved superior results tothe FCN-8s net on the four evaluation metrics. The proposed methodhad the four evaluation metrics equal to 0.6612, 76.10%, 79.59%, and0.8472 respectively, and the FCN-8s net had the evaluation metricsequal to 0.6514, 67.64%, 78.84%, and 0.8247 respectively. The sensi-tivity was above the 1:1 line, revealing that the FCN-8s net out-performed the proposed two-stage segmentation method on this eva-luation metric. The proposed method had the sensitivity equal to83.75%, and the FCN-8s net had the sensitivity equal to 94.80%. Thedetailed results on the dataset for season 2019 were shown in AppendixD, Table S8. It can be seen that the comparison results that the per-formance of the two-stage segmentation method over the dataset forseason 2019 was superior to that of the FCN-8s net agreed with that onthe dataset for season 2018. The performances of the two methods onthe dataset for the two seasons were shown in Fig. 12 .For the evaluation results of the proposed two-stage segmentationmethod Fig. 12 a), it can be observed that the sensitivity was very closeto the 1:1 line while the other four evaluation metrics of Qseg, Preci-sion, F1 score, and SSIM were below the 1:1 line, indicating that themethod maintained its ability to correctly segment all the ear pixels.Compared to the performance on the dataset for season 2018, theproposed two-stage method made more errors of predicting non-earpixels as ear pixels. Although the performance of the proposed two-stage segmentation method on the dataset for season 2019 slightlydecreased (Qseg 0.6612, F1 score 79.59%, SSIM 0.8472), it wasstill capable of segmenting winter wheat ears from canopy imagescaptured at /uniFB02owering stage. The similar results can be observed on theevaluation results of the FCN-8s net Fig. 12 b). In order to improve theperformance the proposed two-stage method over the dataset for season2019, transfer learning was adopted. The training, validation and testsets were constructed by following the same protocols for the datasetfor season 2018. The results were shown in Fig. 13 The detailed resultson the dataset for season 2019 by transfer learning were shown inAppendix D, Table S9.It can be seen from the results that there were four evaluationmetrics of Qseg, Precision, F1 score, and SSIM were below the 1:1 line,indicating that the method was able to segment the ear pixels with moreprecision. The evaluation metric of sensivity was above the 1:1 line,indicating that some of the ear pixels were misclassi /uniFB01ed as backgroundduring the process of optimization. In general, transfer learning didimprove the performance of the proposed two-stage method over thedataset for season 2019 (Qseg 0.6884, F1 score 81.51%,SSIM 0.8704), enabling it to segment ears from the datasets for bothseason 2018 and season 2019.To further assess the generalization ability of the two-stage seg-mentation method, tests were performed using canopy image of coarserresolution. The images were captured at the experimental station ofTianjin climate center, Tianjin, China. The winter wheat cultivar wasJimai 22. The device for image capture was smart phone (MEIZU M5note), which was at height of m and oriented vertically downwards.The pixel resolution of the images was 1440 2560, resulting in aground resolution of 0.5 mm. Nine canopy images were adopted to testthe performance of the two methods, and some of the results wereshown in Fig. 14 According to Fig. , NOSPs of 2000 was adopted forthe two-stage segmentation method. The detailed results on the ninecanopy images were shown in Appendix D, Table S10Results showed decreased performances for both methods as eval-uated over the nine canopy images Table ). This result was expectedsince the images presented new variability, such as relatively lowground resolution and di /uniFB00erent winter wheat cultivars. It was believedthat the decreased performances for the two methods were caused bydi/uniFB00erent reasons. For the FCN-8s net, it can be seen from Fig. 14 hatthere were noise pixels that misclassi /uniFB01ed as class ear while the FCN-8snet achieved mean sensitivity value of 86.34%. It was thereforeconcluded that the decreased performance for FCN-8s net was causedby the pixels which were not covered in the training set. For the case ofthe two-stage method, it can be seen in Fig. 14 that some of the earpixels were misclassi /uniFB01ed as class background, indicating that the DCNNin the coarse segmentation failed to correctly classify these superpixels.Fig. 15 demonstrated some of the superpixels misclassi /uniFB01ed by theDCNN.It was revealed that the winter wheat cultivar had an in /uniFB02uence onthe two-stage segmentation method since the superpixels in the /uniFB01gurewere containing few noise pixels. However, due to the fact that the two-stage segmentation method correctly classi /uniFB01ed over 65% ear pixels(Appendix D, Table S10), the reason for the misclassi /uniFB01cations was not asingle factor, but integrating other factors. According to the predictedscores of the misclassi /uniFB01ed superpixels Fig. 15 bottom row), one cansee that the DCNN performed better on the superpixels with rich detailsof ear, indicating the lack of descriptive information caused by lowground resolution was the ‘other factor ’. Therefore, it was concludedthat both the change of winter wheat cultivar and the lack of de-scriptive information contributed to the misclassi /uniFB01cation of the two-stage segmentation method. Nevertheless, the results laid the fact thatthe proposed two-stage segmentation method had good generalizationability, which was robust tool for segmenting ears of winter wheatfrom canopy images captured at the /uniFB02owering stage.3.4. FCN-8s net demonstrated good potential for UAV-based canopy imagesIn order to test the potentials of the method for segmenting ears ofwinter wheat from UAV-based RGB images, experiments were con-ducted at the experimental station of the Institute of Environment andSustainable Development in Agriculture, Chinese Academy ofAgricultural Sciences located in Shunyi, Beijing, China. The winterwheat cultivar was Zhongmai 1062. The canopy images of winter wheatEars: 0.0418 Background: 0.9582 Ears: 0.2179 Background: 0.7821 Ears: 0.3282 Background: 0.6718 Ears: 0.4283 Background: 0.5717 Fig. 15. Some of the superpixels mis-classi /uniFB01ed by the DCNN, the bottom rowdemonstrated the predicted scores of thecorresponding superpixels.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH74.56%, F1 score 63.58%, SSIM 0.8497Original image Ground truth FCN-8s net, Qseg =0.5188, Precision 63.97%, Sensitivity 73.30%, F1 score 68.32%, SSIM 0.8496Original image Ground truth FCN-8s net, Qseg =0.5456, Precision =72.56%, Sensitivity 68.75%, F1 score =70.60%, SSIM 0.8573Original image Ground truth FCN-8s net, Qseg =0.5025, Precision 68.85%, Sensitivity 65.04%, F1 score 66.89%, SSIM 0.8726Original image Ground truth FCN-8s net, Qseg =0.4739, Precision 59.63%, Sensitivity 69.78%, F1 score 64.31%, SSIM 0.8640Original image Ground truth FCN-8s net, Qseg =0.4660, Precision 55.14%, Sensitivity Fig. 16. The performance of the FCN-8s net over the UAV-based canopy images.J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHwere captured by an Unmanned Aerial Vehicle (UAV) on May 17, 2019,which was the early /uniFB02owering stage of winter wheat. The UAV forimage collection was the DJI Mavic pro (SZ DJI Technology Co.,Shenzhen, China), which was consumer-level drone with digitalcamera. The images which were captured from the nadir view directionat the height of m had pixel resolution of 4000 3000, resulting ina ground resolution of 0.18 mm. The two methods were tested over /uniFB01veUAV-based canopy images, and the results were shown in Fig. 16 .As expected, it can be observed from Fig. 16 that there were falsepositive pixels in the segmentation results. The performance of the FCN-8s net decreased as compared to that over the ground-based canopyimages (Qseg 0.5014, F1 score 66.74%, SSIM 0.8586). Thedetailed results on the UAV-based canopy images were shown in Ap-pendix D, Table S11. It can be seen from the above /uniFB01gure that the eardensity was very low and there were weeds in the /uniFB01eld, thus resulting ina huge amount of noise pixels which was not covered in the training setof the FCN-8s net. This may explain the decreased performance of theFCN-8s net. In the case of the two-stage segmentation method, di /uniFB00erentnumbers of superpixels in the coarse segmentation, i.e., 2000, 3500,and 5000, were tested to adapt to the pixel resolution of the UAV-basedRGB images. However, the DCNN was not able to distinguish the earsfrom background no matter which number was adopted, thus makingthe/uniFB01ne segmentation impossible to achieve valid segmentation results.Although the ground resolution of the UAV-based canopy images wasclose to that of the ground-based images in datasets for season 2018,the ear density was very low and the size of the ears was relativelysmall, which was just the scene that would invalidate the two-stagesegmentation method based on the analysis in Section 3.3 This mayexplain the failure of the two-stage segmentation method. On the otherhand, the FCN-8s net that trained with limited number of imagesachieved promising segmentation, which highlighted its potential forhigh throughput applications. It can be expected that the FCN-8s netwill achieve better results with considerable number of training images.4. ConclusionA two-stage segmentation method for ears of winter wheat based ondigital images and deep learning techniques was proposed in this paper.The results showed that the proposed two-stage segmentation methodwas able to achieve accurate segmentation of winter wheat ears fromcanopy images captured at /uniFB02owering stage (Qseg 0.7197, F1score 83.70%, SSIM 0.8605). The performance of the proposedsegmentation method was compared to the widely adopted segmenta-tion methods. Results showed that the proposed two-stage segmenta-tion method outperformed the compared methods, making reliabletool to segmentation of winter wheat ears from canopy images capturedat the /uniFB02owering stage. The results have also shown that the FCN-8s netdemonstrated great potentials for segmentation of winter wheat ears(Qseg 0.6917, F1 score 81.77%, SSIM 0.8326). In order tovalidate the utility of the proposed two-stage segmentation method,generalization tests were constructed. Results showed the two-stagesegmentation method was still capable of accurately segmenting ears ofwinter wheat, even though the performance slightly decreased. Changeof winter wheat cultivar and lack of descriptive information were twofactors that could degrade the performance of the two-stage segmen-tation method. Tests of the methods for segmenting ears of winterwheat from UAV-based RGB images showed the FCN-8s had goodchance to achieve satisfactory performances for UAV-based canopyimages.Although the FCN-8s net demonstrated great potentials for seg-menting the winter wheat ears, it is very necessary to enlarge the size ofthe training set, thus covering as much variability expected to occur inpractice as possible. In the following research, considerably more workwill need to be done to increase the number of images in the training setfor the FCN-8s net. Moreover, canopy images captured at other stageswill also be included in the dataset so that the method can extend itsapplication scope.CRediT authorship contribution statementJuncheng Ma: Conceptualization, Methodology, Software, Writing- original draft, Writing review editing, Funding acquisition. YunxiaLi:Software, Formal analysis, Data curation. Keming Du: Validation,Project administration, Funding acquisition. Feixiang Zheng:Validation, Writing review editing. Lingxian Zhang:Conceptualization, Methodology, Writing review editing,Supervision. Zhihong Gong: Validation, Resources. Weihua Jiao:Validation, Visualization.Declaration of Competing InterestThe authors declare that they have no known competing /uniFB01nancialinterests or personal relationships that could have appeared to in /uniFB02u-ence the work reported in this paper.AcknowledgmentsThe Authors wish to acknowledge The National Natural ScienceFoundation of China (31801264), Young Elite Scientists SponsorshipProgram by CAST (2018QNRC001) and The National Key Research andDevelopment Program of China (2016YFD0300606) for their fundingsupport of this research.Appendix A. Supplementary materialSupplementary data to this article can be found online at https://doi.org/10.1016/j.compag.2019.105159 .ReferencesAchanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., Süsstrunk, S., 2012. SLIC superpixelscompared to state-of-the-art superpixel methods. IEEE Trans. Pattern Anal. Mach.Intell. 34, 2274 –2281. https://doi.org/10.1109/TPAMI.2012.120 .Breiman, L., 2001. Random forests. Mach. Learn. 45 (1), –32.https://doi.org/10.1007/978-3-662-56776-0_10 .Duan, L., Huang, C., Chen, G., Xiong, L., Liu, Q., Yang, W., 2015. Determination of ricepanicle numbers during heading by multi-angle imaging. Crop J. 3, 211 –219. https://doi.org/10.1016/j.cj.2015.03.002 .Fernandez-Gallego, J.A., Kefauver, S.C., Gutiérrez, N.A., Teresa, M., Taladriz, N., Araus,J.L., 2018. Wheat ear counting in /uniFB01eld conditions high throughput and low costapproach using RGB images. Plant Methods –12.https://doi.org/10.1186/s13007-018-0289-4 .Ferreira, A. dos S., Freitas, D.M., Silva, G.G. da, Pistori, H., Folhes, M.T., 2017. Weeddetection in soybean crops using ConvNets. Comput. Electron. Agric. 143, 314 –324.https://doi.org/10.1016/j.compag.2017.10.027 .García-Mateos, G., Hernández-Hernández, J.L., Escarabajal-Henarejos, D., Jaén-Terrones,S., Molina-Martínez, J.M., 2015. Study and comparison of color models for automaticimage analysis in irrigation management applications. Agric. Water Manag. 151,158–166. https://doi.org/10.1016/j.agwat.2014.08.010 .Ghosal, S., Blystone, D., Singh, A.K., Ganapathysubramanian, B., Singh, A., 2018. Anexplainable deep machine vision framework for plant stress phenotyping. Proc. Natl.Acad. Sci. U. S. A. 115, 4613 –4618. https://doi.org/10.1073/pnas.1716999115 .Hamuda, E., Mc Ginley, B., Glavin, M., Jones, E., 2017. Automatic crop detection under/uniFB01eld conditions using the HSV colour space and morphological operations. Comput.Electron. Agric. 133, 97 –107. https://doi.org/10.1016/j.compag.2016.11.021 .Hu, Q. xia, Tian, J., He, D. jian, 2017. Wheat leaf lesion color image segmentation withimproved multichannel selection based on the Chan –Vese model. Comput. Electron.Agric. 135, 260 –268. https://doi.org/10.1016/j.compag.2017.01.016 .Liu, M., Tuzel, O., Ramalingam, S., Chellappa, R., 2011. Entropy rate superpixel seg-mentation. In: Proceedings of the 2011 IEEE Conference on Computer Vision andPattern Recognition, 2097 –2104. https://doi.org/10.1109/CVPR.2011.5995323 .Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semanticsegmentation. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 07 –12-June, 3431 –3440. https://doi.org/10.1109/CVPR.2015.7298965 .Lu, H., Cao, Z., Xiao, Y., Zhuang, B., Shen, C., 2017. TasselNet: Counting maize tassels inthe wild via local counts regression network. Plant Methods 13, –17.https://doi.org/10.1186/s13007-017-0224-0 .Ma, J., Du, K., Zhang, L., Zheng, F., Chu, J., Sun, Z., 2017. segmentation method forgreenhouse vegetable foliar disease spots images using color information and regiongrowing. Comput. Electron. Agric. 142, 110 –117. https://doi.org/10.1016/j.compag.2017.08.023 .J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUHMa, J., Du, K., Zheng, F., Zhang, L., Gong, Z., Sun, Z., 2018. recognition method forcucumber diseases using leaf symptom images based on deep convolutional neuralnetwork. Comput. Electron. Agric. 154, 18 –24.https://doi.org/10.1016/j.compag.2018.08.048 .Ma, J., Li, Y., Chen, Y., Du, K., Zheng, F., Zhang, L., Sun, Z., 2019. Estimating aboveground biomass of winter wheat at early growth stages using digital images and deepconvolutional neural network. Eur. J. Agron. 103, 117 –129. https://doi.org/10.1016/j.eja.2018.12.004 .Madec, S., Frederic, B., Liu, S., De Solan, B., Xiuliang, J., Lu, H., Duyme, F., Baret, F.,2018. Ear density estimation from high resolution RGB imagery using deep learningtechnique. Agric. For. Meteorol. 264, 225 –234 https://doi.org/S016819231830337X .Powers, D.M.W., 2007. Evaluation: From Precision, Recall and F-Factor to ROC,Informedness, Markedness Correlation 24.Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster R-CNN: Towards real-time object de-tection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell. 39 (6),1137 –1149. https://doi.org/10.1109/TPAMI.2016.2577031 .Rico-Fernández, M.P., Rios-Cabrera, R., Castelán, M., Guerrero-Reyes, H.I., Juarez-Maldonado, A., 2019. contextualized approach for segmentation of foliage in dif-ferent crop species. Comput. Electron. Agric. 156, 378 –386. https://doi.org/10.1016/j.compag.2018.11.033 .Simonyan, K., Zisserman, A., 2014. Very Deep Convolutional Networks for Large-ScaleImage Recognition –14. https://doi.org/10.1016/j.infsof.2008.09.005.Tang, J., Wang, D., Zhang, Z., He, L., Xin, J., Xu, Y., 2017. Weed identi /uniFB01cation based on K-means feature learning combined with convolutional neural network. Comput.Electron. Agric. 135, 63 –70.https://doi.org/10.1016/j.compag.2017.01.001 .Uzal, L.C., Grinblat, G.L., Namías, R., Larese, M.G., Bianchi, J.S., Morandi, E.N., Granitto,P.M., 2018. Seed-per-pod estimation for plant breeding using deep learning. Comput.Electron. Agric. 150, 196 –204. https://doi.org/10.1016/j.compag.2018.04.024 .Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., 2004. Image quality assessment:From error visibility to structural similarity. IEEE Trans. Image Process. 13, 600 –612.Xiong, X., Duan, L., Liu, L., Tu, H., Yang, P., Wu, D., Chen, G., Xiong, L., Yang, W., Liu, Q.,2017. Panicle-SEG: robust image segmentation method for rice panicles in the /uniFB01eldbased on deep learning and superpixel optimization. Plant Methods 13, –15.https://doi.org/10.1186/s13007-017-0254-7 .Zhou, C., Liang, D., Yang, X., Yang, H., Yue, J., Yang, G., 2018. Wheat ears counting in/uniFB01eld conditions based on multi-feature optimization and TWSVM. Front. Plant Sci. 9.https://doi.org/10.3389/fpls.2018.01024 .Zhu, Y., Cao, Z., Lu, H., Li, Y., Xiao, Y., 2016. In- /uniFB01eld automatic observation of wheatheading stage using computer vision. Biosyst. Eng. 143, 28 –41.https://doi.org/10.1016/j.biosystemseng.2015.12.015 .J. Ma, et al. &RPSXWHUVDQG(OHFWURQLFVLQ$JULFXOWXUH