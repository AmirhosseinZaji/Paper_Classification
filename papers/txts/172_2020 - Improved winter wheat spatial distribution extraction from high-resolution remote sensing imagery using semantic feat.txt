remote sensing ArticleImproved Winter Wheat Spatial DistributionExtraction from High-Resolution Remote SensingImagery Using Semantic Features andStatistical AnalysisFeng Li1,2,†, Chengming Zhang3,4,*,†, Wenwen Zhang3,†, Zhigang Xu5, Shouyi Wang3,Genyun Sun6and Zhenjie Wang61School of Geosciences, China University of Petroleum (East China), Qingdao 266580, China;B18010057@s.upc.edu.cn2Shandong Provincial Climate Center, NO.12 Wuying Mountain Road, Jinan 250001, China3College of Information Science and Engineering, Shandong Agricultural University, 61 Daizong Road,Taian 271000, China; 2018120639@sdau.edu.cn (W.Z.); 2017110610@sdau.edu.cn (S.W.)4Shandong Technology and Engineering Center for Digital Agriculture, 61 Daizong Road, Taian 271000, China5School of Computer Science, Hubei University of Technology, 28 Nanli Road, Wuhan 430068, China;bocog@hotmail.com6College of Ocean and Space Information, China University of Petroleum (East China), Qingdao 266580,China; sungenyun@upc.edu.cn (G.S.); sdwzj@upc.edu.cn (Z.W.)*Correspondence: chming@sdau.edu.cn; Tel.:+86-139-5382-3659†These authors are co-ﬁrst authors as they contributed equally to this work.Received: 23 December 2019; Accepted: February 2020; Published: February 2020/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046Abstract:Improving the accuracy of edge pixel classiﬁcation is an important aspect of usingconvolutional neural networks (CNNs) to extract winter wheat spatial distribution information fromremote sensing imagery. In this study, we established method using prior knowledge obtainedfrom statistical analysis to reﬁne CNN classiﬁcation results, named post-processing CNN (PP-CNN).First, we used an improved ReﬁneNet model to roughly segment remote sensing imagery in order toobtain the initial winter wheat area and the category probability vector for each pixel. Second, weused manual labels as references and performed statistical analysis on the class probability vectorsto determine the ﬁltering conditions and select the pixels that required optimization. Third, basedon the prior knowledge that winter wheat pixels were internally similar in color, texture, and otheraspects, but di↵erent from other neighboring land-use types, the ﬁltered pixels were post-processedto improve the classiﬁcation accuracy. We used 63 Gaofen-2 images obtained from 2017 to 2019 ofa representative Chinese winter wheat region (Feicheng, Shandong Province) to create the datasetand employed ReﬁneNet and SegNet as standard CNN and conditional random ﬁeld (CRF) aspost-process methods, respectively, to conduct comparison experiments. PP-CNN’s accuracy (94.4%),precision (93.9%), and recall (94.4%) were clearly superior, demonstrating its advantages for theimproved reﬁnement of edge areas during image classiﬁcation.Keywords:convolutional neural network; semantic features; statistical features; Gaofen-2 imagery;winter wheat; post-processing; spatial distribution; Feicheng; China1. IntroductionDetermining the accurate spatial distribution of winter wheat is of great signiﬁcance for agriculturalproduction management, crop yield estimation, and national food security [1,2]. Remote sensingimagery has become the main source of such data characterizing this information. Image segmentationRemote Sens.2020,12, 538; doi:10.3390/rs12030538www.mdpi.com/journal/remotesensingRemote Sens. 2020,12, 538 of 18technology is now widely used to produce pixel-by-pixel classiﬁcation results that can extract widerange of spatial distribution information 3,4]. The speciﬁc pixel feature extraction method and theclassiﬁer both have decisive impacts on the accuracy of the classiﬁcation results 5].E↵ective features can improve the accuracy of the classiﬁcation result. The fundamental goal offeature extraction methods is to clearly di ↵erentiate the feature value of given object type from that ofother types 6,7]. Based on statistical analysis, an ↵ective feature extraction method can be obtained.For example, spectral indexes, which have been widely used in the classiﬁcation of middle- andlow-resolution remote sensing imagery, are obtained by statistical analysis of the spectral informationof the pixels 8]. Commonly used methods include various vegetation indexes 9,10], the AutomatedWater Extraction Index (AWEI) 11], the Normalized Di ↵erence Built-up Index (NDBI) 12], and theRemote Sensing Ecological Index (RSEI) 13]. The Enhanced Vegetation Index (EVI) 8], NormalizedDi↵erent Vegetation Index (NDVI) 10], and other indexes derived from NDVI are ↵ective at extractingvegetation information and have been widely used for extracting crop spatial distributions fromlow-resolution remote sensing imagery. Some researchers have taken advantage of the high temporalresolution of middle- and low-spatial resolution remote sensing imagery to obtain the spectral indexcharacteristics of time series before extracting crop information with good results 14–16]. Whenapplying statistical analysis technology to high-resolution remote sensing images, it is necessary tofully consider the impact of increasingly detailed pixel information on the extraction results 6,8,10].When classifying high spatial resolution remote sensing imagery, information for both thetarget pixel and adjacent pixels must be considered 17,18]. Texture features are commonly used toexpress information related to adjacent pixels 19]; these can be extracted by methods including thegray level of co-occurrence matrix (GLCM) 20], Gabor ﬁlters 21], Markov random ﬁelds 22], andwavelet transforms 23]. As texture features can accurately express the spatial correlation betweenpixels, combining these with spectral features can ↵ectively improve the classiﬁcation accuracy ofhigh-resolution remote sensing imagery 24]. The combination of traditional texture feature extractionmethods can obtain more ↵ective features 23,25].The development of machine learning has allowed researchers to use machine learning abilitiesto improve pixel feature extraction. However, early machine learning methods such as neuralnetworks 26,27], support vector machines 28,29], decision trees 30,31], and random forests 32,33]still use pixel spectral information as input. Although these methods can be ↵ective at obtainingfeatures, these remain single-pixel features, without utilizing the spatial relationships betweenadjacent pixels.The development of convolutional neural networks (CNNs) has greatly improved featureextraction. CNNs use trained convolution kernels to form feature extractor and then generatea feature vector for each pixel in the input image block 34,35]. Unlike other feature extraction methods,CNNs can simultaneously extract the features of given pixel and the spatial correlation featuresbetween adjacent pixels 36,37]. Classic CNNs include fully convolutional networks (FCNs) 38],SegNet 39], DeepLab 40], ReﬁneNet 41], and U-Net 42]. FCNs and SegNet only use high-levelsemantic features to generate the feature vectors of pixels, yielding very rough object edges 38,39].DeepLab uses CRFs to post-process the segmentation results outputted by CNN; this signiﬁcantlyimproves the quality of the results 40]. ReﬁneNet and U-Net use low-level ﬁne features and high-levelrough features to generate pixel-level feature vectors. This strategy is conducive to the expression ofmulti-depth information 41,42].ReﬁneNet and most other classic CNNs use two-dimensional convolution. The two-dimensionalconvolution method is suitable for processing images with small number of channels, such as cameraimages and optical remote sensing images 43,44]. Improved classic CNNs have been widely applied toremote sensing image segmentation 45] as well as target identiﬁcation 46–48], monitoring 49–51], andother ﬁelds. For example, CNNs have been successfully used to extract spatial distribution informationfor various crops, including wheat 52], rice 53], and corn 54]. Two-dimensional convolution methodsare unsuitable for processing images with many channels, such as hyperspectral remote sensingRemote Sens. 2020,12, 538 of 18images 55]. Aiming to preserve the spectral and spatial features of hyperspectral remote sensingimages, researchers use three-dimensional convolution to extract spectral–spatial information 55,56].Because three-dimensional convolution can fully utilize the abundant spectral and spatial informationof hyperspectral imagery, three-dimensional convolution has achieved remarkable success in theclassiﬁcation of hyperspectral images.When remote sensing images are segmented by CNNs, the intended results can be obtainedonly by using appropriate feature extraction methods and classiﬁcation methods according to thecharacteristics of the images 57,58]. CNN and traditional feature extraction methods have di ↵erentadvantages, and CNN cannot completely replace traditional feature extraction methods. The fusion ofdi↵erent feature extraction methods can improve the accuracy of the segmentation results 59].When CNNs are used for pixel classiﬁcation, the accuracy is high in the inner area but low in theedge area, resulting in rough edges 60,61]. Because the rough edges are caused by the di ↵erences infeature values between pixels of the same type, it is necessary to introduce appropriate post-processingmethods to improve the accuracy of edge pixel classiﬁcation 62–64]. The fully connected CRFcomprehensively uses the pixel spatial distance information and the semantic information generated bythe CNN to ↵ectively improve the edge accuracy of segmentation, but the amount of data required formodel calculation is too large. Researchers used recurrent neural networks 62] and convolution 63]to improve the calculation ciency. Reference 65] comprehensively used the pixel spatial distanceinformation and category information as constraints for network training to improve the accuracy ofimage segmentation results.Object-level information is an information category commonly used in post-processing methods; itincludes object shape information 65] and position information 65,66]. Using object-level informationto post-process the CNN segmentation results can improve the ﬁneness of the edges. Multiresolutionsegmentation algorithms 67] and patch-based learning 65,68] have been used to successfully generateimage object information. Classiﬁers are equally important; using more powerful classiﬁers suchas decision trees, the results obtained are better than those obtained by simple linear classiﬁers 69].Methods for extracting more knowledge and more suitable post-processing methods still requirefurther research.In order to obtain ﬁne winter wheat spatial distribution information from high spatial resolutionremote sensing imagery using CNNs, we proposed post-process CNN (PP-CNN) that uses priorknowledge of the similarity in color and texture between the inner and edge pixels of the target type andtheir di ↵erences from other types to post-process CNN segmentation results and ↵ectively improvethe accuracy of edge pixel classiﬁcation (and thus overall classiﬁcation). The main contributions of thiswork are as follows.• PP-CNN uses conﬁdence to evaluate the reliability of the pixel-by-pixel classiﬁcation resultsobtained using CNN and clariﬁes the calculation method of conﬁdence.• PP-CNN proposes new hierarchical classiﬁcation strategy. Features generated by standard CNNfrom the large receipt ﬁelds are used for the ﬁrst-level classiﬁer; features generated from the smallreceipt ﬁelds are used for the second-level classiﬁer. As this hierarchical classiﬁcation strategycombines the advantage of the large receipt ﬁeld and the small receipt ﬁeld, it thus achieves thegoal of obtaining ﬁne edges.2. Study Area and Data2.1. Study AreaFeicheng is county-level city covering 1277 km2in central-western Shandong Province, China(35530to 36190N, 116280to 116590E; Figure 1). This is an important Chinese production areafor commodity grains such as winter wheat (the main local crop). The area has warm temperatecontinental sub-humid monsoon climate with four distinct seasons; the average annual precipitation is645.7 mm, the average annual temperature is 13.6C, and the average annual sunshine duration isRemote Sens. 2020,12, 538 of 182281.3 h. Feicheng’s variable terrain includes mountains along its northern border and central hills,separated by several plains and rivers; its landscape and climate are representative of many Chineseregions, making it an appropriate study area for our purposes.Figure 1. Location and terrain of Feicheng in Shandong Province, China.2.2. Remote Sensing ImageryWe collected 63 Gaofen-2 (GF-2) remote sensing images as experimental data: 19 from 2017,23 from 2018, and 21 from 2019. Each image contained multi-spectral bands (blue, green, red, andnear-infrared) with 4-m resolution and panchromatic bands with 1-m resolution. After mosaicking,the images from each year covered Feicheng completely. As winter wheat has distinct characteristicsduring winter, all images were chosen during that time to improve the accuracy of the extraction results.We used Environment for Visualizing Images (ENVI) software to conduct four pre-processing stepsfor all images. First, multi-spectral and panchromatic orthographic correction was completed usingmeasured ground control points and the rational polynomial coe cient (RPC) model, based on 30-mresolution DEM data from the Shuttle Radar Topography Mission https: //earthexplorer.usgs.gov /).Second, radiometric calibration involved calibrating the multi-spectral data from the originaldigital number (DN) value to the equivalent radiance by:l=DN⇤g+b, (1)where lis the equivalent radiance obtained after conversion, DNis the DN value of the pixel, gisthe calibration coe cient, and bis the calibration ↵set; both gandbwere published by the ChinaResource Satellite Application Center http: //www.cresda.com /cn/).Third, atmospheric correction used the Fast Line-of-sight Atmospheric Analysis of Hypercubes(FLAASH) module in ENVI. Given the acquisition season, latitude, and land cover, the Sub-ArcticSummer model was adopted for the atmospheric model and the Rural model was adopted for theaerosol model; the initial visibility was 40 km.Fourth, fusion processing of the panchromatic and multi-spectral data used the Gram-SchmidtPan Sharpening module in ENVI. After fusion, the spatial resolution of the resulting image was mwith the red, blue, green, and near-infrared bands; each image was 7300 ⇥6900 pixels.Remote Sens. 2020,12, 538 of 182.3. Ground Survey DataThe main land-use types in the study area during winter include winter wheat, buildings, roads,woodland, water bodies, agricultural buildings, unplanted farmland, and other. In the GF-2 imagery,buildings, roads, water bodies, agricultural buildings, unplanted farmland, and other all have obviouscolor and texture features that can be easily distinguished visually. However, winter wheat andwoodland (especially some evergreen trees) are more similar in color and texture. To address this, weconducted ground investigations throughout the study area from December 2017 to January 2019,obtaining 119 samples (83 winter wheat and 36 woodland) for which the coordinates, type, and otherinformation were recorded, along with photos (Figure 2).Figure 2. Distribution of ground sampling points used to distinguish winter wheat from woodlandwithin the study area.2.4. Labeled Image DatasetWe selected 317 non-overlapping 960 ⇥720-pixel sub-regions within the fused image (Section 2.2),then labeled each manually. After labeling was completed, each sub-region corresponded to labelﬁle, forming an image–label pair (Figure 3). These ﬁles were single-band ﬁles in which the number ofpixel rows and columns was consistent with the corresponding image. Each labeled pixel was given acategory number: winter wheat (1), buildings (2), roads (3), water bodies (4), agricultural buildings (5),unplanted farmland (6), woodland (7), and other (8).Remote Sens. 2020,12, 538 of 18Figure 3. Example of image–label pair: a) original Gaofen-2 image and b) labeled image by pixel.3. MethodOur method consisted of three steps. First, the improved ReﬁneNet generated the initialsegmentation and outputted category probability vector for each pixel (Section 3.1). Second, theseinitial segmentations were statistically analyzed using manual labels as reference to determine theconﬁdence threshold (Section 3.2). Third, all pixels below the conﬁdence threshold were post-processedto generate their ﬁnal category label (Section 3.3).3.1. Initial Segmentation by CNNIn the common CNN structure, the feature extractor comprises multiple overlapping convolutionallayers, each of which was followed by pooling, batch normalization, and activation layers (Figure 4).The convolution layer contained several convolution kernels, most of which were ⇥3. The poolinglayer aggregated the features, which was beneﬁcial for screening out features with good discrimination.The batch normalization layer was used to normalize the feature values. The activation layer adopted anonlinear function. According to Hornik 70], the use of an activation layer facilitates better expressionsof the correlation features between similar pixels and better optimization of features.Figure 4. Basic structure of convolutional neural networks (CNNs) used for image segmentation.Remote Sens. 2020,12, 538 of 18The feature vector generator is generally composed of deconvolution layers, which can generatefeature vectors of equal length for each pixel. These generated feature vectors are used as the inputsfor the classiﬁer to determine the pixel category. Therefore, the deconvolution performance directlydetermines the model performance. At present, most CNNs used for image segmentation havesimilar feature extractor structures; they are mainly distinguished by their feature vector generators.For example, FCN uses the interpolation method as feature vector generator, while SegNet usesthe deconvolution kernel. More recent CNNs generate pixel-level feature vectors using traineddeconvolution kernels.Unlike other CNNs, ReﬁneNet 42] uses new “multipath” structure to fuse ﬁne low-level featuresand rough high-level features, ↵ectively improving the distinguishability of features and greatlyimproving the accuracy of segmentation results. The ReﬁneNet feature vector generator consists offour levels. Each level uses the results of both the higher-level semantic feature deconvolution and thefeature extractor at the same level as the input. This multi-level feature fusion strategy improves thedistinguishability of features.Considering the superior performance of the ReﬁneNet model, we chose this as the initialsegmentation model in our study. Similar to other CNNs, ReﬁneNet also employs the Softmax modelas classiﬁer.We used modiﬁed Softmax model as classiﬁer. The modiﬁed SoftMax model also takes apixel-level feature vector as the input, and calculates the probability of classifying the pixel into eachcategory. The category corresponding to the maximum probability value was assigned as the categoryof the pixel. The probabilities were organized into category probability vector. The output includedthe category probability vector and initial category for each pixel.3.2. Statistics for Initial Classiﬁcation ResultsStatistical analysis showed that the most pixels which had been correctly classiﬁed were locatedinside the winter wheat planting area, and the most pixels which had been incorrectly classiﬁedwere located at the edge of this area. Statistical analysis also showed that the di ↵erence betweenthe maximum probability value and the second-highest probability value was generally large in thecategory probability vectors of pixels that had been correctly classiﬁed, but that it was generally smallor nearly equivalent in the category probability vectors of pixels that had been incorrectly classiﬁed.We proposed the conﬁdence level (CL) as an indicator for the credibility of the CNN segmentationresults. The CL of category probability vector was calculated as:CL=pipj, (2)where pis category probability vector, piis the maximum value in p, and pjis the second-highestvalue in p.Our analysis showed that the classiﬁcation result for pixel could be considered credible if the CLof this pixel was higher than the minimum conﬁdence threshold (minCL); otherwise, it was considerednon-credible. Those pixels with CL values lower than minCL required post-processing. In our study,based on the statistical analysis of the training results, 0.21 was selected as minCL.3.3. Low-Conﬁdence Pixel Post-Processing3.3.1. Feature SelectionBased on the prior knowledge that the inner pixels and edge pixels in winter wheat planting areashave very similar colors and textures, and the near-infrared (NIR) band is sensitive to crops, we createda feature vector for each pixel using the red, blue, green, and near-infrared bands along with NDVI,uniformity (UNI), contrast (CON), entropy (ENT), and inverse di ↵erence (INV). NDVI was calculatedfollowing Wang et al. 10],Remote Sens. 2020,12, 538 of 18NDVI =NIR RedNIR+Red, (3)UNI, CON, ENT, and INV were extracted using the methods proposed by Yang and Yang, basedon GLCM 23],UNI =Xqi=1Xqj=1(g(i,j))2, (4)CON =Xq1n=0n2⇢Xqi=1Xqj=1g(i,j)whereij=n, (5)ENT =Xqi=1Xqj=1(g(i,j)logg(i,j) (6)INV =Xqi=1Xqj=1g(i,j)1+(ij)2, (7)In (4)–(7), qis the gray level quantization and g(i,j) is the element of GLCM.The feature vector vof each pixel had nine elements, structured as:v=(red,green ,blue,NIR,NDVI ,UNI,CON ,ENT ,INV) (8)3.3.2. Vector Distance Calculation MethodWe used the improved Euclidean distance to calculate the vector distance of the two featurevectors. The standard Euclidean distance is deﬁned as:d(x,y)=rXbi=1(xiyi)2, (9)where xandyare the feature vectors to be compared, xiandyiare the feature components, and bis the length of the feature vector. Smaller distances between the two feature vectors correspond togreater similarity. In the standard Euclidean distance, all elements are considered to have equal weight,without considering the inﬂuence of the aggregation degree of elements on the distance.Statistically, among the features of the samples of the same category, higher concentration of thevalue of certain feature corresponds to stronger distinguishability of this feature and greater weightthat should be assigned to this feature. Similarly, greater dispersion in the value of certain featurecorresponds to weaker distinguishability and smaller assigned weight of this feature.Based on prior knowledge, we introduced the reciprocal of the feature value distance as theweight factor to improve the Euclidean distance, thus better reﬂecting the inﬂuence of feature valueaggregation on the vector distance. This weight factor was calculated as:wi=1|max imin i|, (10)where iis the position number of the component in the feature vector, wiis the weight of the component,max iis the maximum value of the ith components of all feature vectors, and min iis the minimum valueof the ith components of all feature vectors. On this basis, the vector distance calculation formula was:d(x,y)=rXni=1wi(xiyi)2, (11)where xandyare the feature vectors to be compared, xiandyiare the feature components, wiis theweight of component i, and nis the component number of the feature vector.Remote Sens. 2020,12, 538 of 183.3.3. Vector Distance Threshold Determination• Firstly, each complete crop planting area in the training image was set as statistical unit.The vector distance dbetween each pixel and other pixels was calculated individually, and themaximum vector distance diof the unit was recorded, where iwas the number of the statistical unit.• Secondly, the vector distance threshold vdt) was obtained by:vdt=max1i⌧ndi, (12)where nis the number of statistical units.3.3.4. Low-conﬁdence Pixel ClassiﬁcationWe used the following steps to optimize the results of winter wheat planting areas outputted bythe improved ReﬁneNet model:• NDVI for each pixel was calculated;• UNI, CON, ENT, and INV for each pixel was calculated;• CL was calculated pixel by pixel;• Winter wheat pixels with continuous position and CL >minCL were divided into separate group;• For each group, the adjacent pixels for which CL <minCL were processed individually. For acertain adjacent pixel p, we calculated the vector distances between pand each pixel in the adjacentgroup and then chose the minimum value as the minimum distance mind Ifmind <vdt,pwasre-classiﬁed as winter wheat pixel.3.4. Experimental SetupWe conducted comparative experiment on graphics workstation with 12-GB internal graphicscard and Linux Ubuntu 16.04 operating system. TensorFlow 1.10 software was used to write thestatistical analysis and post-processing code in the Python language. Using ReﬁneNet model fromthe GitHub platform, we modiﬁed the output of the SoftMax model used by ReﬁneNet. We used thisfor initial segmentation and used the output as basic data for statistical analysis.We selected the SegNet and unmodiﬁed ReﬁneNet models as standard CNN and CRF as thepost-process method for comparison with PP-CNN (Table 1). SegNet works like ReﬁneNet, except ituses only high-level semantic features to generate feature vectors for each pixel.Table 1. Models used in the comparative experiment.Name DescriptionPP-CNN The proposed methodSegNet Classiﬁer using only high-level semantic featuresSegNet-CRFSegNet was used as the initial segmentation model, CRF was used as thepost-processing methodPP-SegNet As in PP-CNN, SegNet was used as the initial segmentation modelReﬁneNet Linear model was adopted for feature fusionReﬁneNet-CRFClassic ReﬁneNet was used as the initial segmentation model, CRF was used as thepost-processing methodBy comparing the results from SegNet and ReﬁneNet, we hoped to verify that the strategy ofgenerating features with ReﬁneNet was better than that of generating features with SegNet. Bycomparing the results of SegNet-CRF, ReﬁneNet, and ReﬁneNet-CRF with PP-CNN, we hoped to showthat post-processing could ↵ectively improve the accuracy of segmentation results. By comparing theresults of SegNet with PP-SegNet, we hoped to show that the proposed post-processing method hadstrong adaptability.Remote Sens. 2020,12, 538 10 of 18We applied data augmentation techniques onto the training dataset, including horizontal ﬂip,color adjustment, and vertical ﬂip steps. The color adjustment factors used included brightness, hue,saturation, and contrast. Each image in the training dataset was processed 10 times. All images createdby the data augmentation techniques were only used in training the CNNs.We used cross-validation techniques in the comparative experiments. Each CNN model wastrained over four rounds; in each round, 87 images were selected as test images and the other imageswere used as training images. Each image was used at least once as the test image (Table 2).Table 2. Percent of every category sample used in experiments.Category Percent of Total SamplesWinter wheat 39.00%Agricultural buildings 0.10%Woodland 9.01%Buildings 19.01%Roads 0.81%Water bodies 0.90%Unplanted farmland 24.12%Other 7.05%Table 3shows the hyper-parameter setup we used to train our model. In the comparisonexperiments, the hyper-parameters were also applied to the comparison model.Table 3. The hyper-parameter setup.Hyper-Parameter Valuemini-batch size 32learning rate 0.0001momentum 0.9epochs 200004. ResultsWe randomly selected ten test images from the test data set and assessed their segmentationresults using the SegNet, SegNet-CRF, PP-SegNet, ReﬁneNet, ReﬁneNet-CRF, and PP-CNN models(Figure 5).The six methods had very similar performances within the winter wheat planting areas, withvirtually no misclassiﬁcations. However, di ↵erences were obvious at the edges of these areas. PP-CNNand PP-SegNet misclassiﬁed only very small numbers of discrete pixels, while SegNet had the mosterrors in more continuous pattern, with errors being more common at corners than at edges. ReﬁneNethad signiﬁcantly fewer errors than the SegNet model, with most located near corners and few incontinuous patterns.Comparing SegNet-CRF and PP-SegNet, ReﬁneNet, and PP-CNN, respectively, it can be seen that,on the premise that the initial segmentation results are the same, the results obtained by post-processingusing the proposed method are better than those obtained by using CRF. Considering that CRF hasvery good performance in processing camera images, this may be because the resolution of remotesensing images is lower than that of camera images, which reduces the performance of CRF. It showsthat the appropriate post-processing method should be selected according to the image characteristics.Whether using CRF or the method proposed in this paper, the accuracy of the results afterpost-processing is improved, which also shows the importance of post-processing methods when CNNis applied to image segmentation.Remote Sens. 2020,12, 538 11 of 18Figure 5. Comparison of segmentation results for GF-2 satellite imagery for six test images: a)original image; b) manually labeled image; c) SegNet; d) SegNet-CRF (conditional random ﬁeld); e)PP-SegNet; f) ReﬁneNet; g) ReﬁneNet-CRF; h) PP-CNN.We then produced confusion matrix for the segmentation results for all four methods (Table 4),where each column represents the classiﬁcation result obtained from the segmentation results and eachrow represents the actual category deﬁned by manual classiﬁcation. PP-CNN was clearly superior, withclassiﬁcation errors accounting for only 5.6%, lower than the 13.7% for SegNet, 9.8% for SegNet-CRF,6.2% for PP-SegNet, 7.2% for ReﬁneNet, and 5.9% for ReﬁneNet-CRF.We used the accuracy, precision, recall, and Kappa coe cient to evaluate the performance of thefour models 45] (Table 5). The average accuracy of PP-CNN was 13.7% higher than SegNet, 7.2%higher than ReﬁneNet, and 6.2% higher than PP-SegNet.Table 6shows the average time required for each method to complete the testing of one image.The proposed post-processing method requires an approximate increase of 2% in time and improvesthe accuracy by 7.2%. The time consumed by CRF is higher than that consumed by the proposedRemote Sens. 2020,12, 538 12 of 18method because the CRF must calculate the distances between all pixel–pixel pairs for single image,while the proposed method must calculate the distances for only small number of pixel–pixel pairs.Table 4. Confusion matrix for winter wheat classiﬁcation.Approach Predicted Winter Wheat Non-Winter WheatSegNetWinter wheat 29.6% 9.4%Non-winter wheat 9.9% 51.1%SegNet-CRFWinter wheat 31.9% 7.1%Non-winter wheat 8.3% 52.7%PP-SegNetWinter wheat 33.1% 5.9%Non-winter wheat 5.9% 55.1%ReﬁneNetWinter wheat 32.5% 6.5%Non-winter wheat 6.3% 54.7%ReﬁneNet-CRFWinter wheat 35.3% 3.7%Non-winter wheat 7.8% 53.2%PP-CNNWinter wheat 36.9% 2.1%Non-winter wheat 3.5% 57.5%Table 5. Statistical comparison of model performance.Index SegNet SegNet-CRF PP-SegNet ReﬁneNet ReﬁneNet-CRF PP-CNNAccuracy 80.7% 84.6% 88.2% 87.2% 88.5% 94.4%Precision 79.7% 83.7% 87.6% 86.6% 87.7% 93.9%Recall 79.8% 84.1% 87.6% 86.5% 88.9% 94.4%Kappa 0.663 0.722 0.779 0.763 0.786 0.889Table 6. Statistical comparison of model performance.Index SegNet SegNet-CRF PP-SegNet ReﬁneNet ReﬁneNet-CRF PP-CNNTime [ms] 295 375 301 297 361 302*ms: millisecond5. Discussion5.1. Advantages of PP-CNNWhen an image is segmented pixel-wise by CNN, the accuracy of the results is determinedby the feature extractor, feature generator, and classiﬁer. The ﬁrst two use trained feature extractionrules to process remote sensing images and obtain feature vectors for each pixel, while the third usestrained classiﬁcation rules to process the acquired feature vectors and determine the pixel category.Therefore, both sets of rules aim to express the main common features of similar objects. In remotesensing images, the number of inner pixels for most objects is much larger than the number of edgepixels, such that the trained rules tend to reﬂect the inner features, making classiﬁcation errors morelikely at the edge of the object.In order to further illustrate the inﬂuence of pixel position on feature extraction, we deﬁned thepixel blocks used to calculate feature values as three types: internal (type A), in which the pixel blocksused are all composed of the same kind of pixel; edge (type B), in which the pixel blocks used contain~50% of other types of pixel; and corner (type C), in which the pixel blocks used contain 75% or moreof other categories of pixel (Figure 6). Considering that CNNs use the same convolution kernel forfeature extraction, it is clear that when the channel values of other categories of pixels in the calculatedpixel blocks are di ↵erent from the category of interest, the feature values of pixels in types A, B, and CRemote Sens. 2020,12, 538 13 of 18will be quite di ↵erent. Especially for type pixels, if the di ↵erence between the pixel value and theneighboring pixel value is large, the calculated feature value may be closer to the feature value rangeof the neighboring category. This makes it di cult to ↵ectively solve the problem of higher erroroccurrence in edge pixel segmentation simply by using CNN.Figure 6. Examples of the ↵ect of pixel position on the extracted features; pixel boxes (red) centeredon edge areas contain 50% or more non-winter wheat pixels.Statistical analysis showed that, although crop planting areas may have clear di ↵erences betweeninner and edge pixels in high-level semantic features, these remain quite similar in low-level features(such as color or texture). Considering the high accuracy of inner pixel classiﬁcation in our extractionresults, PP-CNN clearly integrated the advantages of CNNs and statistical features, thus signiﬁcantlyimproving the accuracy of the extraction results.5.2. Inﬂuence of Maximum Vector Distance Threshold on PP-CNN Segmentation ResultsThe PP-CNN method uses color, texture, and other features to compose feature vectors andcombines statistical analysis techniques to post-process the results of the CNN model, thereby providingimproved spatial distribution data for winter wheat. When performing post-processing, we ﬁrstcalculated the vector distance between low-conﬁdence pixels and nearby crop pixels with highconﬁdence. We then compared the obtained vector distance with the vector distance threshold obtainedby statistical analysis to determine whether low-conﬁdence pixels could be classiﬁed as winter wheat.We took the maximum vector distance calculated by all statistical units as the vector distance threshold.To compare the impact of vector distance thresholds on model performance, we used the minimumvector distance (method I), the average of all vector distances (method II), and the maximum vectordistance (method III) as the vector distance threshold, respectively, with the results shown in Table 7.Table 7. Comparison of PP-CNN model performance for minimum (I), average (II), and maximum (III)vector distances.Index II IIIPrecision 96.1% 94.8% 93.9%Recall 90.1% 92.5% 94.4%Remote Sens. 2020,12, 538 14 of 18Method III had the lowest precision but the highest recall rate, because using the maximumdistance as the threshold means that similar pixels of other categories are classiﬁed as winter whatpixels, thus reducing the accuracy. However, method ensures maximum winter pixel extraction.Therefore, when PP-CNN is applied in the real world, researchers should choose among the threemethods according to the extraction target and research goals.5.3. Inﬂuence of Feature Strategy on Classiﬁcation ResultsWe further compared SegNet and ReﬁneNET by analyzing the impact of feature extractionstrategies on the classiﬁcation results. We selected group of semantic features from the last layerof the SegNet and ReﬁneNet models having the greatest di ↵erence. We divided these features intothree groups of pixels: winter wheat edge, winter wheat inner, and non-winter wheat (Figure 7). Here,“inner” meant that when extracting the pixel features, only the winter wheat pixels included in thepixel block participated in the feature calculation; “edge” meant that pixels mixed with other categoriesparticipated. The feature results extracted by ReﬁneNet were more concentrated by type and betterdiscriminated between type; in comparison, the SegNet results were far less coherent. The featurefusion strategy adopted by the ReﬁneNet model was clearly more conducive to improving the accuracyof the results than SegNet’s strategy of only using high-level semantic features.Figure 7. Statistical comparison of extracted features for a) ReﬁneNet and b) SegNet.Remote Sens. 2020,12, 538 15 of 186. ConclusionsUsing CNNs to extract crop spatial distribution information from satellite remote sensing imageryhas become increasingly common. However, the use of CNNs alone usually results in very roughedge areas, with corresponding negative inﬂuence on overall accuracy. We used prior knowledgeand statistical analysis to optimize winter wheat CNN extraction results, especially with regard toedge areas.We analyzed the root cause of increased errors in CNN edge pixel classiﬁcation, then used thecategory probability vector output to calculate the results’ credibility, dividing these into high-credibilityand low-credibility pixels for subsequent processing. We then optimized the accuracy of the latter’sclassiﬁcation by analyzing the characteristics of planting area pixels using prior knowledge ofthe segmentation results. This new extraction strategy ↵ectively improved the accuracy of cropextraction results.Although the PP-CNN post-processing method proposed here was mainly established for cropextraction, it could be applied to the extraction of water, forest, grassland, and other land-use typeswith small internal pixel di ↵erences. However, for land-use types with larger internal di ↵erences, suchas residential land, other post-treatment feature organization methods must be developed. The maindisadvantage of our approach is the need for more manually classiﬁed images; future research shouldtest the use of semi-supervised classiﬁcation to reduce this dependence.Author Contributions: Conceptualization, C.Z. and F.L.; methodology, C.Z.; software, F.L. and S.W.; validation,W.Z. and F.L.; formal analysis, C.Z., F.L., and W.Z.; investigation, W.Z. and S.W.; resources, F.L.; data curation,Z.X.; writing—original draft preparation, C.Z., F.L., and W.Z.; writing—review and editing, C.Z., F.L., W.Z., G.S.,and Z.W.; visualization, Z.X., S.W., and W.Z.; supervision, C.Z.; project administration, C.Z.; funding acquisition,C.Z. All authors have read and agreed to the published version of the manuscript.Funding: This research was funded by the Science Foundation of Shandong, grant numbers ZR2017MD018;the Key Research and Development Program of Ningxia, Grant numbers 2019BEH03008; the National Key Rand Program of China, grant number 2017YFA0603004; the Open Research Project of the Key Laboratory forMeteorological Disaster Monitoring, Early Warning and Risk Management of Characteristic Agriculture in AridRegions, Grant numbers CAMF-201701 and CAMF-201803; the arid meteorological science research fund projectby the Key Open Laboratory of Arid Climate Change and Disaster Reduction of CMA, Grant numbers IAM201801.The APC was funded by ZR2017MD018.Conﬂicts of Interest: The authors declare no conﬂict of interest.References1. Atzberger, C. Advances in remote sensing of agriculture: Context description, existing operational monitoringsystems and major information needs. Remote Sens. 2013,5, 949–981. CrossRef ]2. Zhang, J.; Feng, L.; Yao, F. Improved maize cultivated area estimation over large scale combiningMODIS–EVI time series data and crop phenological information. ISPRS J. Photogramm. Remote Sens. 2014,94,102–113. CrossRef ]3. Mhangara, P.; Odindi, J. Potential of texture-based classiﬁcation in urban landscapes using multispectralaerial photos. S. Afr. J. Sci. 2013,109, 1–8. CrossRef ]4. Wang, F.; Kerekes, J.P.; Xu, Z.Y.; Wang, Y.D. Residential roof condition assessment system using deep learning.J. Appl. Remote Sens. 2018,12, 016040. CrossRef ]5. Jiang, T.; Liu, X.N.; Wu, L. Method for mapping rice ﬁelds in complex landscape areas based on pre-trainedconvolutional neural network from HJ-1 /B data. ISPRS Int. J. Geo Inf. 2018,7, 418. CrossRef ]6. El-naggar, A.M. Determination of optimum segmentation parameter values for extracting building fromremote sensing images. Alex. Eng. J. 2018,57, 3089–3097. CrossRef ]7. Zhang, B.; Liu, Y.Y.; Zhang, Z.Y.; Shen, Y.L. Land use and land cover classiﬁcation for rural residential areasin China using soft-probability cascading of multifeatures. J. Appl. Remote Sens. 2017,11, 045010. CrossRef ]8. Younes, N.; Joyce, K.E.; Northﬁeld, T.D.; Maier, S.W. The ↵ects of water depth on estimating FractionalVegetation Cover in mangrove forests. Int. J. Appl. Earth Obs. Geoinf. 2019,83, 101924. CrossRef ]Remote Sens. 2020,12, 538 16 of 189. Blaschke, T.; Feizizadeh, B.; Hölbling, D. Object-based image analysis and digital terrain analysis for locatinglandslides in the Urmia Lake Basin, Iran. IEEE J. Select. Top. Appl. Earth Obs. Remote Sens. 2014,7, 4806–4817.[CrossRef ]10. Wang, L.; Chang, Q.; Yang, J.; Zhang, X.H.; Li, F. Estimation of paddy rice leaf area index using machinelearning methods based on hyperspectral data from multi-year experiments. PLoS ONE 2018,13, e0207624.[CrossRef ]11. Feyisa, G.L.; Meilby, H.; Fensholt, R.; Proud, S.R. Automated Water Extraction Index: new technique forsurface water mapping using Landsat imagery. Remote Sens. Environ. 2014,140, 23–35. CrossRef ]12. Bhatti, S.S.; Tripathi, N.K. Built-up area extraction using Landsat OLI imagery. GISci. Remote Sens. 2014,51,445–467. CrossRef ]13. Xu, H.Q. remote sensing index for assessment of regional ecological changes. China Environ. Sci. 2013,33,889–897. CrossRef ]14. Wang, W.J.; Zhang, X.; Zhao, Y.D.; Wang, S.D. Cotton extraction method of integrated multi-features basedon multi-temporal Landsat images. J. Remote Sens. 2017,21, 115–124. CrossRef ]15. Kussul, N.; Lavreniuk, M.; Skakun, S.; Shelestov, A. Deep learning classiﬁcation of land cover and crop typesusing remote sensing data. IEEE Geosci. Remote Sens. Lett. 2017,14, 778–782. CrossRef ]16. Beyer, F.; Jarmer, T.; Siegmann, B. Identiﬁcation of agricultural crop types in northern Israel usingmultitemporal RapidEye data. Photogramm. Fernerkund. Geoinf. 2015,2015 21–32. CrossRef ]17. Warner, T.A.; Steinmaus, K. Spatial classiﬁcation of orchards and vineyards with high spatial resolutionpanchromatic imagery. Photogramm. Eng. Remote Sens. 2005,71, 179–187. CrossRef ]18. Li, L.; Liang, J.; Weng, M.; Zhu, H. multiple-feature reuse network to extract buildings from remote sensingimagery. Remote Sens. 2018,10, 1350. CrossRef ]19. Reis, S.; Ta¸ sdemir, K. Identiﬁcation of hazelnut ﬁelds using spectral and Gabor textural features. ISPRS J.Photogramm. Remote Sens. 2011,66, 652–661. CrossRef ]20. Moya, L.; Zakeri, H.; Yamazaki, F.; Liu, W.; Mas, E.; Koshimura, S. 3D gray level co-occurrence matrixand its application to identifying collapsed buildings. ISPRS J. Photogramm. Remote Sens. 2019,149, 14–28.[CrossRef ]21. Chen, J.; Deng, M.; Xiao, P.F.; Yang, M.H.; Mei, X.M. Rough set theory based object-oriented classiﬁcation ofhigh resolution remotely sensed imagery. J. Remote Sens. 2010,14, 1139–1155. CrossRef ]22. Zhao, Y.D.; Zhang, L.P.; Li, P.X. Universal Markov random ﬁelds and its application in multispectral texturedimage classiﬁcation. J. Remote Sens. 2006,10, 123–129. CrossRef ]23. Yang, P.; Yang, G. Feature extraction using dual-tree complex wavelet transform and gray level co-occurrencematrix. Neurocomputing 2016,197, 212–220. CrossRef ]24. Mao, L.; Zhang, G.M. Complex cue visual attention model for harbor detection in high-resolution remotesensing images. J. Remote Sens. 2017,21, 300–309. CrossRef ]25. Liu, P.H.; Liu, X.P.; Liu, M.X.; Shi, Q.; Yang, J.X.; Xu, X.C.; Zhang, Y.Y. Building footprint extraction fromhigh-resolution images via spatial residual inception convolutional neural network. Remote Sens. 2019,11,830. CrossRef ]26. Kim, S.; Son, W.J.; Kim, S.H. Double weight-based SAR and infrared sensor fusion for automatic groundtarget recognition with deep learning. Remote Sens. 2018,10, 72. CrossRef ]27. Gao, J.; Wang, K.; Tian, X.Y.; Chen, J. BP-NN Based Cloud Detection Method For FY-4 Remote Sensingimages. J. Infrared Millim. Waves 2018,37, 477–485. CrossRef ]28. Li, X.; Lyu, X.; Tong, Y.; Li, S.; Liu, D. An object-based river extraction method via Optimized TransductiveSupport Vector Machine for multi-spectral remote-sensing images. IEEE Access 2019,7, 46165–46175.[CrossRef ]29. He, T.; Sun, Y.J.; Xu, J.D.; Wang, X.J.; Hu, C.R. Enhanced land use /cover classiﬁcation using support vectormachines and fuzzy k-means clustering algorithms. J. Appl. Remote Sens. 2014,8, 083636. CrossRef ]30. Zhang, K.W.; Hu, B.X. Individual urban tree species classiﬁcation using very high spatial resolution airbornemulti-spectral imagery using longitudinal proﬁles. Remote Sens. 2012,4, 1741–1757. CrossRef ]31. Sang, X.; Guo, Q.Z.; Wu, X.X.; Fu, Y.; Xie, T.Y.; He, C.W.; Zang, J.L. Intensity and stationarity analysis of landuse change based on CART algorithm. Nat. Sci. Rep. 2019,9, 12279. CrossRef ][PubMed ]32. Santos Pereira, L.F.; Barbon, S.; Valous, N.A.; Barbin, D.F. Predicting the ripening of papaya fruit with digitalimaging and random forests. Comput. Electron. Agric. 2018,145, 76–82. CrossRef ]Remote Sens. 2020,12, 538 17 of 1833. Wang, N.; Li, Q.Z.; Du, X.; Zhang, Y.; Zhao, L.C.; Wang, H.Y. Identiﬁcation of main crops based on theunivariate feature selection in Subei. J. Remote Sens. 2017,21, 519–530. CrossRef ]34. Krizhevsky, A.; Sutskever, I.; Hinton, G. ImageNet classiﬁcation with deep convolutional neural networks.Commun. ACM 2017,60, 84–90. CrossRef ]35. Szegedy, C.; Liu, W.; Jia, Y.Q.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A.Going deeper with convolutions. In Proceedings of the 2015 IEEE Conference on Computer Vision andPattern Recognition, Boston, MA, USA, 7–12 June 2015. CrossRef ]36. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv2014, arXiv:1409.1556.37. He, K.M.; Zhang, X.Y.; Ren, S.Q.; Sun, J. Deep residual learning for image recognition. In Proceedings of the2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016.[CrossRef ]38. Long, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks for semantic segmentation. In Proceedingsof the 2015 IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA, 7–12 June 2015.[CrossRef ]39. Badrinarayanan, V.; Kendall, A.; Cipolla, R. SegNet: deep convolutional encoder-decoder architecture forimage segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 2017,39, 2481–2495. CrossRef ]40. Chen, L.C.; Papandreou, G.; Kokkinos, I.; Murphy, K.; Yuille, A.L. DeepLab: Semantic image segmentationwith deep convolutional nets, Atrous convolution, and fully connected CRFs. IEEE Trans. Pattern Anal. Mach.Intell. 2018,40, 834–848. CrossRef ]41. Lin, G.S.; Milan, A.; Shen, C.H.; Reid, I. ReﬁneNet: Multi-path reﬁnement networks for high-resolutionsemantic segmentation. In Proceedings of the 2017 IEEE Conference on Computer Vision and PatternRecognition, Honolulu, HI, USA, 21–26 July 2017. CrossRef ]42. Ronneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional networks for biomedical image segmentation.InMedical Image Computing and Computer-Assisted Intervention—MICCAI 2015 Lecture Notes in ComputerScience; Navab, N., Hornegger, J., Wells, W., Frangi, A., Eds.; Springer: Berlin, Germany, 2015; Volume 9351.[CrossRef ]43. Cui, W.; Wang, F.; He, X.; Zhang, D.Y.; Xu, X.X.; Yao, M.; Wang, Z.W. Multi-scale semantic segmentation andspatial relationship recognition of remote sensing images based on an attention model. Remote Sens. 2019,11,1044. CrossRef ]44. Fu, G.; Liu, C.J.; Zhou, R.; Sun, T.; Zhang, Q.J. Classiﬁcation for high resolution remote sensing imageryusing fully convolutional network. Remote Sens. 2017,9, 498. CrossRef ]45. Lu, J.Y.; Wang, Y.Z.; Zhu, Y.Q.; Ji, X.H.; Xing, Y.T.; Li, W.; Zomaya, A.Y. P_segnet and NP_segnet: New neuralnetwork architectures for cloud recognition of remote sensing images. IEEE Access 2019,7, 87323–87333.[CrossRef ]46. Shustanov, A.; Yakimov, P. CNN design for real-time tra sign recognition. Procedia Eng. 2017,201, 718–725.[CrossRef ]47. Dai, X.B.; Duan, Y.X.; Hu, J.P.; Liu, S.C.; Hu, C.Q.; He, Y.Z.; Chen, D.P.; Luo, C.L.; Meng, J.Q. Near infrarednighttime road pedestrians recognition based on convolutional neural network. Infrared Phys. Technol. 2019,97, 25–32. CrossRef ]48. Wang, D.D.; He, D.J. Recognition of apple targets before fruits thinning by robot based on R-FCN deepconvolution neural network. Trans. Chin. Soc. Agric. Eng. 2019,35, 156–163. CrossRef ]49. Ferentinos, K.P. Deep learning models for plant disease detection and diagnosis. Comput. Electron. Agric.2018,145, 311–318. CrossRef ]50. Cheng, X.; Zhang, Y.H.; Chen, Y.Q.; Wu, Y.Z.; Yue, Y. Pest identiﬁcation via deep residual learning in complexbackground. Comput. Electron. Agric. 2017,141, 351–356. CrossRef ]51. Liu, F.; Shen, T.; Ma, X.; Zhang, J. Ship recognition based on multi-band deep neural network. Opt. Precis.Eng. 2017,25, 166–173. CrossRef ]52. Chen, Y.; Zhang, C.M.; Wang, S.Y.; Li, J.P.; Li, F.; Yang, X.X.; Wang, Y.Y.; Yin, L.K. Extracting crop spatialdistribution from Gaofen imagery using convolutional neural network. Appl. Sci. 2019,9, 2917. CrossRef ]53. Xie, B.; Zhang, H.K.; Xue, J. Deep convolutional neural network for mapping smallholder agriculture usinghigh spatial resolution satellite image. Sensors 2019,19, 2398. CrossRef ]Remote Sens. 2020,12, 538 18 of 1854. Yang, W.; Yang, C.; Hao, Z.Y.; Xie, C.Q.; Li, M.Z. Diagnosis of plant cold damage based on hyperspectralimaging and convolutional neural network. IEEE Access 2019,7, 118239–118248. CrossRef ]55. Li, Y.; Zhang, H.; Shen, Q. Spectral–spatial classiﬁcation of hyperspectral imagery with 3D convolutionalneural network. Remote Sens. 2017,9, 67. CrossRef ]56. Sellami, A.; Farah, M.; Farah, I.R.; Solaiman, B. Hyperspectral imagery classiﬁcation based on semi-supervised3-D deep neural network and adaptive band selection. Expert Syst. Appl. 2019,129, 246–259. CrossRef ]57. Alonzo, M.; Andersen, H.E.; Morton, D.C.; Cook, B.D. Quantifying boreal forest structure and compositionusing UAV structure from motion. Forests 2018,9, 119. CrossRef ]58. Zhang, C.; Pan, X.; Li, H.; Gardiner, A.; Sargent, I.; Hare, J.; Atkinson, P.M. hybrid MLP-CNN classiﬁerfor very ﬁne resolution remotely sensed image classiﬁcation. ISPRS J. Photogramm. Remote Sens. 2018,140,133–144. CrossRef ]59. Jozdani, S.E.; Johnson, B.A.; Chen, D. Comparing deep neural networks, ensemble classiﬁers, and supportvector machine algorithms for object-based urban land use /land cover classiﬁcation. Remote Sens. 2019,11,1713. CrossRef ]60. Carranza-Garc ía, M.; Garc ía-Guti érrez, J.; Riquelme, J.C. framework for evaluating land use and landcover classiﬁcation using convolutional neural networks. Remote Sens. 2019,11, 274. CrossRef ]61. Zhang, C.M.; Han, Y.J.; Li, F.; Gao, S.; Song, D.J.; Zhao, H.; Fan, K.Q.; Zhang, Y.N. new CNN-Bayesianmodel for extracting improved winter wheat spatial distribution from GF-2 imagery. Remote Sens. 2019,11,619. CrossRef ]62. Zheng, S.; Jayasumana, S.; Romera-Paredes, B.; Vineet, V.; Su, Z.; Du, D.; Huang, C.; Torr, P.H. Conditionalrandom ﬁelds as recurrent neural networks. In Proceedings of the 2015 IEEE International Conference onComputer Vision (ICCV), Santiago, Chile, 7–13 December 2015; pp. 1529–1537.63. Teichmann, M.T.T.; Cipolla, R. Convolutional CRFs for Semantic Segmentation. arXiv 2018, arXiv:1805.04777.64. Audebert, N.; Boulch, A.; Saux, B.E.; Lef èvre, S. Distance transform regression for spatially-aware deepsemantic segmentation. Comput. Vis. Image Underst. 2019,189, 102809. CrossRef ]65. Fu, T.; Ma, L.; Li, M.; Johnson, B.A. Using convolutional neural network to identify irregular segmentationobjects from very high-resolution remote sensing imagery. J. Appl. Remote Sens. 2018,12, 025010. CrossRef ]66. Mboga, N.; Georganos, S.; Grippa, T.; Lennert, M.; Vanhuysse, S.; Wol ↵, E. Fully Convolutional Networksand Geographic Object-Based Image Analysis for the Classiﬁcation of VHR Imagery. Remote Sens. 2019,11,597. CrossRef ]67. Zhao, W.; Du, S.; Emery, W.J. Object-based convolutional neural network for high-resolution imageryclassiﬁcation. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2017,10, 3386–3396. CrossRef ]68. Papadomanolaki, M.; Vakalopoulou, M.; Karantzalos, K. Novel Object-Based Deep Learning Frameworkfor Semantic Segmentation of Very High-Resolution Remote Sensing Data: Comparison with Convolutionaland Fully Convolutional Networks. Remote Sens. 2019,11, 684. CrossRef ]69. Mi, L.; Chen, Z. Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation.ISPRS J. Photogramm. Remote Sens. 2020,159, 140–152. CrossRef ]70. Hornik, K. Approximation capabilities of multilayer feedforward networks. Neural Netw. 1991,4, 251–257.[CrossRef ]©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open accessarticle distributed under the terms and conditions of the Creative Commons Attribution(CC BY) license http: //creativecommons.org /licenses /by/4.0/).