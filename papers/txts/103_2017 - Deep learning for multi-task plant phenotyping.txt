Deep Learning for Multi-task Plant PhenotypingMichael P. Pound1Jonathan A. Atkinson2Darren M. Wells2Tony P. Pridmore1Andrew P. French1,21School of Computer Science, University of Nottingham, NG8 1BB, UK2School of Biosciences, University of Nottingham, LE12 5RD, UK{michael.pound, jonathan.atkinson, darren.wells,tony.pridmore, andrew.p.french }@nottingham.ac.ukFigure 1: selection of results from our deep network locating spikes (middle) and spikelets (bottom) on the ACID datasetAbstractPlant phenotyping has continued to pose challenge tocomputer vision for many years. There is particular de-mand to accurately quantify images of crops, and the natu-ral variability and structure of these plants presents uniquedifﬁculties. Recently, machine learning approaches haveshown impressive results in many areas of computer vision,but these rely on large datasets that are at present not avail-able for crops. We present new dataset, called ACID, thatprovides hundreds of accurately annotated images of wheatspikes and spikelets, along with image level class annota-tion. We then present deep learning approach capableof accurately localising wheat spikes and spikelets, despitethe varied nature of this dataset. As well as locating fea-tures, our network offers near perfect counting accuracyfor spikes (95.91%) and spikelets (99.66%). We also ex-tend the network to perform simultaneous classiﬁcation ofimages, demonstrating the power of multi-task deep archi-tectures for plant phenotyping. We hope that our datasetwill be useful to researchers in continued improvement ofplant and crop phenotyping. With this in mind, alongsidethe dataset we will make all code and trained models avail-able online.1. IntroductionCrop phenotyping performs crucial role in the de-velopment of higher-yielding plants, which in itself offersone solution to the continuing challenge of global foodsecurity. For cereal plants, yield is measured in termsof grain, found within the spikes at the tip of the plant.Therefore, counting both the number of spikes, and theso-called spikelets within them (see Fig. 8) is an importantmeasure. In this work, we aim to further the state-of-the-artin wheat phenotyping. We present dataset, publiclyavailable, that can be used by researchers to improve theirwheat phenotyping ability via machine learning. Using this12055dataset, we apply deep network architecture to performsimultaneous localization of the spikes and spikelets, andimage classiﬁcation. The result is system capable ofcounting and locating both spikes and spikelets, in thepresence of varied phenotypes, occlusion, clutter, andarbitrary rotations. Example outputs of our approach canbe seen in Fig. 1. The dataset and all related code can befound at http://plantimages.nottingham.ac.uk/ .Motivation. Biologically, the ability to phenotype spiketraits is of great importance, with uses in several differentresearch areas. For example, spikelet development and thusyield can be affected by abiotic stresses such as high tem-perature and drought [12, 26] or changes in sowing date[1]. Other traits such as the presence of awns, bristle orhair like structure extending from the end of each ﬂoret (seeFig. 2), have been linked with increased photosynthesis un-der drought and greater harvestable yield [4, 19]. Intricacieswithin the spike and features of the spikelets themselveshave been shown to be useful when predicting yield [14].Locating and counting spikes, and particularly thespikelets within, presents demanding computer visionchallenge, as can be seen in Figs. 1, and 9. Occlusion,self similarity, variation in appearance, viewpoint sensitiv-ity and lighting challenges weigh heavy on the ability of tra-ditional image analysis to satisfactorily solve this task. Butthese challenges are not unique to wheat spikes; rather theyare representative of general plant and crop phenotypingchallenges that present themselves when we start to movefrom the lab to the ﬁeld. Therefore, by addressing this spe-ciﬁc, challenging task, we are harnessing deep learning tosolve realistic phenotyping problem which if successfulbodes well for other phenotyping automation felt beyondthe reach of traditional analysis. This then is our acid testof deep learning in the wild.1.1. ContributionsWe present new dataset (which we call ACID: the An-notated Crop Image Dataset) of accurately labeled wheatimages, and adapt an appropriate CNN architecture to ad-dress the challenge of wheat spike phenotyping. Our net-work performs multi-task learning, simultaneously locatingspike features, whilst also classifying the awned phenotypeof the wheat in each case. An overview of our network ar-chitecture is shown in Fig. 3. In summary, our contributionsare:•A new publically-available dataset containing 520 im-ages of wheat plants exhibiting wide range of canopyand spike phenotypes. Each plant has been carefullyannotated by an expert, including the locations of eachspike, and individual spikelets. Each image has alsobeen classiﬁed as exhibiting, or not, an awned pheno-type.•A novel application of deep CNN architecture formulti-task learning in wheat phenotyping. We have ex-tended state-of-the-art CNN architecture to performsimultaneous localisation of features and classiﬁcationof images. This network is trained end-to-end fromscratch on the ACID dataset, and we present detaileddiscussions of our training process, along with resultson its performance.•An adapted data augmentation approach to handleviews of multiple similar objects. Unlike many ex-isting image sets, ACID contains many instances ofthe same class of object, with extremely similar ap-pearance. This presents unique challenge to the tra-ditional network training and data augmentation ap-proaches seen previously. We present spike-centricdata augmentation approach that varies the data be-tween epochs as much as possible, leading to improvedresults.2. Related WorkAdvances in image-based plant phenotyping helpeddrive plant research for many years. Traditionally, low-level image processing approaches were the norm, usingpixel-based techniques and hand crafted models to aid lo-calization and measurement of plants (for example [8, 2]).Recently machine-learning based approaches have seen in-creased adoption, allowing systems to learn how to ﬁnd andsegment plants, based often on hand-crafted feature set(for thorough overview of machine learning approachesin plant phenotyping see [22]). Most recently, deep learningpromises step-change in the performance of many image-based systems (e.g. [18], [23]), but adoption by the plantphenotyping community is still in its infancy.To date, most spikelet and ear counting is done by hand,following method similar to [14], which relates crop yieldto features in the spike and spikelets without using imageanalysis. Some methods do exist for automatically detect-ing heading and ﬂowering in wheat [21] uses bag-of-visual-words approach to identify growth stages in ﬁeld-grown wheat. Low level features are extracted using theSIFT algorithm. Finally support vector machine classiﬁ-cation is used to classify growth stage. Accuracies forstage classiﬁcation range from 85% (ﬂowering) to 99% (lategrowth stage). Colour and texture have been used in pre-liminary study to count wheat ears [7], reporting accuracyup to 85%, although, as the authors concede, this is acrossa small and limited dataset, and relies on bespoke imageprocessing pipeline.Machine learning approaches have been applied recentlyto number of other plant phenotyping challenges. Somecereal-speciﬁc examples include using expectation maxi-mization to identify wheat streak mosaic virus[6]; Simplex2056Volume Maximisation to discover characteristic spectra inhyperspectral data of barley diseases [25]; and supportvector machine method to detect ﬂowering rice in RGB im-ages [11]. support vector machine approach has also beenused [27] to learn from SIFT features and codebook gen-erated over 7,500 images to classify higher plant taxa fromimages of leaves, achieving an accuracy of 72%.Recently, number of machine learning-focused ap-proaches have taken part in the CVPPP challenge. Onesuch approach used regression (speciﬁcally support vec-tor regression) to count leaves in overhead views of rosetteplants [10]. Leaf counting is again addressed in [17], whichalso uses machine learning to assist with segmentation viaa Random Forest classiﬁer. Other approaches have viewedthe CVPPP leaf segmentation data set as an instance seg-mentation problem, segmenting each individual leaf in turnin the image. The most prominent paper in this area is oneof the ﬁrst applications of recurrent neural network in thisdomain [20]. spatial memory-equipped network allowsthe system to segment leaves one at time and handle oc-clusion.This brings us to the application of deep learning to plantphenotyping. Demonstrating the power of CNNs for classi-ﬁcation, [3] develop system, LeafNet, for taxon identiﬁ-cation from images of leaves. This system outperforms pre-vious approaches on standard image classiﬁcation datasets.[18] use CNN to classify subset of plant features in smallimage sections. Localisation is performed by scanning eachimage and classifying overlapping sub windows. Conse-quently this approach is relatively slow, and lacks contextdue to the small cropped window sizes. Another approachhas used CNN-LSTM framework to classify plants intogenotype[23]. The use of the LSTM is interesting, as it isused to improve classiﬁcation over time; the author’s hy-pothesise is that growth rate is an important factor in de-termining genotype. The LSTM component does improveclassiﬁcation for the top-down rosette images used.Of course, machine learning, and especially deep ma-chine learning approaches are fuelled only by high quality,annotated datasets [24, 15]. For learning to be effective andefﬁcient, the image data the computer is learning from mustbe both accurately captured and well annotated. This isour motivation for releasing our expertly-annotated datasetalongside the speciﬁc algorithms we have developed.3. MethodIn this section, we describe the new ACID dataset. Wethen discuss our network architecture that performs simul-taneous feature localisation and classiﬁcation, and our dataaugmentation and training approach.Figure 2: Representative images from the ACID dataset.Note presence of awns (bristles) in the right-hand images.Expert annotations are shown.3.1. The ACID DatasetA key contribution of this work is new dataset, con-taining images of wheat plants taken in glasshouse con-ditions. doubled haploid population of spring wheatplants was obtained from the Nottingham/BBSRC WheatResearch Centre and grown in 2l pots in glasshouse. Thispopulation was selected for its wide range in canopy andspike phenotypes. Imaging was conducted using con-sumer grade 12MP camera ﬁxed to custom-built imagingsystem providing consistent black background. Each im-age contains multiple spikes at high resolution, with full an-notation of the position of each spike, and further positionsof each spikelet. Fig. shows representative images fromthis dataset.These images have been annotated by single expert, attheir native resolution of 1956x1530. The dataset is avail-able at full resolution, with no augmentation or cropping.Each image is supplied with JSON ﬁle containing co-ordinates of the base and tip of each ear (occasionally ad-ditional points should the ear be curved), and co-ordinatesof all visible spikelets. Occluded spikelets were not anno-tated; however, partially occluded ears were left as contin-uous polylines rather than split up. This is multi-instancedataset, in which each image contains multiple objects ofthe same class. In total there are 520 images, containing atotal of 4,100 ears and 48,000 spikelets. Each image hasalso been tagged with the presence of an awned phenotype,(Fig. 2, right). Awned plants comprise about 1/3 of thedataset.20573xL LLClassificationHeatmap PredictionInputFigure 3: An overview of our CNN architecture containing stacked hourglass networks.3.2. Network ArchitectureEach image presented to the network could conceiv-ably contain hundreds of similar objects to be localised andcounted. Instead of predicting location directly, we performpixel-wise regression, identifying areas of high-likelihoodof each target. Detected features are then determined asthe maximum points of this likelihood. This heatmap re-gression has been used successfully in, among other areas,human pose estimation [5]. Our network is based upon astacked hourglass network [16], which itself is an evolutionof fully-connected networks [9] and residual networks [13].An outline of the network we use is shown in Fig. 3.The architecture of the network is based upon an encod-ing/decoding structure, in which series of convolutionaloperations and spatial downsampling (red) begin by com-puting ﬁxed-size feature representation of the image. Thisfeature space is then upsampled (blue) back to the origi-nal resolution, while lower-level features are re-combinedin stages. Combining hierarchical features from multiplescales preserves spatial resolution in the network output.Each white block in Fig. represents one or more resid-ual blocks, combining convolution and batch normalisationoperations, and includes an additional skip layer that helpsavoid vanishing gradients, aiding training in very deep net-works such as this one. All residual blocks in the net-work output 256 features. The input to our network isan RGB image of size 256x256. set of initial residualblocks (not shown in Fig. 3) reduces the spatial resolutionto 64x64. The hourglass network operates at this lowerresolution throughout. The output heatmaps are thereforealso 64x64 pixels, with one for each feature being detected.We output two heatmaps in our network, trained on eartips and spikelets separately. The network contains fourstacked hourglasses, and includes intermediate supervision;the heatmap output at the end of each hourglass is used tocalculate loss, which guides training of the network.The ACID dataset contains image-level labels specify-ing whether the imaged plant has an awned phenotype. Wehave extended our network to perform simultaneous classi-ﬁcation by branching off the deep feature layer within theﬁnal hourglass. At this point, prior to upsampling, the dataFigure 4: Data augmentation chooses an ear at random andcreates randomly transformed crop at that location. Anyvisible ground truth points are transformed in the same way.represents spatially-invariant feature representation of theimage. The branch contains two residual blocks, before aﬁnal convolutional block performing classiﬁcation.3.3. Data AugmentationOur task is to locate and count wheat spikes and spikeletsin the ACID dataset. Each image may contain number ofspikes, each of which will contain numerous spikelets. Bothspikes and spikelets may appear very similar within sin-gle image, but exhibit large amount of variability betweenimages, and lines. Due to hardware limitations, many deepnetwork architectures have strict limits on image input size.This network has similar restrictions, where the input sizecannot be increased far beyond 256px before GPU memorybecomes the limiting factor. Many whole-image classiﬁca-tion approaches will scale the image to the correct size dur-ing training or inference. In our case, each object of interestis small with respect to the original image, making globalimage scaling unwise. We wish to be able to accuratelyclassify each large image completely, but preserve higher-resolution detail: each image must be split into regions.Rather than splitting each image up prior to training,creating ﬁxed-size training set, we randomly crop im-ages during training, with crops centred on spike positions.When an image is loaded, random spike is chosen, anda training image produced at that location. The input tothe network is 256x256; however, we have experimentedwith varying initial crop sizes, 256, 384, or 512 pixels. Alarger initial crop that is eventually scaled to the correct in-put size will represent wider ﬁeld of view, with the fea-tures in the original image scaled down. This represents acompromise between the network seeing wider image con-2058text, and higher-resolution features. This does affect results,something we explore below. However, we use default ini-tial crop of 384 pixels, which represents 2/3 scale of theoriginal image resolution.Additional random cropping, scaling, rotation and hori-zontal ﬂipping is added to increase variability in the train-ing set (Fig. 4). This means that when any given imageis loaded, only small part of that image is used duringthat training iteration. This means that higher number oftraining epochs are required to capture the variability of thedataset, but that the eventual network should show increasedperformance.Heatmap output is produced by performing identicaltransformations on the image labels, and then renderingeach visible point as two-dimensional Gaussian. The out-put heat maps are 64x64 pixels and we chose standard de-viation of 1.0 for spike tips, and 0.7 for the slightly smallerspikelets. In practice, we found any reasonable standard de-viation was effective.3.4. TrainingThe ACID dataset of 520 images was split into 415 train-ing images, and 105 testing images. This split was per-formed at the image level, not the spike level, to ensure thatno spikes from the same image could be seen in both train-ing and testing sets. During training, random augmentationwas applied as per Fig. with random rotation and scalingdrawn from normal distributions with standard deviations0.25 radians and 25% respectively. Half of all input imageswere also horizontally ﬂipped at random. No augmentationwas performed on the testing image set. The network wastrained end-to-end, from scratch, using RMSProp. We useda mean squared error loss function with an initial learningrate of 2.5x10-4, and reduced by factor of 10 every 200epochs. Training was run for 500 epochs, although perfor-mance usually plateaued around 300 epochs (Fig. 5).Results below are presented on the ﬁnal trained modelafter 500 epochs, not necessarily the best performing modelduring the entire training run. The time taken to completeeach training run was approximately hours. It is worthreiterating that due to our image augmentation mechanism,one epoch only represents small view of the entire data,even though each image has been used once. This goessome way to explaining the large number of epochs requiredin this case.The classiﬁcation branch is trained in parallel usingbinary cross entropy. The classiﬁcation branch loss isweighted at 5x10-2compared to the heatmap MSE loss, toaccount for this BCE producing larger values in general,and so to avoid driving the training of the network entirelybased on classiﬁcation error.0 100 200 300 400 500Epoch00.20.40.60.81Ear F1 ScoreTrainingTestingFigure 5: Ear accuracy over training and testing imagesthroughout the training process.3.5. Feature LocalisationSpike and spikelet positions are calculated from eachoutput heatmap using non-maximal suppression (NMS).For all output pixels, any pixel with higher intensity thanits four neighbours is classiﬁed as feature. We have foundthat after sufﬁcient training the network reliably positionslocal maxima at feature locations, with output distributionsapproximating the Gaussians used for training. The numberof additional false positives generated by the NMS compo-nent of our approach is negligible.4. Results4.1. EvaluationWe evaluate our approach by calculating both the preci-sion and recall for each network on the training set, alongwith the count accuracy for both spikes and spikelets. Pre-cision represents the fraction of detections that are true pos-itives. Recall represents the fraction of spikes and spikeletsthat have been correctly detected. It is common to combinethese accuracy measures using the F1 score, as generalmeasure of performance.What remains is to distinguish true positives from falsepositives, and determine which features have been correctlydetected. We apply distance threshold for successful de-tection, then vary this threshold to explore the efﬁcacy of theapproach at different tolerances. This normalised distancethreshold is calculated relative to the size of the primary earvisible in each image (which is present in the annotations),see Fig. 8. true positive for either spike tip or spikelet isany predicted location that lies within this normalised dis-tance of ground truth point. Similarly, false negativeis any ground truth point that is not within the normaliseddistance of predicted feature.20590 0.05 0.1 0.15 0.2Normalised Distance00.20.40.60.81Precision RecallSpike Tip DetectionPrecisionRecall0 0.05 0.1 0.15 0.2Normalised Distance00.20.40.60.81Precision RecallSpikelet DetectionPrecisionRecallFigure 7: Precision and recall for the network as the nor-malised distance threshold is adapted.4.2. Numerical ResultsGiven the challenge of the problem, results achieved arevery encouraging. Spikes are located with an F1 of 0.83 @0.1 and 0.89 0.2, spikelets are located with an F1 of 0.88@ 0.05 and 0.96 0.1. For both features, we are conﬁdentthat these normalised distances represent fairly strict toler-ance for error. For comparison, an example wheat spike tipand spikelet marked with these normalised distances can beseen in Fig. 8. Variation in precision/recall over normaliseddistance can be seen in Fig. 7.Qualitative Analysis. Figures and show representa-tive output from the network. The spike tip detection can beseen to work effectively in the presence of occlusion or largeamounts of clutter. Spikelet detection is generally more ac-curate still, and is capable of distinguishing either dual rowsof spikelets, or single rows when the spike is viewed rotated90 degrees (see Fig. 9, lower right for examples of both).Detecting spike tips appears to be the harder problem. Thefailure modes on spike tips offer some insights into the net-work itself. While the network is quite capable of detect-0.100.200.100.05Figure 8: visualisation of the normalised distances weused to measure accuracy. Here you can also clearly seeindividual spikelets within the spikeing tips even when occluded (Fig. 9), some occlusions willcause the detection to fail. We have also observed imagesin which the tip has still been detected, but has been incor-rectly positioned at the edge of occlusion, rather than thetrue occluded location. This error will cause the recall andF1 scores to reduce, but the counting accuracy to remain thesame.Where the tips of two spikes are very close together, thenetwork will usually output two gaussian features very closetogether, or single feature which is more spread out thanusual on the heat map. In either case, this will adverselyaffect the counting accuracy, but will not affect our F1 mea-sure, which is weakness of our NMS feature extractionapproach.Spikelet detection may fail at the boundary between twooverlapping spikes, due to the increased ambiguity betweenone and the other. This kind of overlap is not uncommon inthe ACID dataset, but this also means multiple instances ofthis issue are included in the training set, offering the net-work some ability to distinguish between touching spikes.Effect of Awned Phenotypes. We compared the test-ing F1 scores of all awned plants against all those that arenot awned. We saw marginal improvement of 1.3% F1 intesting accuracy for non-awned plants, suggesting awns areslightly more challenging to phenotype, however this wasnot substantial difference, and may not be signiﬁcant overmany more images.Effect of Augmentation. We trained the same networkwithout data augmentation to measure the impact of lessvaried training set. As expected, there was degradation inperformance for feature localisation on the testing set, par-ticularly in spike tip detection. F1 reduced 7.5% for spikes,and 0.1% for spikelets. With the addition of random rota-2060Figure 9: Some more challenging examples from the namedataset, including occlusion, background clutter, ambiguityand rotational asymmetry.tion and scaling to the testing set, this reduction increasesto 8.1% for ears and 2.7% for spikelets. This suggests thatin non lab environment where image capture is less con-strained, data augmentation will become even more impor-tant.4.3. Input Image ResolutionBased on our own observation of the results, we believethat detection of wheat tips requires great deal more con-text about the local image than individual spikelets. Intu-itively, in cluttered image containing multiple spikes, ifa spike is partially cropped out of the input image, it willmake accurate localisation of the tip harder. To conﬁrm thishypothesis, we varied the input resolution to the network.As above, we initially crop section of image for trainingor inference, perform augmentation, then if necessary theimage is scaled to the size of the network input, 256x256.Altering the initial crop size is equivalent to scaling the en-tire image before it is used, and in essence changes the ﬁeldof view available to the network.We trained two additional networks, with input crop0 0.05 0.1 0.15 0.2Normalised Distance00.20.40.60.81F1 ScoreSpike Tip Detection256px384px512px0 0.05 0.1 0.15 0.2Normalised Distance00.20.40.60.81F1 ScoreSpikelet Detection256px384px512pxFigure 10: The effect of varying input resolution during dataaugmentation.sizes of 256 and 512 pixels. The 256 crop input is nativesource resolution, where no image scaling is performed be-yond that required by scale augmentation. Fig. 10 shows theresults of this experiment, in which smaller ﬁeld of viewperforms notably worse on spike tip detection. In particularit is the recall ability of the network that is impaired, its abil-ity to ﬁnd tips, rather than too many false positives. Spikeletdetection appears marginally better with small ﬁeld ofview. This is perhaps also expected, spikelets are smallfeatures, and 256pixel crop with no scaling preservesthese at higher-resolution. However, it is also possible thatthe smaller crop size beneﬁts from the normalised distancemeasure, with the size of an ear being larger with respect tothe output heatmap when smaller view is used. Neverthe-less, the beneﬁt is marginal, and has effectively disappearedwhen the normalised distance threshold reaches 0.1.Given these results, it seems reasonable to recommendan input window size of 384 or 512 pixels. We measured theaverage length of spikes in the ACID dataset, and found thisto be 235 pixels, with large standard deviation of 68 pixels.2061Table 1: Percentage error for spike tip and spikelet countingat different input crop resolutions.Resolution (px) Tip Error (%) Spikelet Error (%)256x256 -14.46 0.0600384x384 -5.13 3.81512x512 -4.09 0.34Thus, either of these two resolutions offers complete viewof many spikes, improving tip localisation.4.4. Counting AccuracyTo measure counting accuracy, we compared only thenumber of predicted features against the number of groundtruth points and computed percentage error, to simulatea count-based phenotyping task. These results were com-puted over 30 iterations of the testing set, to ensure that arepresentative sample of image crops was obtained. The re-sults can be seen in Table 1.Given the lower recall performance on tips for the 256pixel input size, the increased error is to be expected. It isinteresting, however, that the 512 pixel input size performswell on both spike and spikelet counting, with very encour-aging accuracy. This improvement over the smaller inputsizes could be due to fewer spikes being truncated at theedges of the image, or larger ﬁeld of view adding contextthat helps resolve ambiguity.The negative values for spike tip error indicate that thenetwork tends to underestimate, rather than overestimatethe number of tips. This follows from the results presentedin Fig. 7, in which the precision of the network was higherthan recall. Similarly, spikelet detection can slightly over-estimate, where the recall of the network is higher than theprecision4.5. ClassiﬁcationWe have extended the network to perform simultaneousclassiﬁcation of awned plants. We frame the classiﬁcationas the task of outputting value close to one if the plantis awned, and close to zero if not. During inference, wethreshold at 0.5 to convert the network branch output into aﬁrm prediction. The classiﬁcation accuracy for awns at theinput resolutions we have examined are shown in Table 2.As above, results were computed over 30 iterations of thetesting data.Accuracy on heatmap generation of this network was un-affected by the addition to the architecture. As we might ex-pect intuitively, classiﬁcation accuracy increases as the win-dow seen by the network increases. Nevertheless, all threeresolutions offer extremely accurate classiﬁcation of awnedplants. Recognising awns is one of the easier classiﬁcationchallenges posed on wheat plants, compared with, for in-Table 2: Awned classiﬁcation accuracy at different inputresolutions.Resolution (px) Accuracy (%)256x256 98.39384x384 98.49512x512 99.00stance, growth stage classiﬁcation. However, this acts asa demonstration that the hourglass design, if extended, canperform additional tasks beyond feature localisation. Ourfuture work in this area will explore growth stage classiﬁ-cation, ﬂowering, and senescence.5. ConclusionWe have presented new dataset, ACID, containing de-tailed annotations of wheat spikes and spikelets, as wellas image level awn classiﬁcation, on varied phenotypicset of wheat lines. We have extended deep convolu-tional neural network architecture to perform regressionof feature locations, as well as image-level classiﬁcation.We report very encouraging results on all aspects of thedataset. Our future work will focus on additional image-level classiﬁcation, such as ﬂowering plants, and retrain-ing this network for ﬁeld images, requiring additions tothe dataset and application of transfer learning. Individ-ual classiﬁcation of spikelets (following promising workin [14]) will also be interesting to pursue, with viewto predicting yield. All data and code can be found athttp://plantimages.nottingham.ac.uk/ .References[1] I. Arduini, L. Ercoli, M. Mariotti, and A. Masoni. Sow-ing date affect spikelet number and grain yield of durumwheat. Cereal Research Communications 37(3):469–478,Sept. 2009.[2] P. Armengaud, K. Zambaux, A. Hills, R. Sulpice, R. J. Pat-tison, M. R. Blatt, and A. Amtmann. Ez-rhizo: integratedsoftware for the fast and accurate measurement of root sys-tem architecture. The Plant Journal 57(5):945–956, 2009.[3] P. Barr, B. C. Stver, K. F. Mller, and V. Steinhage. Leafnet:A computer vision system for automatic plant species iden-tiﬁcation. Ecological Informatics 40:50 56, 2017.[4] A. Blum. Photosynthesis and Transpiration in Leaves andEars of Wheat and Barley Varieties. Journal of ExperimentalBotany 36(3):432–440, Mar. 1985.[5] A. Bulat and G. Tzimiropoulos. Human Pose Estimationvia Convolutional Part Heatmap Regression pages 717–732.Springer International Publishing, Cham, 2016.[6] J. J. Casanova, S. A. O039;Shaughnessy, S. R. Evett, andC. M. Rush. Development of wireless computer vi-sion instrument to detect biotic stress in wheat. Sensors ,14(9):17753–17769, 2014.2062[7] F. Cointault, D. Guerin, J. Guillemin, and B. Chopinet. In-ﬁeld triticum aestivum ear counting using colourtexture im-age analysis. New Zealand Journal of Crop and Horticul-tural Science 36(2):117–130, 2008.[8] A. French, S. Ubeda-Tom ´as, T. J. Holman, M. J. Bennett,and T. Pridmore. High-throughput quantiﬁcation of rootgrowth using novel image-analysis tool. Plant Physiology ,150(4):1784–1795, 2009.[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-ture hierarchies for accurate object detection and semanticsegmentation. In Proceedings of the IEEE conference oncomputer vision and pattern recognition pages 580–587,2014.[10] M. V. Giuffrida, M. Minervini, and S. A. Tsaftaris. Learningto count leaves in rosette plants. Proceedings of the Com-puter Vision Problems in Plant Phenotyping (CVPPP) 2016.[11] W. Guo, T. Fukatsu, and S. Ninomiya. Automated charac-terization of ﬂowering dynamics in rice using ﬁeld-acquiredtime-series rgb images. Plant Methods 11(1):7, 2015.[12] N. J. Halse and R. N. Weir. Effects of temperature on spikeletnumber of wheat. Australian Journal of Agricultural Re-search 25(5):687–695, 1974.[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-ing for image recognition. In Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition pages770–778, 2016.[14] Y. Li, Z. Cui, Y. Ni, M. Zheng, D. Yang, M. Jin, J. Chen,Z. Wang, and Y. Yin. Plant density effect on grain numberand weight of two winter wheat cultivars at different spikeletand grain positions. PLOS ONE 11(5):1–15, 05 2016.[15] G. Lobet. Image analysis in plant sciences: Publish thenperish. Trends in Plant Science 2017.[16] A. Newell, K. Yang, and J. Deng. Stacked hourglass net-works for human pose estimation. In European Conferenceon Computer Vision pages 483–499. Springer, 2016.[17] J.-M. Pape and C. Klukas. Utilizing machine learning ap-proaches to improve the prediction of leaf counts and indi-vidual leaf segmentation of rosette plant images. In H. S.S. A. Tsaftaris and T. Pridmore, editors, Proceedings of theComputer Vision Problems in Plant Phenotyping (CVPPP) ,pages 3.1–3.12. BMV Press, September 2015.[18] M. P. Pound, A. J. Burgess, M. H. Wilson, J. A. Atkinson,M. Grifﬁths, A. S. Jackson, A. Bulat, G. Tzimiropoulos,D. M. Wells, E. H. Murchie, et al. Deep machine learningprovides state-of-the-art performance in image-based plantphenotyping. bioRxiv page 053033, 2016.[19] G. J. Rebetzke, D. G. Bonnett, and M. P. Reynolds. Awnsreduce grain number to increase grain size and harvestableyield in irrigated and rainfed spring wheat. Journal of Ex-perimental Botany 67(9):2573–2586, Apr. 2016.[20] B. Romera-Paredes and P. H. S. Torr. Recurrent instancesegmentation. CoRR abs/1511.08250, 2015.[21] P. Sadeghi-Tehran, K. Sabermanesh, N. Virlet, and M. J.Hawkesford. Automated method to determine two criticalgrowth stages of wheat: Heading and ﬂowering. Frontiers inPlant Science 8:252, 2017.[22] A. Singh, B. Ganapathysubramanian, A. K. Singh, andS. Sarkar. Machine learning for high-throughput stress phe-notyping in plants. Trends in plant science 21(2):110–124,2016.[23] S. Taghavi Namin, M. Esmaeilzadeh, M. Najaﬁ, T. B.Brown, and J. O. Borevitz. Deep phenotyping: Deep learn-ing for temporal phenotype/genotype classiﬁcation. bioRxiv ,2017.[24] S. A. Tsaftaris, M. Minervini, and H. Scharr. Machine learn-ing for plant phenotyping needs image processing. Trends inplant science 21(12):989–991, 2016.[25] M. Wahabzada, A.-K. Mahlein, C. Bauckhage, U. Steiner,E.-C. Oerke, and K. Kersting. Metro maps of plant diseasedynamicsautomated mining of differences using hyperspec-tral images. 10(1):e0116902.[26] I. F. Wardlaw. Interaction between drought and chronic hightemperature during kernel ﬁlling in wheat in controlled en-vironment. Annals of Botany 90(4):469–476, Oct. 2002.[27] P. Wilf, S. Zhang, S. Chikkerur, S. A. Little, S. L. Wing, andT. Serre. Computer vision cracks the leaf code. Proceedingsof the National Academy of Sciences 113(12):3305–3310,2016.2063