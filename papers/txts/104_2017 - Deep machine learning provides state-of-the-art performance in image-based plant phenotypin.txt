GigaScience,6 ,2 1 ,1 1 0doi: 10.1093/gigascience/gix083Advance Access Publication Date: 23 August 2017ResearchRESEARCHDeep machine learning provides state-of-the-artperformance in image-based plant phenotypingMichael P. Pound1, Jonathan A. Atkinson2, Alexandra J. Townsend2,Michael H. Wilson3, Marcus Grif/f_iths2,A r nS .J c s n1, Adrian Bulat1,Georgios Tzimiropoulos1,D r e nM .W l s2, Erik H. Murchie2,Tony P. Pridmore1and Andrew P. French1,2,∗1School of Computer Science, University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham, NG81BB, UK,2School of Biosciences, University of Nottingham, Sutton Bonington Campus, Nr Loughborough, LE125RD, UK and3Centre for Plant Sciences, Faculty of Biological Sciences, University of Leeds, Leeds, LS2 9JT, UK∗Correspondence address.Andrew P. French, School of Computer Science, University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham, NG81BB, UK. Tel: 0115 9516374; E-mail:andrew.p.french@nottingham.ac.ukAbstractIn plant phenotyping, it has become important to be able to measure many features on large image sets in order to aidgenetic discovery. The size of the datasets, now often captured robotically, often precludes manual inspection, hence themotivation for /f_inding fully automated approach. Deep learning is an emerging /f_ield that promises unparalleled results onmany data analysis problems. Building on arti/f_icial neural networks, deep approaches have many more hidden layers in thenetwork, and hence have greater discriminative and predictive power. We demonstrate the use of such approaches as partof plant phenotyping pipeline. We show the success offered by such techniques when applied to the challenging problemof image-based plant phenotyping and demonstrate state-of-the-art results (>97% accuracy) for root and shoot featureidenti/f_ication and localization. We use fully automated trait identi/f_ication using deep learning to identify quantitative traitloci in root architecture datasets. The majority (12 out of 14) of manually identi/f_ied quantitative trait loci were alsodiscovered using our automated approach based on deep learning detection to locate plant features. We have shown deeplearning–based phenotyping to have very good detection and localization accuracy in validation and testing image sets. Wehave shown that such features can be used to derive meaningful biological traits, which in turn can be used in quantitativetrait loci discovery pipelines. This process can be completely automated. We predict paradigm shift in image-basedphenotyping bought about by such deep learning approaches, given suf/f_icient training sets.Keywords:Phenotyping; deep learning; root; shoot; QTL; image analysisBackgroundThe large increase in available genomic information in plantbiology has led to need for truly high-throughput phenotyp-ing work/f_lows to bridge the increasing genotype-phenotype gap.Image analysis has become key component in these work-/f_lows [1], where automated measurement and counting haveReceived:26 May 2017;Revised:27 July 2017;Accepted:16 August 2017C/circlecopyrtThe Author 2017. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium,provided the original work is properly cited.1Downloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 20202 Pound et al.allowed for increased throughput and unbiased, consistentmeasurement systems. Machine learning has proven to be oneof the most /f_lexible and powerful analysis techniques, withapproaches such as Support Vector Machines 2] and RandomForests 3] achieving the highest success rates to date. Whilstthese techniques provide considerable success in many situa-tions 4], their performance is saturating and often falls short ofthe high accuracy required for fully automated systems. How-ever, with careful crafting of features, these approaches can havepractical application still. What deep learning promises is thelearning of the features themselves; often, given suf/f_icient train-ing data, allowing for increases of accuracy.Before introducing deep learning, it is helpful to /f_irst con-sider traditional machine learning techniques applied to bioim-age analysis. It is generally assumed that raw images willcontain too much information for machine learning approachto ef/f_iciently process. For this reason, much of the established re-search in this /f_ield involves pre-computation of domain-speci/f_icimage features hand-crafted, for example, to detect areas of highcontrast such as types of edges and corners. This pre-processingis intended to capture enough information to represent classesof objects but contain signi/f_icantly fewer dimensions than thefull set of original image pixels 4]. The output of this featuredetection is passed into classi/f_ier, where classes (here, pheno-typic traits) can be ef/f_iciently separated. Crucially, the choice offeatures is left to the designer and is often limited to existingsets, popular in the literature. These hand-crafted features arenot guaranteed to provide the subsequent learning algorithmwith the optimal description of the data, which in turn will re-duce its effectiveness. It is easy to accidentally limit the appli-cation of the algorithm to speci/f_ic tasks; an approach that per-forms well in one task may fail to perform in different task.There is, therefore, motivation to produce more general learn-ing approaches.Early general approaches include the biologically inspired ar-ti/f_icial neural networks (ANNs), which use set of simulatedneuron-like connections and transfer inputs via set of learntfunctions to series of outputs. These represent set of acti-vations propagating through network structure, triggered byinput data, and resulting in an output activation pattern. ANNstypically use three layers, one for input, hidden internal layer,and an output layer. Modern deep learning approaches extendthis concept and may contain many additional layers of arti/f_i-cial neurons (hence the term deep )a dw t hi c e s dc m l x -ity bring signi/f_icantly increased discriminative power 5]. Cuttingedge algorithms and computational hardware have bought thetraining time for such networks down to practical levels achiev-able in most labs. Convolutional neural networks (CNNs) special-ize this representation further, replacing the neuron layers withfeature-detecting convolution layers (biologically inspired by theorganization of the visual /f_ield) 6], before /f_inishing with tradi-tional ANN layers to perform classi/f_ication (Fig. 1). CNNs havebeen quickly adopted by the computer vision community, buthave also recently been used successfully in the life sciences 7]and medicine 8].The CNN transforms feature maps from previous layers, cre-ating rich hierarchy of features that can be used for clas-si/f_ication. For example, while the initial layer may computesimple primitives such as edges and corners, deeper into thenetwork feature maps, based on these will highlight groups ofcorners and edges. Deeper still, feature maps may contain com-plex arrangements of features representing real-world objects[9]. It is important to note that these features are learnt by theCNN training algorithms and are not hand-coded.Modern CNNs will typically use many layers, which makestraining the networks complex, often requiring hundreds, some-times thousands, of images to train to the desired accuracy 10].However, once trained, their accuracy is unrivaled, and they canbe transferred to other related domains by re-training using sig-ni/f_icantly fewer images 11]. CNN is trained by iteratively pass-ing example images containing the objects to be detected intothe network and adjusting the network parameters based onthe results. The values of the convolutional /f_ilters are automat-ically adjusted to improve the result the next time similar im-age is seen, process that is repeated for as many images aspossible.To demonstrate the effectiveness of this deep learning ap-proach, we /f_irst trained separate CNNs on tasks central toplant phenotyping, framed as classi/f_ication problems. In the/f_irst, we address the following question: given small section ofa root system image, can CNN identify if root tip is present?The architecture of root system is an important aspect of itsphysiological function; the root system’s structure allows it toaccess different nutrients and water within the soil pro/f_ile. Inphenotyping, particularly with high-throughput 2D approaches,identifying features such as root tips represents the rate-limitingstep in data quanti/f_ication. We prepared training image data inwhich some images contained root tips and some did not. Thiswas derived from dataset containing 2500 annotated images ofwhole root systems, and automatically generated classi/f_icationimages, by cropping at the annotated tip locations (See Fig. 2,l f tside). This dataset is publically available in Pound et al. 12].In the second classi/f_ication problem, given an image of sec-tion of plant shoot, we ask: can CNN identify biologically rel-evant features such as leaf and ear tips, bases, etc.? This wouldallow high-throughput phenotyping on an extremely large num-ber of lines based on single images. It also allows 3D shootstructure to be linked with physiological functioning: e.g., theFigure 1: simpli/f_ied example of CNN architecture operating on /f_ixed size image of part of an ear of wheat. The network performs alternating convolution andpooling operations (see the online methods for details). Each convolutional layer automatically extracts useful features, such as edges or corners, output ting numberof feature maps. Pooling operations shrink the size of the feature maps to improve ef/f_iciency. The number of feature maps is increased deeper into the ne twork toimprove classi/f_ication accuracy. Finally, standard neural network layers comprise the classi/f_ication layers, which output probabilities for each lass.Downloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 2020Deep learning for plant phenotyping 3Figure 2: Example training and validation images from our root tip and shoot feature datasets. Positive samples were taken at locations annotated by user. Neg ativesamples were generated on the root system and at random for the root images, and on computed feature points on the shoot images.separation into individual leaves and organs allows us to placebiologically distinct plant parts within useful functional con-text (different leaves, reproductive organs). To do this, we hand-annotated 1664 images of wheat plants, labelling leaf tips, leafbases, ear tips, and ear bases. Classi/f_ication images were thenautomatically extracted from these images as before (see Fig. 2,right side). This dataset is also publically available in Pound et al.[12].We then quantify the accuracy of /f_inding the features in the2i a es t .W ea s os o wh wi ti sp s i l et ol c l z et ef a -tures within the image—answering questions such as, where arethe root tips located? The Methods section explains in detail theprocess of preparing the networks and data and the training ofthe CNNs, as well as the localization approach used.A common goal of phenotyping studies is the use of map-ping populations to investigate the genetic architecture of com-plex traits by identifying quantitative trait loci (QTL; regions ofDNA that correlate with phenotypic variations). QTL analysis isbased on detecting an association between phenotype and geno-typic markers; the markers are used to partition populationinto genotypic groups, whereupon trait differences between thegroups can be identi/f_ied 13]. The collective effect of numerousgenes controls the genetic variation in quantitative trait. Iden-tifying such QTL is of agronomic importance and feeds into thedevelopment of crop species. QTL discovery itself relies on thestatistical analysis of phenotypic traits and has been limited bythe lack of unbiased, high-throughput techniques to extract traitvalues from image sets.Finally, then, we demonstrate that it is possible to auto-matically derive traits from images using these features, whichcan be used to identify the underlying genetic architecture byidentifying QTL, key goal of many phenotyping studies. Theoutput of the root CNN (the detected root tips) is then used to de-rive simple descriptive traits automatically, which are then usedin QTL discovery process and compared to QTL discovery via amore manual approach.Data descriptionT wo datasets have been used in this paper, each presenting aunique challenge to deep learning. By presenting both, we wishto highlight the wide applicability of the approach.Root analysisBread wheat Triticum aestivum .) seeds were sieved to uniformsize, sterilized, and pre-germinated before transfer to growthpouches in controlled environment chamber (12-hour pho-toperiod: 20◦C day, 15◦C night, with light intensity of 400µmol m–2 s–1 PAR), as per Atkinson et al. 14]. After days(two-leaf stage), individual pouches were transferred to copystand for imaging using Nikon D5100 DSLR camera con-trolled using NKRemote software (Breeze Systems Ltd, Camber-ley, UK). Root system architectural traits were extracted fromimages of 2697 seedlings using the RootNav software (RootNav,RRID:SCR 015584 )[15] and used to produce the input images forCNN training.Shoot analysisWheat varieties were grown as detailed previously 16]. Plantsin pots were imaged according to the protocol of Pound et al.[17]. The developmental stages of the plants in both years oftrial were the same. At anthesis, wheat plants (roots and shoots)were removed from the /f_ield and taken to photography studiolocated close by to prevent wilting and damage to the shoots.They were imaged using /f_ixed Canon 650D cameras, with aminimum of 40 images per plant. Images were captured using arevolving turntable, including /f_ixed size calibration target. Thistarget is used to facilitate 3D reconstruction, which does not fea-ture in this work.Downloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 20204 Pound et al.Table 1: Classi/f_ication results for both root and shoot datasetsFeature Correctly classi/f_ied Misclassi/f_ied Accuracy (%)Root tip 2904 73 97.5Root tip negative 5687 65 98.9Total/average 8591 138 98.4Feature Correctly classi/f_ied Misclassi/f_ied Accuracy (%)Leaf tip 2225 113 95.2Leaf base 2299 52 97.8Ear tip 686 15 97.9Ear base 765 23 97.1Shoot negative 6110 136 97.8Total/average 12 085 339 97.3Leaf tips represent the hardest classi/f_ication problem in the datasets, with large variations in orientation, size, shape, and colour. In all cases, he accuracy has remainedabove 95%, with the average accuracy of both networks above 97%. The root tip network performs marginally better overall, perhaps to be expected due to the simplernature of the image data. Complete confusion matrices can be found in Additional /f_ile 3.Further details on preparation of the image data for the net-works can be found in the Methods section.AnalysesClassi/f_icationOnce networks are built and training has been completed (seethe Methods section), the learned parameters of the network arethen stored and can be used to perform classi/f_ication when re-quired. The /f_inal accuracy of the networks described in this pa-per is the result of /f_inal evaluation over all validation imagesonce training was stopped. Our CNN models, learned parame-ters, and all the related scripts for training and validation willbe made publically available 12].For both the root and shoot data, we randomly separated 80%of the data into training set, and 20% remained for validation.To evaluate the accuracy of each network, we ran each valida-tion image through the network, obtaining the likelihood of eachclass. These were then compared to the true label for each im-age to ascertain whether the network had correctly classi/f_iedthe image. Based on this, the accuracy of the root tip detectionnetwork was found to be 98.4%. The shoot dataset, containing 4classes of shoot features, along with numerous instances of clut-tered, non-plant background, represents an even more challeng-ing task. In this case, the shoot network successfully classi/f_ied97.3% of images. In both cases, CNNs here have out-performedrecent state-of-the-art systems (e.g., accuracies of 80–90% havebeen typical) 2,18]. Accuracy results for individual classes canbe seen in Table 1.N t ea s ot a tb t ht e es e a i sa em c hmore challenging than typical successes seen to date as the im-ages involved are much less constrained.LocalizationAs well as identifying features by classifying image crops, it isnecessary in quantitative phenotyping to locate the featureswithin the larger image. For example, reliably identifying thelocations of root tips is bottleneck in automated root systemanalysis 15] and is often omitted from image analysis softwaredue to the challenges localization presents. As another example,locating seed feature points must occur before automated trac-ing in RootTrace (RootTrace, RRID:SCR 015585 )[19]. Localizationof the different biological feature classes for shoot is vital incapturing the architecture of the plant, essential for phenotyp-ing. We also later show that automated localization of such fea-tures can be used to identify the underlying genetic architectureof traits.We have extended our root and shoot classi/f_iers to performlocalization by scanning over each original image, applying therespective classi/f_ier over each image at regular pixel intervals(often referred to as stride). Selection of the stride is straight-forward and is compromise between pixel-wise accuracy ofthe resulting classi/f_ication map and computational ef/f_iciency.A stride of will produce sub-images centred on every pixel,such that images will overlap with the majority of the previoussub-image. This means that feature visible in one image willalso be visible in number of consecutive images around it. Forboth the root and shoot system images, we chose stride of 4,which results in single scan that takes under minutes, andyet will output classi/f_ication map showing each feature loca-tion clearly. The scripts we used to perform this classi/f_icationand repeat this automatically over any number of images can bedownloaded alongside our models.As the output of the network is set of class probabilities,pixels observed as above likelihood threshold are marked asbelonging to speci/f_ic class (see Fig. 3).Testing localization accuracyWe have tested the real-world accuracy of our localization stepby measuring the proportion of location windows containingfalse-positives or -negatives. This testing was performed onunseen test data, comprising 20 images for roots and 20 forshoots. In both cases, no images, or parts of these images, hadbeen used in the training or validation of either network. Accu-racy was measured as the percentage of pixels that were cor-rectly classi/f_ied as either true-positives or true-negatives. False-positives were determined as those pixels that were classi/f_ied asa feature but were outside of radius around any ground truthfeatures. This radius was set as half of the classi/f_ication win-dow size, in which any feature should be visible. False-negativeswere those pixels within the same radius of ground truth fea-ture that were not correctly classi/f_ied as those features. Separateresults for roots and shoots, and for each class, can be seen inTable 2;t s ti a e sa do t u tc nb es e ni nA d t o a l/f_i e2 .The accuracy of the root tip location is 99.85% and the ac-curacy of the shoot feature location is 99.07% when totalledover all features. Accuracy that is higher than that of the baseclassi/f_iers presented earlier (Table 1)i sn ts r r s n .D r n gtraining of the networks, we generated particularly challeng-ing negative examples of image features; these examplesDownloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 2020Deep learning for plant phenotyping 5Figure 3: Localization examples. Images showing the response of our classi/f_ier using sliding window over each input image. a)T r ee a p e so fw e tr o tt plocalization. Regions of high response from the classi/f_ier are shown in yellow. b) wo examples of wheat shoot feature localization. Regions of high response fromthe classi/f_ier for leaf tips are highlighted in orange, leaf bases in yellow, ear tips in blue, and ear bases in pink. portion of the second image has been zoomed andshown with and without features highlighted. More images can be seen in Additional /f_ile 1.Table 2: Testing results for our image scanning approach over 20 unseen root images and 20 unseen shoot imagesFeature False positive (%) False negative (%) Feature accuracy (%)Roots Root tip 0.03 0.12 99.85Shoots Leaf tip 0.24 0.12 99.64Leaf base 0.22 0.10 99.68Ear tip 0.08 0.02 99.91Ear base 0.11 0.05 99.85Feature accuracy is the number of true-positive and true-negative pixels, divided by the total number of pixels over the 20 images. Actual testing ima ges and resultscan be seen in Additional /f_ile 2.comprise only very small fraction of each whole, real-worldimage. The scripts used for testing will be made available along-side our models 12].Application to QTL discoverySo far we have demonstrated the success of the approach inlocating features in images. Here, we wish to show the powerof complete pipeline for phenotyping and discovery. We willuse traits derived from features automatically discovered via ourdeep learning approach to identify signi/f_icant QTL for the rootsystem, highlighting the power of the approach for genetic dis-covery. As baseline, using the semi-automated software pack-age RootNav 15], root traits were manually determined from1709 images of the seedling root systems of 92 members of awheat doubled haploid mapping population 14]. These trait val-ues were then used to identify 29 root QTL 14], representing 5classes of trait. This same image set formed part of the trainingdataset for the root tip detection CNN. We will here consider onlytraits related to root tips as this is the feature our network spe-cializes in, but of course different and additional features couldbe learned in the future.The output of the root tip CNN after scanning over an imageis heatmap of high-likelihood tip locations. This was adaptedto produce individual co-ordinates for each identi/f_ied root tip.Mathematical morphology was used to erode the heat map witha3×3 structuring element using iterations. This removessmall artefacts’ output as single pixels in the heat map and canseparate some root tips that are close together. This level of ero-sion was chosen as compromise between effectively removingnoise and removing root tips themselves in error. connectedTable 3: List of root traits derived from the tip-detection CNN outputand how they were computedName DescriptionTip count The sum of all connected components foundHull area The area of the convex hull derived from thecentroids of all tipsWidth/depth The width and depth of the bounding boxsurrounding all tipsWidth:depthratioCalculated as width divided by depthMean X/Y The mean and positions of all tipsStandarddeviation X/YThe standard deviation of the and positionsof all tipsTop100/200/300pxcountA count of the number of tips located in the top100-, 200-, and 300-pixel strips below the seedposition calculated aboveTotal length An estimate for the length of the root system,calculated as the sum of the distances from eachtip to the seed positionCentre massX/YThe mean X, position of all tipsThe name is derived from the trait they can be seen to estimate or represent.component algorithm was then used to /f_ind single centroid ofeach foreground region, representing the most likely root tip lo-cations. Geometrical traits were then conceived, which were de-rived from these recorded tip positions (listed in Table 3). Notethat if detecting more than just tips of roots (perhaps the seedlocation or the roots themselves), much more complex and po-tentially informative traits could be derived. However, here weDownloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 20206 Pound et al.Table 4: QTL discovery results from user-supervised (RootNav, RN) and CNN-derived deep learning (DL) approachesRN DLTrait Chr Pos LOD CI Chr Pos LOD CI Additive effect Nearest markerCentre of mass (x) 1A 70.3 2.5 47.7–163.6Width/depth ratio 4D 4.8 2.7 0.8–67.6 4D 2.8 3.2 0.8–67.6 0.07 IAAV5065Total root length 6D 4.4 24.0 2–53 6D 4.4 12.7 2–53 –2201 wsnp Exc4789 8550135Convex hull 6D 4.4 17.6 2–53 6D 4.4 17.3 2–53 –264 026 wsnp Exc4789 8550135Centre of mass (x) 6D 26 2.8 0–92.5 6D 17.1 2–53 –151 wsnp Exc4789 8550135Centre of mass (y) 6D 4.4 19.1 2–53 6D 4.4 10.0 0–53 –105 wsnp Exc4789 8550135Lateral count/tip count 6D 4.4 9.1 0–53 6D 4.4 10.2 0–53 –4.53 wsnp Exc4789 8550135Maximum depth 6D 4.4 22.7 2–53 6D 4.4 25.1 2–53 –388 wsnp Exc4789 8550135Maximum width 6D 4.4 6.4 0–53 6D 15.0 2–53 –241 wsnp Exc4789 8550135Total root length 7D 27 9.0 16–52 7D 30 3.4 16–52 –1122 Kukri c48125 714Lateral count/tip count 7D 29 2.4 16–101.8 7D 29 4.5 16–101.8 –2.76 wsnp Rac8297 14095831Centre of mass (x) 7D 19 2.7 16–38.8Convex hull 7D 34 3.5 16–62.4 7D 34 4.4 16–62.4 –123 896 Kukri c48125 714Maximum depth 7D 30 5.8 16–52 7D 30 6.9 16–62.4 –155 wsnp Rac8297 14095831Note there are QTL identi/f_ied using RN that are missed by the DL approach; all others were identi/f_ied by both methods. Chr: chromosome; CI: con/f_idence int ervalstart and end positions; DL: deep learning; Pos: position; RN: RootNav.demonstrate with simple tip-based traits and use these traitsto identify QTL via the same pipeline developed for the origi-nal RootNav-derived images 14]. Here we make an estimate forseed location derived from tips alone, taken as the mid-point ofthe top of the bounding box surrounding all seed tips. This is anestimate only, but is calculated consistently for all images.The traits in Table 3were then used in subsequent QTL anal-ysis. QTL calculation and plotting of logarithm of odds (LOD)scores were conducted using package “qtl” on best linear pre-dictors in the /f_irst step as single QTL model employing the ex-tended Haley-Knott method on imputed genotypes. Signi/f_icantthresholds for the QTLs were calculated from the data distri-bution. Final QTL LOD scores and effects were received from amultiple QTL model using the QTL detected in the initial scan.The high-density Savannah ×Rialto iSelect map 20]w su e ,with redundant markers and those markers closer than 0.5 cMstripped out. Outputs of the analysis program R/qtl 21] are sum-marized in Table 4. Many of the QTL found in the original Root-Nav study were based on measurements of root angle and thuswould not be expected to be found using parameters computablefrom tip positions alone; thus, these were not considered inthese analyses (please see original paper for the full list) 14].However, as can be seen in Table 3,n a l ya lt a t sr l t dt ot plocation that the semi-automated RootNav approach returnedwere also picked up by deep learning.Traits derived from the CNN resulted in the detection of 12QTL; all of these coincide with loci discovered using the manualRootNav approach. The QTL on chromosome 1A for one trait,“Centre of mass (x),” was not detected using the deep learningapproach but was found using trait values from RootNav. Thistrait represents the centre of mass of the root system in the hor-izontal direction and only varies by 11 mm across the mappingpopulation in the RootNav data. By here estimating the seed po-sition, this small amount of variation is not captured using theroot tip positions alone, and thus the QTL is not detected. Addi-tionally, the trait itself is likely to be of little biological relevancealthough it is signi/f_icant in the RootNav analysis, so we includeit here for completeness. Finally, it is worth noting though that asecond QTL for the same trait was detected on chromosome 6Dusing both systems.Extraction of phenotypic information using RootNav requiresa skilled user and considerable investment of time (the mostexperienced users take on average minutes to process an im-age). The CNN-derived tip detection pipeline runs completelyunattended, is free from operator bias, and successfully found86% of the tip-related QTL previously identi/f_ied using trait val-ues extracted via the semi-automated RootNav pipeline. Thishighlights the potential for deep learning in delivering the auto-mated, high-throughput extraction of useful data from imagesrequired for phenotyping studies.Of course, the bene/f_its of deep learning are only possiblegiven suf/f_icient quantities of representative training data. Thedeeper the network, the more data are required. Quality of train-ing data and the training protocol can affect /f_inal results. Tra-ditional machine learning may work with smaller quantities oftraining data due to fewer parameters having to be learnt in themodels. For comparison, the root architecture dataset presentedin this study has also been used with crafted feature set andRandom Forest classi/f_ication in similar phenotyping pipeline;we refer the reader to Atkinson et al. 22] for more details.DiscussionCNNs offer unparalleled discriminative performance in classi/f_i-cation of images and localization tasks. Here, we have demon-strated their ef/f_icacy of not only the classi/f_ication, but alsolocalization of plant root and shoot features, signi/f_icantly im-proving upon the state of the art. To our knowledge, this is the/f_irst demonstration of deep learning applied in the localizationof plant features. The success here parallels the success of deeplearning in related image analysis tasks such as leaf segmenta-tion 23]. We have also demonstrated the ability to derive mean-ingful traits from simple feature detection as demonstrator,from which we successfully identify signi/f_icant QTL, corrobo-rated by manual methods. The successful application of deeplearning in QTL analysis parallels the application of traditionalmachine learning on similar task 22]. To improve our ownmethods in future work, we will explore the application of so-called fully convolutional networks, performing segmentationdirectly, rather than via scanning approach. We also hope toapply feature localization to other datasets, and in particular ex-amine the ef/f_icacy of these techniques in /f_ield images.Deep learning is very general technique; CNNs can be eas-ily applied to other challenging problems and determine use-ful features for classi/f_ication automatically during training. Mi-croscopy, x-ray, ultrasound, magnetic resonance imaging, orDownloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 2020Deep learning for plant phenotyping 7other forms of medicinal and structural imaging are all targetswhere deep learning will yield excellent results. Areas involv-ing challenging, unstructured images—such as those from the/f_ield—are of particular interest for future work.Training of CNN methods of course depends on high-qualityannotations on which to train. Despite skilled biological ex-perts performing the annotation, even here we should expectsome error in the annotations, over the hundreds of imagesand many thousands of features. Whilst we have not quanti-/f_ied this error on our data, it is worth keeping in mind that wemust minimize such occurrences when using CNNs and thatany claim to accuracy depends on “correct” annotation—whatif the network is right, and the annotator wrong? These arequestions that deep learning will force us to address. Annota-tion is also time-consuming process, and existing datasets willperform key role in boot strapping new techniques and ap-plications of deep learning. This will likely drive renewed ef-fort in large, publicly available datasets, including high-qualityannotations.Potential implicationsWe believe that the substantial increase in throughput offeredby deep learning will lead to an improvement in the under-standing of biological function akin to other high-throughputimprovements in biology, such as expression arrays 24]a dnext-generation sequencing 25], and anticipate numerousparadigm-shifting breakthroughs over the coming years.MethodsTraining and validation image preparationConvolutional neural networks using traditional neural networklayers for classi/f_ication can be applied to images of any rea-sonable size, but once trained at certain size, this must re-main consistent. We chose input sizes of 32 ×32 pixels forroot tip images and 64 ×64 pixels for shoot feature images. Inthe root domain, 32 ×32 image was found to be adequate tocapture root tip feature, along with enough context from thesurrounding image. The 64 ×64 resolution of shoot features waschosen as compromise between ef/f_iciency and the higher res-olution necessary to handle the more complex features seen inthese images. Choosing size appropriate to the feature of inter-est whilst maintaining balance with computational ef/f_iciencyis key here.For root images, we obtained root tip positions from an exist-ing database of manually annotated root systems, paired withthe captured input images. For each source image, we createdcropped training images centred on each recorded root tip posi-tion. This resulted in variable number of training images persource image, depending on how many root tips had been an-notated by the user. We restricted root tip images to primaryand lateral roots that were longer than half the window size (16pixels). Avoiding extremely short lateral roots avoids ambiguitywith root hairs, which appear frequently on many of the images.For all training images in the root dataset, we cropped source im-ages at 42 ×42 pixels, and then performed an additional crop to32×32 randomly during training. This approach, known as dataaugmentation, is akin to producing many more training imageswith variation in the location of the tips within the cropped win-dows, such that the root tips do not appear in the exact centre ofeach training image every time. This approach has been shownto produce improved accuracy when the classi/f_ication target isnot necessarily in the centre of each image, as may be the casewhen we use our scanning localization approach.We additionally generated negative training images, whichdo not contain the features of interest, with two times morenegative images than positive ones. We increased the numberof negative images in order to adequately capture the wide vari-ety of different negative images that are possible on in this data.Half of the negative data was generated at random points on thesource image, but limited to areas that contained no root tips.The remaining negative data were generated at random posi-tions on the known root system, again avoiding root tips. Thisis form of hard negative mining, where negative data are gen-erated on regions that appear similar to the positive data. Wewant the network to learn that we are only interested in tips ofroots, not other structures on the root. This has been shown toimprove the accuracy of machine learning algorithms over neg-ative data produced entirely at random 26]. The total number ofimages produced was 43 641, which was split at random into atraining set of 34 912 and validation set of 8729.A similar approach was used for the preparation of shoot fea-ture images. For each source image, we selected cropped imagesat each manually annotated location, as with the root tips. Theshoot images are higher resolution than the root images, so wefound that we obtained better accuracy if we cropped 128 ×128images, then scaled to 64 ×64 for use in the network. This sim-ply includes more of each image within the /f_ield of view of thesmaller windows; i.e., we retain more contextual information.Each type of feature (e.g., leaf tip, ear tip) was summed to pro-duce an overall positive image count, and we then generatedan equal number of negative images per source image. Unlikethe root system data, where information on the position of theremaining root system (derived from the manual annotations)could be used to generate hard negative data, the shoot annota-tions only included the speci/f_ic features to be classi/f_ied. In orderto generate hard negative data, we used Harris feature detector[27] to generate candidate points of interest, then selected fromthis set at random (discounting areas around positive features).This ensured that the negative data contained large amountsof clutter and other plant material, rather than just plain back-ground regions. Finally, we generated small number of addi-tional images from truly random locations to ensure that ar-eas such as the white background were represented suf/f_iciently.The resulting dataset contained 62 118 images, of which 49 694were training images, and the remaining 12 424 were used forvalidation.At this point, we have constructed suitable training sets ofimages derived from manual annotations. The next task is todevelop the network architecture itself and train the subsequentnetworks.CNN architecture designWe used the Caffe deep learning library 28] to develop each net-work. In Caffe, networks are described using series of struc-tured /f_iles, along with information on training and validation,such as how frequently to perform validation when training it-erations, and so-called hyperparameters, such as the learningrate, which will be described below.We designed separate CNN architectures for each problem.These architectures are shown in Fig. 4; they adopt commonapproach to CNN design, utilizing multiple convolutional layersusing ×3 kernels prior to each pooling layer 29]. The shootCNN contains more layers to accommodate the larger input im-age size. It also includes increased feature counts in deeperDownloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 20208 Pound et al.Figure 4: The architecture of both convolutional neural networks (left: root, right:shoot). In each case, convolution and pooling layers reduce the spatial resolu-tion to ×1, while increasing the feature resolution. All convolutional layersused kernels size ×3 pixels, and the number of different /f_ilters is shown at theright of each layer. Following the convolution and pooling layers, the fully con-nected (neural network) layers perform classi/f_ication of the images. We includedrecti/f_ied linear unit (ReLu) layers between all convolutional and fully connectedlayers, and dropout layers between each fully connected layer.layers to address the more challenging classi/f_ication task posedby the shoot images. Both networks end in neural networkclassi/f_ication layers (often referred to as fully connected lay-ers) that reduce the output sizes to and 5, respectively. Oncetrained, these /f_inal neurons represent the likelihood that thenetwork has observed each class (e.g., root tip or not root tip)and can be read to determine which class the network hasidenti/f_ied.The root CNN contained two groups of two convolutional lay-ers, and one max pooling layer. Following these, two /f_inal con-volutional layers performed further feature extraction, beforethree standard neural network layers performed the classi/f_ica-tion. The feature size of the convolutional layers was increasedafter each pooling layer, beginning at 64 convolutional /f_ilters,up to 256 /f_ilters. Finally, the neural network layers gradually re-duced the feature size back down to two, representing the sep-arate “root tip” and “root negative” classes.The shoot CNN contains three groups of convolutions andpooling layers. The number of convolutional layers betweenpooling layers varied slightly throughout the architecture in or-der to ensure that the spatial resolution of the data was alwaysa multiple of two. single /f_inal convolution is followed by threeneural network layers performing the classi/f_ication. The featuresizes of the convolutional and neural network layers were alsoincreased beyond that of the root CNN. Feature sizes started at64 /f_ilters, up to maximum of 512 /f_ilters. The neural networklayers decrease this feature size back down to 5, representingthe classes being detected.Recent developments in CNNs have proposed additionalcomponents that improve performance. Neural networks re-quire non-linear functions between layers in order to capturethe complex non-linearity of the classi/f_ication tasks. Tradition-ally, sigmoid or tanh functions have been used, where the re-sult of each convolutional /f_ilter at each position is passed into anonlinear function before being passed to the next layer. Morerecent work 10] proposed an alternative function, the recti/f_iedlinear unit (“Relu”), which has been shown to improve the speedof training deep networks. We utilized Relu layers between allconvolutional layers and between all fully connected neural net-work layers. Other work 30] proposed an approach whereby apercentage of fully connected neurons is randomly deactivatedduring each iteration of training; this has been shown to avoidthe over/f_itting problem, in which the classi/f_ication of the train-ing data improves, but at the expense of generality on the un-seen data. By deactivating neurons some of the time, the fullyconnected layers are forced to learn from all parts of the net-work, rather than become focused on few key neurons. We in-cluded dropout layers with 50% dropout rate between the fullyconnected layers.CNN training and validationThe Caffe library is built to perform iterative training and vali-dation for as long as is required. Periodically, the accuracy of thenetworks was measured using the separate validation data, andlearning was halted after steady state was reached, where nofurther improvement was seen if the network was left training.The learning rate speci/f_ies how quickly the network attempts toimprove based upon the current set of images it is examining.This is an important feature of network learning; low learn-ing rate will mean the network does not adapt suf/f_iciently fastto correctly classify the images it sees. learning rate that istoo high may cause the network to wildly over-adapt, meaningit will improve on the current set of images, but at the expenseof all the images it has seen previously. As with most modernCNN approaches, we chose higher learning rate to begin train-ing, then periodically decreased this rate to “re/f_ine” the networkto higher and higher accuracies. We began with learning rateof 0.1, then decreased the learning rate by factor of 10 every20 000 iterations. In practice, we found that our networks wererobust to changes in this learning rate, but that we stopped see-ing any real improvement in accuracy when the learning rate fellbelow ×10−3. Before entry into the network, the mean imagecolour for each dataset was subtracted from each image in orderto centre pixel values around 0.Downloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 2020Deep learning for plant phenotyping 9Availability of data and materialsData further supporting this work, such as the root and shootimage datasets, as well as the Root Caffe model and ShootCaffe model, are open and available in the GigaScience reposi-tory, Giga DB [12]. Further details on the methods used in thisstudy are also available at Protocols.io 31].Additional /f_ilesAdditional /f_ile 1: Images showing the response of our classi/f_ierusing sliding window over selected input images, for roots andshoots. Roots: regions of high response from the classi/f_ier areshown in yellow. Shoots: regions of high response from the clas-si/f_ier for leaf tips are highlighted in orange, leaf bases in yellow,ear tips in blue, ear bases in pink.Additional /f_ile 2: Unseen test images. Pairs of images are pre-sented: original images on the left, localized features shown onthe right. Roots: regions of high response from the classi/f_ier areshown in yellow. Shoots: regions of high response from the clas-si/f_ier for leaf tips are highlighted in orange, leaf bases in yellow,ear tips in blue, ear bases in pink.Additional /f_ile 3: Confusion matrices for the root and shootclassi/f_ication datasets.AbbreviationsCNN: Convolutional Neural Network; QTL: Quantitative TraitLoci.Competing interestsThe authors declare that they have no competing interests.FundingThis work was supported by the Biotechnology and BiologicalSciences Research Council [grant number BB/N018575/1], par-tially funding APF and MPP; and European Research CouncilAdvanced Grant [FUTUREROOTS 294729], funding JAA.Author contributionsM.P.P. developed the deep learning system and image process-ing and carried out the method development, along with A.P.F.,D.M.W., and T.P.P. J.A.A. assisted, and collected and annotateddata along with A.J.B. and M.G. M.H.W. and E.H.M. assisted withthe preparation of the root and shoot datasets, respectively.A.S.J., A.B., and G.T. provided valuable deep learning expertise.A.P.F., M.P.P., D.M.W., J.A.A., and T.P.P. wrote the manuscript, withassistance from all authors.References1. Walter A, Liebisch F, Hund A. Plant phenotyping: from beanweighing to image analysis. Plant Methods 2015; 11(1):1–11.2. Wilf P, Zhang S, Chikkerur et al. Computer vision cracksthe leaf code. Proc Natl Acad Sci S 2016; 113(12):3305–10.3. Ho. TK. Random decision forests. In: Proceedings of the ThirdInternational Conference on Document Analysis and Recog-nition, vol. 1, 1995. p. 278–82.4. Singh A, Ganapathysubramanian B, Singh AK et al. Machinelearning for high-throughput stress phenotyping in plants.Trends Plant Sci 2016; 21(2):110–24.5. Lecun Y, Bottou L, Bengio et al. Gradient-based learningapplied to document recognition. Proc IEEE 1998; 86(11):2278–324.6. Hubel DH, Wiesel TN. Receptive /f_ields and functional archi-tecture of monkey striate cortex. Physiol 1968; 195(1):215–43.7. Zhou J, Troyanskaya OG. Predicting effects of noncoding vari-ants with deep learning-based sequence model. Nat Meth-ods 2015; 12(10):931–4.8. Esteva A, Kuprel B, Novoa RA et al. Dermatologist-level clas-si/f_ication of skin cancer with deep neural networks. Nature2017; 542(7639):115–8.9. Zeiler MD, Fergus R. Visualizing and understanding convo-lutional networks. In: Fleet D, Pajdla T, Schiele B, TuytelaarsT, eds. Computer Vision ECCV 2014. Cham, Switzerland:Springer International Publishing, 2014:818–33.10. Krizhevsky A, Sutskever I, Hinton GE. ImageNet classi/f_ica-tion with deep convolutional neural networks. In: Pereira F,Burges CJC, Bottou L, eds. Advances in Neural InformationProcessing Systems 25. New York, USA: Curran Associates,Inc., 2012:1097–105.11. Long J, Shelhamer E, Darrell T. Fully convolutional networksfor semantic segmentation. The IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), 2015, p. 3431–40.12. Pound MP, Atkinson JA, Burgess AJ et al. Supporting datafor “Deep Machine Learning provides state-of-the-art per-formance in image-based plant phenotyping.” GigaScienceDatabase 2017. http://dx.doi.org/10.5524/100343 .13. Rajpal VR, Rao SR, Raina SN, eds. Molecular Breeding for Sus-tainable Crop Improvement. Cham, Switzerland: SpringerInternational Publishing, 2016.14. Atkinson JA, Wingen LU, Grif/f_iths et al. Phenotypingpipeline reveals major seedling root growth QTL in hexaploidwheat. Exp Bot 2015; 66(8):2283–92.15. Pound MP, French AP, Atkinson JA et al. RootNav: navi-gating images of complex root architectures. Plant Physiol2013; 162(4):1802–14.16. Burgess AJ, Retkute R, Pound MP et al. High-resolutionthree-dimensional structural data quantify the impact ofphotoinhibition on long-term carbon gain in wheat canopiesin the /f_ield. Plant Physiol 2015; 169(2):1192–204.17. Pound MP, French AP, Murchie EH et al. Automated recoveryof three-dimensional models of plant shoots from multiplecolor images. Plant Physiol 2014; 166(4):1688–98.18. Neumann B, Walter T, ´erich ´e et al. Phenotypic pro/f_ilingof the human genome by time-lapse microscopy reveals celldivision genes. Nature 2010; 464(7289):721–7.19. French A, Ubeda-Tomas S, Holman TJ et al. High-throughputquanti/f_ication of root growth using novel image-analysistool. Plant Physiol 2009; 150(4):1784–95.20. Wang S, Wong D, Forrest et al. International wheat genomesequencing consortium. In: Lillemo M, Mather D, Appels Ret al., eds, Characterization of Polyploid Wheat GenomicDiversity Using High-Density 90 000 Single NucleotidePolymorphism Array. Plant Biotechnol J. 2014; 12:787–96,doi:10.1111/pbi.12183.21. Broman KW, Wu H, Sen et al. R/qtl: QTL mapping in exper-imental crosses. Bioinformatics 2003; 19(7):889–90.22. Atkinson JA, Lobet G, Noll M, Meyer PE, Grif/f_iths M,Wells DM. Combining semi-automated image analysis tech-niques with machine learning algorithms to acceleratelarge scale genetic studies. GigaScience 2017 Oct 1; 6(10):1–7.doi:10.1093/gigascience/gix084.Downloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 202010 Pound et al.23. Romera-Paredes B, Torr PHS. Recurrent instance segmenta-tion. Computer Vision ECCV. 2016;312–29.24. Kilian J, Whitehead D, Horak et al. The AtGenEx-press global stress expression data set: protocols, eval-uation and model data analysis of UV-B light, droughtand cold stress responses. Plant Cell Mol Biol 2007; 50(2):347–63.25. Brenchley R, Spannagl M, Pfeifer et al. Analysis of thebread wheat genome using whole-genome shotgun se-quencing. Nature 2012; 491(7426):705–10.26. Felzenszwalb PF, Girshick RB, Mcallester et al. Ob-ject detection with discriminatively trained part-basedmodels. IEEE Trans Pattern Anal Mach Intell 2010; 32(9):1627–45.27. Harris C, Stephens M. combined corner and edge detec-tor. In: Proceedings of the Fourth Alvey Vision Conference.1988;147–51.28. Jia Y. Caffe: convolutional architecture for fast feature em-bedding. ArXiv Preprint 2014. arXiv:14085093 .29. Simonyan K, Zisserman A. Very deep convolutional net-works for large-scale image recognition. ArXiv Preprint 2014.arXiv:14091556Cs .30. Srivastava N, Hinton G, Krizhevsky et al. Dropout: sim-ple way to prevent neural networks from over/f_itting. MachLearn Res 2014; 15(1):1929–58.31. Pound MP, French AP. Deep learning for plant pheno-typing. protocols.io. 2017. https://dx.doi.org/10.17504/protocols.io.jcncive .Downloaded from https://academic.oup.com/gigascience/article-abstract/6/10/gix083/4091592 by guest on 15 July 2020