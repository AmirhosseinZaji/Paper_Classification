DeepWheat: Estimating Phenotypic Traitsfrom Crop Images with Deep LearningShubhra Aich1, Anique Josuttes2, Ilya Ovsyannikov1, Keegan Strueby2, Imran Ahmed1,Hema Sudhakar Duddu2, Curtis Pozniak2,3, Steve Shirtliffe2, and Ian Stavness∗11Dept. Computer Science,2Dept. Plant Sciences,3Crop Dev. Centre, Univ. Saskatchewan, CanadaAbstractIn this paper we investigate estimating emergence andbiomass traits from color images and elevation maps ofwheat ﬁeld plots. We employ state-of-the-art deconvo-lutional network for segmentation and convolutional archi-tectures, with residual and Inception-like layers, to estimatetraits via high dimensional nonlinear regression. Evalua-tion was performed on two different species of wheat, grownin ﬁeld plots for an experimental plant breeding study. Ourframework achieves satisfactory performance with meanand standard deviation of absolute difference of 1.05and1.40counts for emergence and 1.45and2.05for biomassestimation. Our results for counting wheat plants from ﬁeldimages are better than the accuracy reported for the simi-lar but arguably less difﬁcult, task of counting leaves fromindoor images of rosette plants. Our results for biomass es-timation, even with very small dataset, improve upon allpreviously proposed approaches in the literature.1. IntroductionMeasuring the phenotypic traits of crops, which are thedifferences in plant characteristics caused by the interactionof the plant’s genetics and the environment, is important inplant breeding research as it allows the breeders to selectcrop varieties with desirable physical characteristics, suchas high yield, resistance to stress, and ability to be eas-ily harvested. Traditionally, phenotypic measurements aremade manually in the ﬁeld, which is both labor intensiveand potentially inaccurate due to substantial sub-samplinginvolved. To overcome these drawbacks, image-based auto-mated phenotypic traits estimation is emerging as an impor-tant area of applied computer vision research with the goalof capturing more accurate information at large scale forbetter crop production.∗Corresponding Author: ian.stavness@usask.caFigure 1: Eleven leaves in an image from the standard leafcounting dataset 9] (left) and eleven wheat plants in anoutdoor image used for emergence counting in this paper.Counting plants from the right image is more challengingto due to variable number of leaves per plant and occlusion.In many crops, including wheat, emergence (the densityof plants within the ﬁeld) and biomass (the total mass ofeach plant) are important phenotypes. Emergence is impor-tant because vigorous and uniform crop stand is neededto compete for moisture, nutrients, and sunlight. Plants thatemerge late will have lower yield than the early emerg-ing ones due to the increase in competition for sunlightand essential nutrients 21]. Determining biomass in differ-ent crop varieties is important because it is correlated withyield 32] and photosynthetic activity, and is an indicatorof overall plant health 12]. These phenotypes are labourintensive and destructive to measure manually: emergencetypically requires physically touching plants in the ﬁeld todetermine which leaves belong to which plant, and biomassmeasurements are made by cutting out plants from the ﬁeldand measuring their mass. Furthermore, these phenotypesare traditionally measured on only small sub-sample ofthe experimental plot area, which can result in sampling er-ror. The combination of high importance and high measure-ment difﬁculty makes these phenotypes good candidates forimage-based phenotyping in any crop breeding programs.Counting plants is related to the well-studied problem ofcounting leaves from plant images 14,7], but much more3232018 IEEE Winter Conference on Applications of Computer Vision978-1-5386-4886-5/18/$31.00 ©2018 IEEEDOI 10.1109/WACV.2018.00042challenging. Wheat seeds are planted in close proximity,therefore, the plants grown from these seeds are highly oc-cluded by each other in the image. To illustrate the levelof difﬁculty, Figure 1shows sample image from the stan-dard leaf counting dataset 9] and another image from thedataset we are using for wheat emergence counting. Bothimages have the same label: 11 leaves in the left image,and 11 wheat plants in the right image. In the left image,the number of leaves is unambiguous despite few smallleaves in the center, which is not the case for plant count inthe right image. According to the plant science experts whogenerated the ground truth counts and who have experiencecounting plants in the ﬁeld, while counting from the images,they looked at the stems as close to the ground as possible.When stem seemed unreasonably thick, they presumedthat there were more plants behind the visible ones. Plantbases indicated by the yellow arrows in the ﬁgure are easyto count. However, in regions denoted by the green arrows,it may look like there is one plant, based on the thickness ofthe plants, amount of leaves, and age of plants, the count ofplants was estimated by the raters as more than one. Hence,both intuition and experience play role in accurate emer-gence counting, making it difﬁcult image analysis task.In this paper, we propose completely data-driven frame-works for emergence counting and biomass estimation. Wedevelop generalized architectures for phenotypic traits es-timation blending the concepts of learning sparse structurevia dense, multiscale representations 33] and residual orshortcut connections 15]. We train our models from scratchto keep our phenotypic estimation tasks independent of theother large-scale machine learning tasks pursued with verylarge models. For this reason, to efﬁciently train the data-hungry deep models with few training samples, we alsopropose novel data augmentation strategy based on ran-domized minimal region swapping of the superpixels in animage, which can be used to augment low to medium res-olution images. Also, we examine the quality of learningof the emergence counting architecture qualitatively by vi-sualizing salient regions using the class activation mapping(CAM) 35] approach. We ﬁnd that the learned network fea-tures focus on image regions that are responsible for count-ing, notably the base of each leaf-cluster, and the dense re-gions of leaves, according to the plant breeding experts whoprovided the ground truth counts.To the best of our knowledge, this is the ﬁrst work onimage-based phenotypic trait estimation of crops with deeplearning. The name DeepWheat refers to our overall sys-tem because of the ﬁrst use of deep learning in this domainand since we have used the image dataset of two speciesof wheat for the evaluation. Although we evaluate our ap-proach on wheat, our design allows the frameworks to begeneralized to other types of crops with minimal additionalmanual intervention.2. Related WorkDespite the signiﬁcance of emergence and biomass incrop breeding, little computer vision research has been doneon the automated estimation of these traits from images.Leaf counting has been studied in more detail due to stan-dardized dataset of rosette plants and previous computer vi-sion competitions 5]. Recent approaches to leaf countinghave employed convolutional neural networks to count byregression 7]. We adopt similar approach in this studyto evaluate if it extends to much more difﬁcult phenotypingtasks such as plant and biomass counting from ﬁeld images.A few studies have looked at plant density estimationin maize 31,30,29] and wheat 23,16] from RGB im-ages. All of these previous methods employ traditionalimage processing pipeline that requires hand-tuned param-eters tailored to the speciﬁc crop of interest. In the wheatstudies, the plant counting algorithm depends on the accu-rate segmentation of leaves, followed by extracting regionalproperties of the leaves as features, and then training sim-ple artiﬁcial neural network (ANN) 23] or support vectormachine (SVM) 16]. In both papers, the initial segmenta-tion of the plant foreground from the soil background is ac-complished with simple naive approaches: Otsu threshold-ing on the “b” channel of Lab image or predeﬁned RGBtransformation channel (2G−2B−2.4R). However, sim-ple threshold-based segmentations are not robust to variableillumination in different ﬁeld environments. Indeed, thesesegmentation approaches are found to give very poor resultsfor the images used in our study and are therefore not usefulbenchmarks for comparison.A number of previous studies have attempted to estimatebiomass, but most have done so from ﬁeld-based measure-ments and are therefore not applicable to image datasets. Afew studies have used aerial images as basis for biomassestimation. In 27], naive linear regression models are ﬁttedonplant height and plant coverage in aerial images. In 26],different linear and nonlinear combinations of height mea-sured with an ultrasonic sensor, leaf area index measuredwith plant canopy sensor, and vegetation indices fromcanopy reﬂectance obtained using portable spectrometerare used as the predictors and biomass is used as the re-sponse of the multiple linear regression model. The productof leaf area index and dry matter content per leaf area is re-garded as the estimation of above-ground biomass (AGB)in [25]. The authors also provide comparison against themodels developed using exponential regression, partial leastsquare regression and simple artiﬁcial neural networks. In[20], AGB was estimated from height information obtainedfrom the Digital Terrain Model (DTM) derived from LiDARdata. For each plot, simple statistical measures of height,such as mean, quadratic mean, standard deviation, skew-ness, kurtosis, and percentile of height along with heightbins at ﬁxed intervals, are used as the predictors for regres-324Figure 2: Workﬂow for emergence counting: 1) loosely segment the plant regions from RGB plot images with the segmen-tation module, 2) extract small patches containing plants via connected component analysis, 3) use counting module forindividual counts on each patch, 4) sum all the patches to get the overall emergence count for single plot.Figure 3: Manual ground-truth generated for relaxed segmentation of plants showing manually drawn contours around plantregions (red). Later, contours are ﬁlled with simple morphological hole-ﬁlling to create the binary segmentation mask.sion modeling. similar approach is taken in 19] withadditional vegetation indices extracted from hyperspectraldata. In terms of the list of predictor variables, the approachin [10] can be considered an extended version of the othertwo[19,20] with height information plus the vegetation in-dices based on both hyperspectral and unmanned aerial ve-hicle (UA V) images.3. Our ApproachIn this section, we describe the design of both emergencecount and biomass estimation frameworks in detail. Al-though both traits are estimated by convolutional networksperforming regression, the architectures and overall work-ﬂows are different.3.1. Emergence CountingFigure 2depicts the overall computational procedure forcounting crop emergence. First, we loosely segment theplant regions from the RGB plot images through the seg-mentation module described later. Next, we extract all thesegmented patches from the whole image, as indicated bythe red rectangles in Figure 2and input each patch image tothe counting module to get the individual emergence countsfor each patch. Finally, we sum up all the predicted countsfor single plot image to get the overall prediction for emer-gence count for that particular plot. In this framework, boththe segmentation and the counting modules comprise deeparchitectures which we describe below.3.1.1 SegmentationOur motivation for segmenting plot images into smallerpatches is twofold. First, due to the very high resolutionof plot images ∼2500×7500 ), it is not computationallyfeasible to do the emergence counting task on the whole im-age at once. Instead, either sequential or parallel countingover disjoint plant regions is required. Second, data-drivenapproaches, like deep learning, require many training sam-ples, whereas we have only few high-resolution plot im-ages available for that purpose. Therefore, we generate non-overlapping patches of segmented plant regions to provideus with more than hundred subsamples from each plot im-age for further training of the counting model.325InputEmergence Counting Biomass EstimationConv+ LRN ReLU (CNR) Max-Pooling Residual-CNR Inception-CNR Residual-Inception-CNR Global Avg Pooling (GAP) Fully Connected (FC)Figure 4: Emergence and biomass estimation architectures. We use 7×7receptive ﬁelds in the initial CNR block withunit stride. The number of ﬁlters after each max-pooling operation is doubled, except the ﬁrst one for emergence counting.residual -CNR is simpliﬁed version of the residual block described in 15], where we keep the number of receptive ﬁeldsconstant inside the block. We use simpliﬁed “Inception” module 34], where the number of input and output receptive ﬁeldsare the same. Inside our Inception block, we employ half of the size of ﬁlters for 3×3convolution, quarter of the input sizefor the equivalent 5×5convolution, and half of the rest for pooling and unit convolution each. For the emergence network,to visualize the representations learned by our model, we use global average pooling (GAP) 22].From the design perspective, we relax the output of thesegmentation module from exact segmentation to soft orrelaxed segmentation for several reasons. First, generatingthe exact ground-truth manually for images like the onesshown in Figure 3is more tedious and time-consumingprocess than deﬁning loose or relaxed contours aroundplants. Moreover, for deep networks, learning to count fromthe subsamples with exact vs. loose segmentations is sim-ilar since the background is uniform and so, it is unlikelythat the model would pick up distinctive features from thebackground region. This claim is also validated by CAM[35] visualizations of the network in the Experiments sec-tion that show saliency in foreground regions. In addition,the wheat leaves are thin and partly occluded; therefore, go-ing for precise segmentation could result in missing verythin or hard-to-detect regions of the plants which could de-teriorate the counting performance since the model respon-sible for counting would assume the segregated leaves asdifferent instances rather than single one.To perform soft segmentation with deep learning, we usethe SegNet architecture 8,7] rather than deconvolutionalnetworks containing fully connected (FC) layers 24] witha many more training parameters. This is because the prob-lem we are dealing with is easier than the exact segmen-tation and much simpler than general multi-class semanticsegmentation both in terms of the cardinality of the outputcategories and the nature of the domain since the diversityof the pixel intensities in single plot image is highly re-strained compared to that of natural images. Furthermore,our concern is not to get an overall-high precision segmen-tation mask, rather we are concerned with not missing plantregions in the image for the counting model afterward.3.1.2 Counting by RegressionIn this paper, we focus on different species of the cropwheat, which except the very late season, resembles mostlyto grass crops. The leaves of such plants are the most de-formable among all kinds of plants and crops, and so, setof wheat plants in an image might appear in combinatori-ally large number of variations. Thus, to successfully countthe number of plants in the image, the deep model must beable to deal with such combinatorial number of deforma-tions and resulting occlusions as much as possible.As argued in the NIN paper 22], simple stack of con-volutional layers with an over-complete set of ﬁlters fol-lowed by nonlinearity and pooling serve well when the un-derlying concepts to be learned via abstract representationare linearly separable. However, for highly nonlinear latentconcepts, replacing plain convolutional blocks with smallnetworks inside the basic architecture is already proved tobe useful in several large-scale image classiﬁcation tasks[33,34]. Hence, we take inspiration from these works,where the representation in each layer is approximated fromthe dense multi-scale feature responses learned in the pre-vious layer. Also, we incorporate the concept of residuallearning 15] in our architecture, which we experimentallyfound to be useful for faster training in case of stacked-convolutional architecture for our task.Therefore, in the design of our network as depicted inFigure 4, four different convolutional blocks are used. Ourinitial convolutional block (CNR) is simple convolutionoperation followed by local response normalization and rec-tiﬁed nonlinearity 18]. Next, we use simpliﬁed resid-ual version of the original residual block described in 15],326Figure 5: Sample RGB plot images (left) with correspond-ing DEMs (right) showing wheat plants from emergence asindividual plants (top) to full crop canopy (middle) and dur-ing the reproductive stage (bottom). DEM values (height)converted to grayscale for visualization.in the sense that the number of feature maps is constantthroughout the block from input to output. Also, for deeperlayers, where the number of receptive ﬁelds is compar-atively higher, we incorporate the “Inception” version ofCNR followed by the residual -Inception version. All thesemodules are crafted to have the same input-output capacity.Finally, for the ease of visualization of the salient regionsdetected by our model, we simply use the global averagepooling (GAP) 22] layer. We experimented with differ-entsetups of fully connected layers instead of GAP and gotslightly improved performance. However, we prefer visual-ization over those minor improvements to encourage furtherresearch based on visualization. Lastly, we have not usedany pre-trained model because unlike classiﬁcation prob-lems, the capacity of the ﬁnal layer does not scale up withthe complexity of the counting task. In addition, openingup the full network for ﬁnetuning might result in signiﬁcantoverﬁtting due to comparatively smaller datasets.3.2. Biomass EstimationFor biomass estimation, we have both 5channel ortho-mosaics Blue,Green, Red,Near-infrared, red- Edge) anddigital elevation maps (DEM). Sample RGB images areshown in Figure 5. The pixel values of the DEM ﬁles indi-cate the elevation of plants from the ground. Note that, theRGB images of the plots available for emergence counts inthe previous section and biomass estimation here are fromdifferent sources. The plot images for biomass estimationare lower resolution ∼120×480) than those used for emer-gence counting (see Section 4.1).above-ground biomass refers to the weight of all plantmaterial above the ground. We expect that there is re-lationship between biomass and height or elevation val-ues of the DEM images, but this relationship is difﬁcultto observe from simple biomass versus elevation graphs.However, representing values from each plot as differ-ent dimension in Rnspace, we have found small angles([30◦−32◦]in our dataset) between the normalized ele-Figure 6: Sample RGB plot images (left) with correspond-ing DEMs (right) showing the original image (top row)and images generated by our RMRS data augmentation pro-cedure (other rows). DEM values (height) converted tograyscale for visualization.vation vector and the biomass vector. This suggests non-linear relationship between these two quantities and we takethis as motivation for further computational analysis.Now, to apply any data-hungry models like deep learn-ing to estimate biomass from these images, one of the mainobstacles is the extremely low number of available sam-ples (∼100) for training and testing. One of the obviousways to overcome this drawback is to ﬁgure out suitabledata-augmentation strategy. In this paper, we have deviseda novel, simple and effective randomized data augmentationscheme that can be utilized to generate sufﬁciently largenumber of augmented samples from each image. The ideais based on swapping similar superpixels in the image ran-domly. We call this approach the randomized minimal re-gion swapping (RMRS) algorithm. The steps of the RMRSalgorithm are as follows:1. Get the list of Ksuperpixels from RGB to gray-converted image and sort by their mean values.2. Generate randomized list of length Nof the num-ber of random swaps needed to generate the pool of Naugmented samples from single image. The randominteger values are in the range [low,⌊K/2⌋], wherelow is the predeﬁned threshold for the minimum num-ber of swaps needed to create an augmented sample.3. For each number rin the list generated in step 2, gener-ate randomized list of length rof either even or oddsuperpixel indices in the range [1,⌊K/2⌋]and swapminimal rectangular regions between those even(odd)superpixels and their consecutive odd(even) counter-parts in the sorted list. Even-odd consideration is nec-essary to avoid unaugmentation by repeated swaps.327Figure 7: Normalized summation of the elevation for thesamples augmented from single image. The ﬁrst pointrepresents the elevation of the original sample and the rest(499) are the augmented ones. The range of normalizedelevation is in the range [∼0.99,1.0]indicating that thetotal elevation for all the samples are similar to the original.In our implementation, we use SLIC 6] as the super-pixel algorithm. Figure 6shows sample augmentation re-sults for single image along with the original one. As canbe seen, it is impossible to identify the augmented samplesas the artiﬁcial ones by looking only at RGB images, eventhough the corresponding DEMs appear to be highly dis-cretized. Hence, as part of further exploratory analysis,we plot the normalized summation of all pixel values or el-evations of each DEM ﬁle for all the augmented samplesalong with the original one. Figure 7shows this normalizedelevation plot for single image and its augmented sam-ples. As you can see, the normalized elevation varies inthe range [∼0.99,1.0], which means that although the aug-mented DEM ﬁles look different and discretized, the con-tents of the DEM pixels remain nearly constant after beingaugmented by the RMRS algorithm.In addition to increasing the number of training sam-ples, augmenting data this way has another advantage asa byproduct. We hypothesize that the spatial relationshipsamong the pixels in DEM images have little to do withthe prediction of biomass since plants can be found in al-most any region in the plot images. Therefore, the countingmodel should learn to map the pixel values from DEM im-ages into the real-valued space of biomass in an almost spa-tially invariant manner. For data augmentation by RMRSalgorithm, new samples are just different permutations ofthe original one. From the practical standpoint, the inter-pretation might be that to generate an augmented sample,we swap the plants with similar color information withinthe plot. Thus, by learning to predict from this augmenteddataset, the model may intrinsically learn spatially invari-ant mapping from color and elevation to biomass.Finally, we use similar network architecture forbiomass estimation (Figure 4). The only difference betweenthis model and the emergence count one is that the parame-ters and the placement of the computational blocks or layersare slightly modiﬁed to ﬁt the model into this problem.4. ExperimentsThis section contains the experimental details of ourwork. First, we describe the datasets used for both tasks.Next, training procedure and implementational details ofthe networks are provided. Finally, the evaluation metricsare described and the evaluation results are reported in com-parison to previous work along with the qualitative visual-ization of the salient regions.4.1. DatasetsThe dataset used for emergence count consists of 274wheat (Triticum durum) plots of 1.5m×3.7marea. High-resolution aerial images ∼2500×7500 pixels per plot)were captured for each plot by walking through the ﬁeldwith GoPro Hero camera 3] mounted on monopodwith gimbal for stabilization. Covering plots with this de-vice has the advantage of getting very high-resolution im-ages appropriate for detailed computational analysis com-pared to other remote sensing technologies.For biomass estimation, we used aerial drone images for48 wheat (Triticum aestivum) plots for two dates: June 27and July 20, 2016. The UA images have been captured us-ing MicaSense RedEdge camera 4] on DraganFly Com-mander drone 2]. The RedEdge camera includes ﬁve dif-ferent sensors, one for each band: Blue (∼465−485nm),Green (∼550−570nm),Red(∼658−678nm),NIR (∼820−860nm), and RedEdge (∼707−727nm). The out-put from these sensors was post-processed using the AgisoftPhotoscan 1] to generate an orthomosaic image and digi-tal elevation map. For each of these dates, manual groundtruth measurement of biomass have also been conducted.For manual counting, plants were cut randomly from theplots at ground-level using sickles, dried, and then weightsof those plants were noted. The dataset is randomly splitinto two equal subsets for training and testing.4.2. Training and ImplementationWe used Torch 11] as the deep learning framework. Totrain the segmentation network, we generated 0.25Msub-samples of size 224×224 from 10 high-resolution plot im-ages. The network was trained for 30epochs over this aug-mented dataset. SGD-momentum was used as the optimizerwith ﬁxed learning rate, momentum, and weight decay of0.01,0.9,and0.0001 respectively, over the training period.Both the emergence count and biomass estimation net-works were trained with similar parameter settings. Adamoptimizer 17] was used with learning rate and weight de-cay both set to 0.0001 Absolute value and Smooth L1 mea-sures 13] are used as the error criteria (loss functions) for328training emergence and biomass models, respectively. Foremergence network training, we slowed down the trainingrate later based on our observation of the training statis-tics. Training for the emergence network was conductedfor100 epochs, whereas the biomass estimation networkwas trained with different combinations of input channelsfor50epochs with the same initial parameter settings. Wewill provide the link for pre-trained models and codes in theﬁnal version of this paper.Note that the emergence count network was trained on7855 patches extracted from 37images and their slightlyaugmented versions. On the other hand, the biomass net-work was trained with about 0.15Maugmented trainingsamples generated by the RMRS algorithm from 48plotsamples. Codes, pre-trained models, and datasets are pub-licly available here.14.3. EvaluationHere, we provide three evaluations of our approach.First, we assess the performance of our segmentation net-work for generating relaxed binary segmentations. Next,both emergence count and biomass estimation networks areevaluated based on the metrics listed in Equation 1below.Among these metrics, we take MAD and SDAD from theleaf counting benchmark 7]. The other is simply variantof these measures. In addition, we provide CAM visualiza-tion for the emergence counting model.Precision =True PositiveTrue Positive +False Positive85.59Recall =True PositiveTrue Positive +False Negative83.76Accuracy =True Positive +True NegativeAll93.76Table 1: Binary segmentation resultsEmergence evaluation: Precision, recall, and accuracyare measured to evaluate the segmentation network (Table1). Results for precision ∼86% and recall ∼84% are asomewhat low because the ground truth segmentations arenot precise, but loosely deﬁned contours covering all theplant regions in the images. To justify our outputs, we havevisually checked almost all the test segmentation results andﬁnd almost no plant regions undetected by the network.⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩ai,ti=actual and target counts for ithsampleN=Number of samples%Difference(%D) =/summationtexti|ai−ti|I[ai−ti/negationslash=0]/summationtextitiMean Absolute Difference (MAD) =/summationtexti|ai−ti|NStd Absolute Difference (SDAD) =/radicalBig/summationtexti(|ai−ti|−MAD )2N−1(1)1https://github.com/p2irc/deepwheat WACV-2018Problem MAD SDAD %DPrev. Leaf Counting 7] 1.62 2.30 -Plain Architecture 1.13 1.42 27.04Inception Architecture 1.08 1.38 25.78Our Emergence Counting 1.05 1.40 25.08Table 2: Evaluation metrics for the emergence count modelTable 2lists the evaluation metrics for our emergencecounting network. As stated in the introduction, we did notﬁnd appropriate literature to benchmark our approach. Theclosest approach is the one used for Arabidopsis and To-bacco leaf counting problem 7]. We achieve %D of 25%and MAD and SDAD of1.05and1.40which is more ac-curate than previously reported results for one of the bestleaf counting system currently available. These results arenotable because counting wheat plants with thin, overlap-ping leaves from outdoor images is substantially more dif-ﬁcult than counting leaves from indoor images of rosetteplants (as discussed in the Introduction and illustrated inFigure 1). We have also included the results for the corre-sponding plain and Inception-only version to justify the ad-ditional complexity of our ﬁnal model. The plain networkwas trained for twice the number of epochs than others.The salient regions detected by our counting model forsample RGB images are shown as heatmaps, generated byCAM 35], in Figure 8. Although in the original paper,CAM is used to visualize class-speciﬁc mapping of thesalient regions, for our counting task, it can also be used forvisualizing the regions responsible for making the counts.As already discussed, the bases of leaf-clusters are the mostsigniﬁcant parts for successful counts followed by dense re-gions of overlapping leaves. The sample heatmaps also fol-low this counting strategy. In the heatmaps, the bases ofthe plants are marked with red (highest saliency) followedby the leaves with yellow, which clearly indicates that ourmodel is capable of identifying the correct regions in theimages responsible for counting. Nonetheless, our percent-age deviation is bit high because of the inherent difﬁcultyof counting the plants due to severe occlusion and largeleaf deformations. To enable CAM visualization, we cutout additional fully connected layers, which had provideda slight performance boost, but the resulting visualizationprovides more valuable insight into the learning process forplant counting.Biomass evaluation: Table 3contains the same metricsasin Equation 1for biomass models trained with differentinput channel combinations. Here, H,R,G,B,N, and Estand for DEM ,Red ,Green ,Blue ,NIR and RedEdgechannels, respectively.As can be seen, the model trained with only H(DEM )as input gives %D of∼26% which is ∼4% and∼2%lower than the model trained with RGBH and all the chan-329Figure 8: Sample RGB images (left), their CAM 35] visu-alizations (middle), and superimposed images (right). Notethat, RGB images are padded by black to maintain con-stant size of 224×224. Red and blue indicate the mostand the least signiﬁcant regions responsible for emergencecounting. As you can see, the plant bases are detected asthe most salient regions (red) which the experts also use forcounting followed by the leaves (yellow).nels. At this point, it is unclear whether the deep learningmodel takes care of any of the RGB texture in the biomassimage. Intuitively, although color information or greennessof the RGB image might be important, the texture informa-tion is not that signiﬁcant for biomass estimation. However,there is high variance in the color information under dif-ferent weather conditions. For instance, if the weather isovercast, crops will appear dark-green, for sunny weather,it will be yellowish-green, and so on. Another critical issueis that after augmenting data using RMRS algorithm, albeitthe very local texture property and the total energy of theimages are more or less preserved, semi-local texture prop-erty is destroyed. We are not sure whether this lack of semi-local texture causes the network trained with RGBE inputto perform poorer than the one with only DEM input. Thisissue can only be explored further if sufﬁcient raw trainingsamples are available in future.On the other hand, the fact that the model works bet-ter when two extra non-visible wavelengths, such as, NIRandRedEdge are provided along with RGB is consistentwith the plant science literature 28] where vegetation in-dices extracted from hyperspectral and visible wavelengthdata are used as strong indicators of photosynthetic mea-surements of plants. However, the utility of hyperspectraldata for biomass estimation is still an open question.In Table 3, we provide comparison against the re-cent literature. We implemented the methods described in[20,19,10] on our data for comparison. These three papersreported the effect of different feature combinations fromthe set of simple statistical features based on height and dif-Method MAD SDAD %DH1+MARS[ 20] 1.66 2.03 29.61H2+PLS[ 19] 3.86 2.72 68.92H2+MARS[ 20,19] 1.74 2.07 30.96OH 3+MLR[ 10] 1.67 1.63 29.67Ours RGBH 1.67 2.05 29.75Ours RGBNEH 1.53 1.62 27.38Ours H) 1.45 2.05 25.88Table 3: Comparison of biomass estimation metrics to othermethods and with different input channels H≡DEM ,Red,Green, Blue,Near-infrared, and red Edge)ferent vegetation indices as the predictor variables for theirregression models. In this table, we use the combination offeatures that performed best on our dataset. H1,H2, and H3indicates slightly different variations statistical height fea-tures and OH 3stands for the combination of H3and Opti-mized Soil-Adjusted egetation Index (OSA VI) Also, MARS(Multivariate Adaptive Regression Splines) ,PLS (PartialLeast Squares) and MLR (Multivariate Linear Regression)are different linear and nonlinear regression algorithms. Ascan be seen, even with such tiny amount of original train-ing data, the best performance of our deep model (trainedwith DEM (H))i s∼4% better than the recent nonlinearregression model for biomass.AcknowledgmentThis research was undertaken thanks in part to fund-ing from the Canada First Research Excellence Fund andthe Natural Sciences and Engineering Research Council(NSERC) of Canada. We also thank Seungbum Ryu andthe USask ﬁeld crew for providing biomass data.5. Conclusion and Future WorkIn this paper, we have developed three different deeplearning models for segmenting plant regions, countingplants, and estimating biomass from aerial ﬁeld images.Our results show better biomass estimation accuracy thanprevious methods and better accuracy for outdoor emer-gence counting as compared to previous studies of indoorleaf counting. Although we have only evaluated our modelon particular species of wheat, we expect that our designmethodology allows for generalization of these models toother types of crops with minimal changes. As future work,we plan to evaluate our networks with other crops that havedifferent plant morphologies, such as pulses and oilseeds.We also plan to further investigate if estimation accuracy forthese phenotypic traits can be improved with larger datasetsin subsequent growing seasons, as well as the use of digi-tal elevation maps together with non-visible wavelengths oflight as input for biomass estimation.330References[1] Agisoft. http://www.agisoft.com/. 6[2] DraganFly. http://www.draganﬂy.com/. 6[3] GoPro. https://gopro.com/. 6[4] Micasense-Rededge Camera.https://www.micasense.com/rededge/. 6[5] Leaf Counting Challenge.https://www.plant-phenotyping.org/CVPPP2017-challenge,2017. 2[6] R. Achanta, A. Shaji, K. Smith, A. Lucchi, . Fua, andS. ¨usstrunk. SLIC superpixels compared to state-of-the-artsuperpixel methods. IEEE Transactions on Pattern Analysisand Machine Intelligence 34(11):2274–2282, Nov 2012. 6[7] S. Aich and I. Stavness. Leaf counting with deepconvolutional and deconvolutional networks. CoRR ,abs/1708.07570, 2017. 1,2,4,7[8] . Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: Adeep convolutional encoder-decoder architecture for scenesegmentation. IEEE Transactions on Pattern Analysis andMachine Intelligence PP(99):1–1, 2017. 4[9] J. Bell and H. M. Dee. Aberystwyth leaf evaluation dataset,Nov. 2016. 1,2[10] J. Bendig, K. u, H. Aasen, A. Bolten, S. Bennertz,J. Broscheit, M. L. Gnyp, and G. Bareth. Combining uav-based plant height from crop surface models, visible, andnear infrared vegetation indices for biomass monitoring inbarley. International Journal of Applied Earth Observationand Geoinformation 39:79 87, 2015. 3,8[11] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: Amatlab-like environment for machine learning. In BigLearn,NIPS Workshop 2011. 6[12] J. Dai, B. Bean, B. Brown, W. Bruening, J. Edwards,M. Flowers, R. Karow, C. Lee, G. Morgan, M. Ottman,J. Ransom, and J. Wiersma. Harvest index and straw yield ofﬁve classes of wheat. Biomass and Bioenergy 85:223 227,2016. 1[13] R. Girshick. Fast r-cnn. In 2015 IEEE International Con-ference on Computer Vision (ICCV) pages 1440–1448, Dec2015. 6[14] M. . Giuffrida, M. Minervini, and S. Tsaftaris. Learning tocount leaves in rosette plants. In S. A. Tsaftaris, H. Scharr,and T. Pridmore, editors, Proceedings of the Computer Vi-sion Problems in Plant Phenotyping (CVPPP) pages 1.1–1.13. BMV Press, September 2015. 1[15] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learningfor image recognition. In 2016 IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR) pages 770–778, June 2016. 2,4[16] X. Jin, S. Liu, F. Baret, M. Hemerl, and A. Comar. Esti-mates of plant density of wheat crops at emergence from verylow altitude uav imagery. Remote Sensing of Environment ,198:105 114, 2017. 2[17] D. . Kingma and J. Ba. Adam: method for stochasticoptimization. CoRR abs/1412.6980, 2014. 6[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenetclassiﬁcation with deep convolutional neural networks. InF. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,editors, Advances in Neural Information Processing Systems25, pages 1097–1105. Curran Associates, Inc., 2012. 4[19] G. . Laurin, Q. Chen, J. A. Lindsell, D. A. Coomes, F. D.Frate, L. Guerriero, F. Pirotti, and R. alentini. Aboveground biomass estimation in an african tropical forest withlidar and hyperspectral data. ISPRS Journal of Photogram-metry and Remote Sensing 89:49 58, 2014. 3,8[20] G. . Laurin, N. Puletti, Q. Chen, . Corona, D. Papale,and R. alentini. Above ground biomass and tree speciesrichness estimation with airborne lidar in tropical ghanaforests. International Journal of Applied Earth Observationand Geoinformation 52:371 379, 2016. 2,3,8[21] K. Lawles, W. Raun, K. Desta, and K. Freeman. Effect ofdelayed emergence on corn grain yields. Journal of PlantNutrition 35(3):480–496, 2012. 1[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR ,abs/1312.4400, 2013. 4,5[23] S. Liu, F. Baret, B. Andrieu, . Burger, and M. Hemmerl.Estimation of wheat plant density at early stages using highresolution imagery. Frontiers in Plant Science 8:739, 2017.2[24] H. Noh, S. Hong, and B. Han. Learning deconvolution net-work for semantic segmentation. In IEEE International Con-ference on Computer Vision (ICCV) pages 1520–1528, Dec2015. 4[25] X. Quan, B. He, M. Yebra, C. Yin, Z. Liao, X. Zhang, andX. Li. radiative transfer model-based method for the es-timation of grassland aboveground biomass. InternationalJournal of Applied Earth Observation and Geoinformation ,54:159 168, 2017. 2[26] B. Reddersen, T. Fricke, and M. Wachendorf. multi-sensor approach for predicting biomass of extensively man-aged grassland. Computers and Electronics in Agriculture ,109:247 260, 2014. 2[27] M. Schirrmann, A. Hamdorf, A. Garz, A. Ustyuzhanin, andK.-H. Dammer. Estimating wheat biomass by combining im-age clustering with crop height. Computers and Electronicsin Agriculture 121:374 384, 2016. 2[28] . J. SELLERS. Canopy reﬂectance, photosynthesis andtranspiration. International Journal of Remote Sensing ,6(8):1335–1372, 1985. 8[29] D. Shrestha and B. Steward. Shape and size analysis of cornplant canopies for plant population and spacing sensing. Ap-plied Eng. in Agric 21(2):295–303, 2005. 2[30] D. Shrestha, B. Steward, and S. Birrell. Video processing forearly stage maize plant detection. Biosystems engineering ,89(2):119–129, 2004. 2[31] D. S. Shrestha and B. L. Steward. Automatic corn plant pop-ulation measurement using machine vision, paper number011067. In 2001 ASAE Annual Meeting American Societyof Agricultural and Biological Engineers, 2001. 2[32] J. M. Soriano, M. Malosetti, M. Rosell, M. E. Sorrells, andC. Royo. Dissecting the old mediterranean durum wheat ge-netic architecture for phenology, biomass and yield forma-tion by association mapping and qtl meta-analysis. PLOSONE 12(5):1–19, 05 2017. 1331[33] C. Szegedy, W. Liu, . Jia, . Sermanet, S. Reed,D. Anguelov, D. Erhan, . anhoucke, and A. Rabinovich.Going deeper with convolutions. In 2015 IEEE Conferenceon Computer Vision and Pattern Recognition (CVPR) pages1–9, June 2015. 2,4[34] C. Szegedy, . anhoucke, S. Ioffe, J. Shlens, and Z. Wojna.Rethinking the inception architecture for computer vision.In2016 IEEE Conference on Computer Vision and PatternRecognition (CVPR) pages 2818–2826, June 2016. 4[35] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Tor-ralba. Learning deep features for discriminative localization.In2016 IEEE Conference on Computer Vision and PatternRecognition (CVPR) pages 2921–2929, June 2016. 2,4,7,8332