Received August 31, 2019, accepted October 4, 2019, date of publication October 11, 2019, date of current version November 18, 2019.Digital Object Identifier 10.1 109/ACCESS.2019.2946589Image Recognition Method Based on anImproved Convolutional Neural Networkto Detect Impurities in WheatYIN SHEN1,3, YANXIN YIN2, CHUNJIANG ZHAO1,3, BIN LI2,3, JUN WANG4, GUANGLIN LI1,AND ZIQIANG ZHANG51College of Engineering and Technology, Southwest University, Chongqing 400715, China2National Research Center of Intelligent Equipment for Agriculture, Beijing 100097, China3Beijing Research Center for Information Technology in Agriculture, Beijing 100097, China4School of Electronics and Information Engineering, Sichuan University, Chengdu 610064, China5Information Engineering College, Capital Normal University, Beijing 100048, ChinaCorresponding authors: Chunjiang Zhao (zhaocj@nercita.org.cn) and Bin Li (lib@nercita.org.cn)This work was supported by the National Key Research and Development Project of China under Grant 2016YFD0702000.ABSTRACT Impurities in wheat seriously affect wheat quality and food security. They are mainly producedduring the operational process of combine harvesters. To solve the recognition and classiﬁcation problemsassociated with impurities in wheat, recognition method using an improved convolutional neural networkis proposed in this article. labeled dataset of normal wheat and ﬁve impurities is constructed, usingwhich the Wiener ﬁltering algorithm and the multi-scale Retinex enhancement algorithm are employed forimage preprocessing. Based on network research using Inception_v3, improvement and optimization areundertaken before designing the WheNet convolutional neural network, which is intended for automaticrecognition of wheat images. Under the same conditions, comparative experiments using the WheNet,ResNet_101 and Inception_v3 networks are conducted. Indexes such as receiver operating characteristic,area under curve (AUC), and recall rate are adopted to evaluate the experimental outcomes. Experimentalresults indicate that the WheNet network achieved the most efﬁcient results. It also shows shorter trainingtime, and its recognition accuracies for Top_1 and Top_5 of the test set are 98.59% and 99.98%, respectively.The mean values of both the AUC and recall rate of the network on the recognition of various images ofimpurities are higher than those of the ResNet_101 and Inception_v3 networks. Consequently, the WheNetnetwork can be useful tool in recognizing impurities in wheat. Furthermore, this method can be used todetect impurities in other ﬁelds.INDEX TERMS Wheat images, impurity recognition, convolution neural network, WheNet network.I. INTRODUCTIONImpurities in wheat are not only an important factor affectingthe total grain output, but also an important index for mea-suring the harvesting quality of combine harvesters. As theseimpurities can seriously reduce the grain quality, recognitionand testing of impurities in wheat is crucial.Although researchers across the world have conductedrecognition and monitoring of impurities in rice and maizeby adopting manual means, the efﬁciencies are observed tobe very low, the main reasons being that manual recognitionThe associate editor coordinating the review of this manuscript andapproving it for publication was Hongjun Su.is time-consuming, needs considerable efforts, and has cer-tain subjectivity, which is prone to mistakes. In particu-lar, when the backgrounds of images are complicated andthe targets themselves undergo dynamic changes, the dif-ﬁculty in recognizing targets increases. Some researchershave identiﬁed impurities in rice and in cotton usingimage processing, hyperspectral imaging, and machine visiontechniques [1]–[3], which have helped improve the accu-racy and efﬁciency of impurity recognition. For example,Wallays et al. [4] conducted an online test of impurities inrice when using combine harvesters by adopting hyperspec-tral imaging, and realized classiﬁcation of wheat and itsimpurities in the images. Mahirah et al. [5] adopted visual162206This work is licensed under Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/VOLUME 7, 2019Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural Networksystem containing dual light sources to test for impuritiesand broken rice grains. However, their system is complex.Shahin and Symons [6] used high light spectroscopy to detectimpurities in Canadian wheat samples with an accuracy rateof 90%. Singh et al. [7] used information from 700-1100 nmhyperspectral band to classify normal wheat and mycelium-damaged wheat, and achieved good results. In most cases,traditional technology is characterized by acquiring featuresof targets such as color, shape, and graining before classifyingthe images through artiﬁcial neural network and supportvector machine [8], [9]. Most of these methods require theproduction of samples, and the system structure is complex.Moreover, the methods are time-consuming, strenuous, andhave poor recognition ability.Currently, given their importance in the ﬁeld of deeplearning [10]–[13], convolutional neural networks (CNNs)have made great progress in aspects such as image recog-nition [14], [15], pest recognition [16], and face recognition[17], [18]. CNNs are being applied increasingly to agri-cultural production. For example, Krizhevshy et al. [19]achieved breakthrough in the recognition of the dataset ofImageNet using CNN. Yuan et al. [20] recognized the typesof chrysanthemums using CNN and succeeded in construct-ing recognition system for the ﬂower. Amara et al. [21]realized the classiﬁcation of blades of banana trees infectedwith diseases using small samples and by adopting CNN;however, the method provided poor image recognition per-formance for large samples. Using CNN, Yang et al. [22]conducted image recognition of agricultural machinery, andrealized the positioning and recognition of ﬁve types ofcommon agricultural equipment. Using GoogLeNet CNN,Huang et al. [23] successfully realized accurate testing ofseasonal febrile diseases of rice ears. Mohanty et al. [24]conducted the recognition of many images of crop diseasesusing CNN, with accuracies reaching as high as 99.35%.Alessandro et al. [25] made use of the SLIC superpixelalgorithm to generate superpixel images and conduct recogni-tion processing using CNN, and the resulting classiﬁcationaccuracy exceeded 98%.These studies characterize CNNs as being able to inde-pendently learn images and provide excellent object recog-nition performance. Therefore, CNNs have become one ofthe most important methods of image classiﬁcation andrecognition. At present, CNNs are being studied with regardto damage caused by pests, fruit recognition, and plantrecognition [26]–[29]. However, only few studies havefocused on the recognition of grain impurities, especiallywith regard to impurities in wheat. This is because the wheatimpurity image distribution structure is complex. Althoughexisting CNNs can achieve image feature extraction, the net-work structure is complex, the number of parameters is large,the training time is long, and the recognition accuracy cannotbe guaranteed [34], [35]. In addition, image recognition tech-nology based on deep learning requires training dataset thatcontains large number of annotated images. Current open-source datasets, such as cifar10, cifar100, and ImageNet,provide data regarding general objects, such as pests, fruit,faces, and ﬂowers. However, none of the open-source datasetsrecognize impurities in wheat. It is therefore necessary toconstruct an open-source dataset for recognizing impurities inwheat, and to perform studies on wheat impurities recognitionusing CNNs. To the best of our knowledge, our study is theﬁrst to recognize impurities in wheat using CNN.In this study, we collect many wheat samples, and beginby preprocessing the wheat images to reduce the inﬂuence ofmotion, shading, and differences in light before classifyingand labeling them. Second, we propose the method for rec-ognizing impurities based on CNN, which is an improvedInception_v3 network that analyzes the image characteris-tics of the impurities in wheat. The designed WheNet CNNadopts the optimized Adam algorithm to raise the efﬁciencyof model training and optimizes the structure and parametersof the CNN so as to increase recognition accuracy. Finally, theresults of comparative experiment between the ResNet_101network and the Inception_v3 network show that the pro-posed WheNet network is advantageous with regard to train-ing time, recognition accuracy, area under curve (AUC), andrecall rate. The feasibility and effectiveness of the WheNetnetwork were thus veriﬁed. The proposed network can effec-tively overcome the drawbacks of traditional methods, andmakes the recognition process more rapid and accurate.II. MATERIALS AND RELATED WORKA. IMAGE ACQUISITION AND ARRANGEMENTThe images were acquired in May 2019 from the Xiaotang-shan Modern Agricultural Science and Technology Demon-stration Park, Beijing, China (coordinates: 4018’ and11646’ E). This area has mean annual precipitationof 575.5 mm and mean annual temperature of 11.9C. Notethat the wheat samples and its impurities are more representa-tive in natural environment. total of 6,000 images of wheatin the grain elevators of the combine harvesters were acquiredduring their operation using MindVision color industrialcamera accompanied with 16-mm focal length lens. Theimage resolution was set as 2,000,000 pixels.In accordance with the image characteristics of the wheatwith impurities, the images were divided into six categories,namely insects, stalks, grass, awns of wheat, spikelet, andnormal wheat, as shown in Fig. 1. Due to the limited numberof acquired image samples and the need to reduce the trainingtime, samples of the training set were enlarged. By revolvingthe samples at seven angles—0, 45, 90, 135, 180, 225,and 270—as well as the horizontal and vertical ﬂips in vari-ous directions, the original dataset for training was expandedto six times as large as the original; the ﬁnal training datasetincluded total of 36,000 images of wheat.In addition, the dataset was divided into two mutuallyindependent sets, one for training and the other for validation.The sample set was composed of number of images fromeach category and 70% of the samples were randomly chosenfrom the sample set for training, and the remaining 30% wereVOLUME 7, 2019 16220701Amirhossein ZajiY. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkFIGURE 1. Examples of images from the partial dataset.TABLE 1. Quantity distribution of wheat images.used for validation. The training set, which was used to trainthe model, comprised total of 25,200 images. The validationset, which validated the model accuracy, comprised totalof 10,800 images. The distribution of the wheat images islisted in Table 1.B. IMAGE PREPROCESSINGAs the existence of lighting and shadows in the images of thewheat complicates image recognition, preprocessing of theoriginal color images was imperative. The common methodsof image preprocessing include image enhancement, deblur-ring, and binarization [30].1) ELIMINATION OF MOTION BLURImages to be inputted in the CNN do not require complicatedpreprocessing. As long as the original images are inputted,the characteristics can be obtained through backpropaga-tion (BP) and layer-by-layer learning. In general, the imagesare adjusted for light, contrast, saturation, noise, and blur.As motion blurring existed in considerable number ofthe wheat images acquired for this experiment, recognizingimpurities from the blurred images was critical. Motion blur-ring refers to image blurring caused by the relative motionbetween the camera and the scene to be shot. In this study,the Wiener ﬁltering algorithm [31] is adopted to preprocessthe wheat images with motion blurring. For example, the orig-inal images of grass (Fig. 2a), spikelet (Fig. 2c), and insect(Fig. 2e), which contained motion blurring, were processed,resulting in clear images (Fig. 2b, 2d, and 2f).2) IMAGE BINARIZATIONThe purpose of image binarization is to differentiate theforeground of image from its background. In this article,Multi-Scale Retinex with Color Restoration (MSRCR) isadopted for this purpose. In this method, ﬁrst, gray valueis assumed before dividing the image into the foregroundand background. Then, calculation and comparison are con-ducted through the Otsu algorithm to determine the optimal162208 VOLUME 7, 2019Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkFIGURE 2. Comparison between the images of the impurities in wheat before and after theelimination of motion blurring.segmentation threshold before ensuring that the maxi-mum difference exists between the background and fore-ground [32]. Fig. 3b is the acquired result of binarizationby adopting the Otsu algorithm directly for Fig. 3a. Theoutlines between the grains in the original image were notclear enough, and thus, we enhanced the original imagebefore conducting binary segmentation. The result obtainedafter processing the original wheat images in Fig. 3a withthe MSRCR algorithm is shown in Fig. 3c. The enhance-ment by the MSRCR algorithm increased the image contrast.As shown in Fig. 3d, the binarization image obtained byadopting the Otsu algorithm after image enhancement by theMSRCR algorithm shows more complete foreground incomparison with that of the original image in Fig. 3b. More-over, after undergoing suitable binary segmentation, cleareroutlines among the grains were evident.C. LABELING OF THE IMAGE DATASETThe CNN algorithm requires labeling many datasets. In thisresearch, TensorFlow, deep learning platform devised byGoogle, was adopted to label the datasets of the wheatimages. First, the sorted training set and validation set werestored under two folders, with each folder containing thesix categories of images seen in Table 1, and the labels ofthe corresponding folders were set up. Then, the built-infunctions of TensorFlow were used to convert each originalimage into binay data, wherein the ﬁrst byte indicated thecategory of the images and the remaining 64 ⇥64⇥3 bytesdenoted the basic information of the images. Finally, weseparately converted the training and validation sets into twoindependent binary documents before obtaining the datasetof the wheat impurity images whose labeling had been com-pleted. In addition, the data reading mode of TensorFlowadopts TFRecord format. TensorFlow and TFRecord havesome supporting functions. TFRecord is binary data storageformat, which can speed up the processing of image data.Using TFRecord, the original image data is actually readﬁrst, then converted to the TFRecord format, and ﬁnallystored on the hard disk. The data processing process isshown in Fig. 4.III. IMAGE RECOGNITION ALGORITHMOF IMPURITIES IN WHEATA. NETWORK STRUCTURE OF INCEPTION_V3Compared with other CNNs such as AlexNet [19],VGGNet [14], and ResNet [10], the Inception_v3 net-work [24] has been successfully applied in many ﬁelds dueto its simplicity and excellent classiﬁcation performance. TheInception module designed in the Inception_v3 network hasraised the utilization ratio of parameters. As shown in Fig. 5,the Inception module was designed as the ⇥1 and ⇥3branches of the convolution kernel, and the structural char-acteristics of impurities at different positions were extractedand learned [33]. By receiving the input from the precedinglayer, the Inception module realizes the fusion of multi-scale characteristics. With convolution kernels of differentsizes added to the same layer, their basic structure includesfour branches, each being applied via ⇥1 convolution.VOLUME 7, 2019 162209Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkFIGURE 3. Image binarization.FIGURE 4. The wheat data set is converted into TFRecord format.The addition of the ⇥1 convolution kernel in front of the3⇥3 convolution kernels attains dimensionality reductionthat can reduce the amount of calculation [12]. With theInception module connecting the nodes through ⇥1 and3⇥3 convolution kernels in the four branches as well as the3⇥3 maximum pooling layer, an efﬁcient sparsity structureconforming to the Hebbian theory, an unsupervised learningmethod and the simplest neuron learning rule, has beenestablished.B. STRUCTURE OF THE WHENET CNNThe complex distribution of the wheat impurity imagesresulted in large amount of image data. Although maturedeep CNNs can extract all the features of images, the complexstructure and long training time can lower recognition accu-racy slightly [34], [35]. Based on the Inception_v3 network,improvement and optimization were undertaken, and theWheNet CNN was designed. The improved network structureis shown in Fig. 6, and the detailed design of the networkparameters is listed in Table 2. As type of multi-layer neuralnetwork, CNN includes multi-layer convolution and pool-ing layers, which are composed of the input layer, convolutionlayer, pooling layer, and output layer [36]. WheNet adoptsthe BP algorithm in terms of the cost function between theminimum training result and the true value, as represented inFormula 1):L=1|X|X|X|i=1ln (p⇣yi|xi⌘) (1)In Formula 1),|X|indicates the size of the training set, whileyiandxidenote the ithtraining sample and its correspondingcategory label, respectively.To extract the characteristics of the wheat images as accu-rately as possible, the preprocessed images of the sampleset were inputted into the WheNet network to receive train-ing before generating the image recognition network [19].To obtain better network parameters of the impurity images,we adjusted the number of convolutional layers and the widthof the WheNet network. We considered 64 ⇥64 image asan example. The network included four convolutional layers(C1, C2, C3, and C4), two pooling layers (P1 and P2), threeInception modules (A, B, and C), two fully connected layers(F1 and F2), as well as one classiﬁcation layer. The ﬁrst twoconvolutional layers were connected with pooling layer, andthe last fully connected layer used the Softmax function forclassiﬁcation. The ﬁnal number of categories was the sameas the number of output categories of the classiﬁcation layer.The description of the kernel layers in the network appearsbelow.1) INPUT LAYERAs the color and pattern of the wheat grains are importantcharacteristics for identity recognition, the ﬁltered negativeand positive samples were converted to 64 ⇥64⇥3matrix through interpolation calculation. In addition, the pos-itive and negative samples were labeled as ‘‘2’’ and ‘‘1,’’162210 VOLUME 7, 2019Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkFIGURE 5. Inception module.FIGURE 6. CNN structure.respectively, and used as the input data for the convolutionalnetwork training.2) CONVOLUTIONAL LAYERThe convolutional layer involves convolving convolutionalkernel that can be learned and the preceding layer charac-teristic pattern that has undergone batch processing. As eachconvolutional kernel has its unique extracted characteristics,the larger the number of the convolutional kernels in con-volutional layer, the greater the number of the characteristicsextracted. The output characteristic pattern is obtained aftercombining many characteristic patterns that have been con-volved and have undergone offset processing via an activationfunction.xlj=fmax(Xi2Mjxl1i⇥klij+blj,0) (2)In Formula 2),lis the sequence number of convolutionallayer, xljrefers to the output of the jthneuron of the convo-lutional layer l,xl1iindicates the output of the ithneuronof the l-1stlayer, Mjdenotes the set of input characteristicpatterns, klijstands for convolutional kernels, and bljrepresentsthe offset.3) POOLING LAYERSTo maintain the invariance of the revolution, translation, andelasticity of the original data, pooling technology is used tointegrate the characteristic points in the small neighborhoodand obtain new characteristics.xlj=f(↵ljpooling (xl1j)+blj) (3)In Formula 3),↵is the weight coefﬁcient, bljdenotes theoffset, and xljindicates the vector of xl1jafter its maximumVOLUME 7, 2019 162211Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkTABLE 2. Design of CNN parameters.value is pooled. The pooled kernel is ⇥3, and the steplength is ⇥2.The common pooling modes include mean-pooling andmax-pooling As maximum pooling can reduce errorsand retain more image information, max-pooling wasadopted in this experiment. If xjdenotes the poolinglayer, the computational process of xjcan be described inFormula 4).xj=max poolingxj1(4)4) FULLY CONNECTED LAYERThere can be several fully connected layers, which in factare the hidden layers of ordinary neural network layers. Eachneuron in fully connected layer is fully connected with allneurons in its preceding layer. The neural nodes in the samelayer have no connection. Through weights on connectinglines, the neural nodes in each layer conduct forward propa-gation and calculation before obtaining the input of the neuralnodes in the next layer.After convolutional and pooling processing, series ofcharacteristic patterns are outputted via images, while theinput received in the next layer is the output of the preced-ing layer. Therefore, the pixels of the characteristic patternsare fetched in sequence, namely the output characteristicpatterns x1,,x3..., xa. After the weighted summation ofthe vectors and the responses of the activation functions,the output formula of the fully connected layers is elicitedas follows :xl=↵lxl1+bl(5)In Formula 5),xlis the output vector of the fully connectedlayers, xl1is the input vector, ↵lis weight coefﬁcient, andblis the offset item.C. INCREASING THE EFFICIENCYOF MODEL TRAINING1) LEARNING RATE AND ADAM OPTIMIZATIONALGORITHMLearning rate is very important parameter in the deeplearning network. In this study, neural network training wascarried out through the exponential decay learning rate [37],which not only makes the network rapidly approach com-paratively excellent solution in the earlier stage of training,but also ensures that the network is unlikely to undergo muchﬂuctuation in the later stage of training, thus being closer tolocal optimum. The formula of learning rate is as follows :decayed _learning _rate=learning _rate⇥decay _rateglobal _stepdecay _steps(6)In Formula 6),learning_rate is the initial learning rate,global_step denotes the global step of the attenuation calcu-lation, decay_rate is the decay rate, and decay_steps refers tothe decay speed (i.e., how many rounds are needed to iterateall the training data). The training data in this case amount to100,000 images, and each batch_size is 64.In this article, the gradient descent algorithm and Adamoptimization algorithm [38] were adopted to conduct thenetwork training. The Adam algorithm follows the methodof gradient descent. The learning step length of the iterationparameters for each time has ﬁxed range, which dynami-cally adjusts the learning rate of each parameter by makinguse of the ﬁrst- and second-order moment estimations of thegradient [39]. The formulae of this algorithm are given below.t=t+1 (7)gt=r✓ft(✓t1) (8)In Formulae 7) and 8),ft(✓) represents the gradient of ✓, thatis, the partial derivative vector of ftfrom ✓under the time step162212 VOLUME 7, 2019Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkTABLE 3. WheNet hyperparameters.t, and the initial parameters t=0 and ✓0=0.mt=↵mt1+(1↵)ht (9)nt=nt1+(1)h2t (10)The biased ﬁrst- and second-order moments were estimatedin Formula (9) and (10), while htis the current gradient,where ↵andrepresent the exponential decay rate and initialparameter of the moment estimation p0=0,q0=0),respectively.bmt=mt1↵t(11)bnt=nt1t(12)In Formulae 11) and 12),bmtandbntare the corrections of mtandnt, the Adam algorithm can make dynamic adjustmentaccording to the gradient, which imposes dynamic restrainton the learning rate.1✓t=bmt·pbnt+"(13)The value of ✓was updated in Formula 13).is equivalent tothe initial learning rate, and "is small constant with stablevalue. Table shows the values of the above parameters forthe training of WheNet in this study.2) LOCAL RESPONSE NORMALIZATIONLocal response normalization (LRN) plays role in conduct-ing normalized processing for each element in accordancewith the given coefﬁcients so as to improve the accuracyof the classiﬁcation on the part of the neural network [30],as shown in Formula 14):bix,y=aix,y/k+↵Xmin(N1,i+n/2)j=max(0,in/2)⇣aix,y⌘2(14)In Formula 14),aix,yrepresents the output of the ithconvo-lutional kernel at position x, y),bix,yrefers to the outputafter local response normalization, Nindicates the numberof convolutional kernels, and k,↵,n, and are constants.3) LOSS FUNCTIONSThe objective of CNN training is to minimize the loss func-tion. The computational formula of loss function isL(W,b)=NXi=1CXJ=1I(byi=j)logpji(15)In Formula 15),Wdenotes weight, bstands for the offsetitems, byirefers to the expected value of the ithtraining sample,jis the category of image samples, and Irepresents theindicator function. When byi=j, the numerical value of iis 1; otherwise, it is 0. pjiis the predicted probability of theithtraining sample in the jthcategory. Cis the number of thecategories of the training samples. Nis the total number oftraining samples. The iteration expression of Wweight isWi=Wi@L(W,b)@W(16)bi=bi@L(W,b)@bi(17)In Formulae 16) and 17),denotes the learning rate usedto control the intensity of the BP.IV. RESULTS AND DISCUSSIONA. COMPARISON WITH OTHER TECHNIQUESIn this article, the network operated on Ubuntu 16.04System. The processing platform was desk computer, theprocessor was Intel Core i5-7500, the dominant frequencywas 3.40 GHz, the internal storage was 64 GB, the harddisk capacity was 4.1 TB, two 8-GB NVIDA GeForceGTX1080 graphics cards were used, and the programminglanguage was Python. The open-source framework of Tensor-Flow deep learning was adopted, and the software PyCharmwas used to program the deep neural network layers, as shownin Table 4.The dataset of the images of the wheat impurities wastrained on ResNet_101 and Inception_v3 before being com-pared with WheNet for recognizing the impurities in thewheat. Table presents the parameter conﬁgurations andtraining results of the three networks. According to the com-parative analysis in Table 5, the network input of WheNet forTABLE 4. Hardware and software environment.VOLUME 7, 2019 162213Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkTABLE 5. Performance comparison of the three CNNs.FIGURE 7. Loss rate curves.recognizing impurities is characterized by smaller numberof input images and shorter training time of 11 h. In addi-tion, the testing efﬁciency of an image is higher than thatof ResNet_101 and Inception_v3, and only 0.1 are neededfor each image. The numerical value of the loss function isrelatively low. The recognition rates on the validation setsTop_1 and Top_5 are 98.59% and 99.98%, respectively.With the incessant increase in the number of iterations,the classiﬁcation errors of the training set show decreasingtrend, as indicated in Fig. 7. When the serial number oftraining iterations of the WheNet network reaches 50,000, thetraining loss essentially stops and approximates to stablevalue. The stable numerical value of mean loss rate is 0.013,which is lower than that on ResNet_101 and Inception_v3.To summarize, judging from the training time, recognitionaccuracy, and efﬁciency, the network structure of WheNetdesigned in this article is sound and provides satisfactorytraining result.B. NETWORK TESTING EXPERIMENT ANDRESULT ANALYSISDue to the considerable difference in various categories ofwheat impurities in the training and validation sets, as wellas the imbalance of the data samples, accuracy is not asufﬁcient metric to describe the practical application per-formance. Thus, to evaluate the recognition effect of thenetworks, in June 2019, we selected 300 images from eachof the different categories of impurities from the operatingFIGURE 8. ROC curve of the image of the wheat impurities and the AUC.images offered by the combine harvesters. They served asa set for testing before comparing the relationship amongWheNet, ResNet_101, and Inception_v3, and were used toevaluate the receiver operating characteristic (ROC) curve,AUC, confusion matrix, and recall rate. These evaluationfunctions can be accessed from the Scikit-learn library.1) ROC CURVE AND AUCFor the ROC curve, the abscissa and ordinate are charac-terized by false positive rate (FPR) and true positiverate (TPR), respectively. The summation of the areas of thevarious parts below the curve is the AUC. In the evaluationaccuracy indexes of image recognition, we can quantize theROC curve of the model by calculating the AUC below thecurve. The closer the location of the ROC curve to the top leftcorner, the better its performance as classiﬁer. The largernumerical value of AUC indicates that this classiﬁer performsbetter. The computation for the FPR and TPR are conductedas follows :FPR =FPN(18)TPR =TPP(19)In Formula (18) and (19), Prefers to the number of real posi-tive samples, Nindicates the number of real negative samples,TPrepresents the number of the positive samples that havebeen predicted by the classiﬁer in Ppositive samples, and FP162214 VOLUME 7, 2019Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkFIGURE 9. Test result of the confusion matrix. (a) Individual classification rate by ResNet_101. (b) Individual classification rate by Inception_v3.(c) Individual classification rate by WheNet.FIGURE 10. Performances of the three tested networks.denotes the number of the positive samples that have beenpredicted by the classiﬁer in Nnegative samples.In most cases, as the ROC curve lies above the straight line(y=x), the value ranges from 0.5 to 1. In addition, the ROCcurve is formed by sequentially connecting the points whosecoordinates are {( x1,y1),(x2,y2),... (xm,ym)}. Thus, AUC iscalculated using the following formula (20):AUC =12Xm1i=1(xi+1xi)·(yi+yi+1) (20)where the coordinate values xiandyiare the values of theabove-mentioned FPR and TPR, respectively.As shown in Fig. 8, when the images are trained inWheNet, ResNet_101, and Inception_v3, it is obvious thatthe AUC of the red curve is larger than that of the greencurve, indicating that WheNet performs better in terms ofaccuracy. Its numerical value is 0.975, indicating that thenetwork has high accuracy rate for recognizing impuritiesin wheat. This result also indicates the good robustness andstrong practicality of WheNet.2) CONFUSION MATRIX AND RECALL RATEWhile evaluating the accuracy of image recognition, the con-fusion matrix is mainly used for comparing the objectiveresults and practical measured values. The recall rate is anindex used to evaluate the performance of the classiﬁers.In Fig. 9, the confusion matrix of the three networks iscalculated wherein the real categories (ordinates) are com-pared with the prediction categories (abscissas) to describethe individual classiﬁcation performance of each network.In the ﬁgure, denotes normal wheat, denotes insects,C denotes stalks, denotes grass, denotes awn of wheat,and denotes spikelet. We can judge the classiﬁcation resultof recognition from the color distribution of the confusionmatrix. The deeper the color, the larger the numerical value.It can be clearly seen that ResNet_101 has the poorest clas-siﬁcation effect and WheNet has the best classiﬁcation rate,as indicated in Fig. 9(a) and Fig. 9(c), respectively.To compare the recall rates among WheNet, ResNet_101,and Inception_v3, we tested the impurities separately. Thecalculation was conducted in accordance with Formula (19),and the calculated results are shown in Table 6. Judgingfrom Table 6, the recall rate of WheNet is 98.0%, and thecorresponding values of ResNet_101 and Inception_v3 are94.5% and 96.7%, respectively, indicating that the WheNetrecognition network is superior to ResNet_101 and Incep-tion_v3. The mean F1-score is 98.0%, which indicates thatthe network has high accuracy, as shown in Table 7. Theresult also shows the advantage of the WheNet network inidentifying impurities in wheat, as shown in Fig. 10.In terms of the recall rates of grass and spikelet, the threenetworks all show high numerical values. The main reason isVOLUME 7, 2019 162215Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural NetworkTABLE 6. Recall rate comparison of the three networks.TABLE 7. F1-score comparison of the three networks.FIGURE 11. Images mistakenly recognized as those of normal wheat.that there is big difference between spikelet and grass onthe one hand and normal wheat on the other in terms of shapeand color.These features are easily differentiated. The recall rate ofinsects is relatively low because insects are small-sized andtheir color is similar to that of wheat. One noteworthy pointis that this network explicitly addresses the recognition ofimpurities in wheat and there is possibility of misjudgmentunder certain conditions. We discuss this point next.3) ERROR ANALYSISIn this research, we arranged and analyzed the images of theimpurities in wheat by centering on the recognition errors ofthe WheNet network, as shown in Fig. 11. The main reasonsfor the recognition errors are as follows:The fact that partial impurities in wheat are similar tonormal wheat in shape and color leads to errors in imagerecognition. For example, in Fig. 11(a) and (b), the imagesof wheat with stalks and of wheat with insect impurities162216 VOLUME 7, 2019Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural Networkare recognized as images of normal wheat. We can solvethis issue by increasing the number of learning samples oradopting the Terahertz imaging method.The existence of occlusion and overlapping between someimpurities and wheat leads to recognition errors. For instance,in Fig. 11(c), the images of the impurities in the awns of wheatare mistakenly recognized as those of normal wheat. Again,in Fig.11(d), the images of wheat with stalks are incorrectlyrecognized as those of wheat with grass as impurities.Judging from the error analysis, the network has insufﬁ-cient local recognition capability for few impurities, andit cannot recognize images correctly and effectively when alarge surface area of wheat is occluded.V. CONCLUSIONThe aims of this research were to address problems such ashigh labor intensity in traditional manual monitoring of wheatimpurities and low recognition rate of current classiﬁcationmethods. Accordingly, we attempted to improve the perfor-mance of the Inception_v3 network and designed methodfor recognizing impurities in wheat characterized by WheNetCNN. The experimental results showed that our approach wasable to recognize impurities from the backgrounds of imagesof wheat approximately 98% of the time when using WheNetCNN. This outcome emphasizes the considerable superiorityof deep learning and its potential ability to detect impuritiesin wheat.The main contributions of this paper can be summarized asbelow:1) We constructed six datasets of labeled images ofwheat, namely normal wheat, insects, grass, stalks, spikelets,and awns of wheat. The training set was composedof 25,200 images and the validation set consisted of10,800 images. These datasets can be used to investigate auto-matic recognition applications and testing on wheat. Thesedatasets also provide important insights into other grain impu-rities.2) In line with practical application requirements and datacharacteristics, we adopted the Adam optimization algorithmto improve the training accuracy of WheNet. The resultingrecognition efﬁciency reached 0.1 per image. The recog-nition rates of both Top_1 and Top_5 on the validationset surpassed 98%. Compared with the classic networks ofResNet_101 and Inception_v3, the WheNet network is supe-rior in terms of recognition accuracy and training time.3) To test the performance of the network further, we ran-domly selected 300 images from each of the categories astest sets and measured the mean value of the AUC as 0.975.The recall rate of WheNet was found to be 98.0%, whichwas higher than that of ResNet_101 and Inception_v3. Thisresult proved that the proposed network demonstrates betterclassiﬁcation performance.In conclusion, the network proposed in this study can accu-rately recognize most impurities in wheat. However, for someimpurities that are similar in color to normal wheat, certainocclusions, and overlapping impurities, the network is proneto recognition errors. In the future, to enable wider applica-tion, we will deepen the network structure and increase thetypes and number of learning samples. We also plan to adoptthe Terahertz imaging method to enhance the recognitionability of the classiﬁers.REFERENCES[1] J. Chen, Y. Gu, Y. Lian, and M. N. Han, ‘‘Method for online identiﬁcationof impurities and broken grains in rice based on machine vision,’’ Trans.Chin. Soc. Agricult. Engineering. vol. 34, no. 13, pp. 187–194, Jul. 2018.[2] J. X. Guo, Y. B. Ying, X. Q. Rao, J. W. Li, Y. X. Kang, and Z. Shi,‘‘Hyperspectral image detection of carding inner impurities,’’ Chin. J.Agricult. machinery. vol. 43, no. 12, pp. 197–203, Dec. 2012.[3] S. Serranti, D. Cesare, and G. Bonifazi, ‘‘The development of hyperspec-tral imaging method for the detection of fusarium-damaged, yellow berryand vitreous italian durum wheat kernels,’’ Biosyst. Eng. vol. 115, no. 1,pp. 20–30, May 2013.[4] C. Wallays, B. Missotten, J. De Baerdemaeker, and W. Saeys, ‘‘Hyper-spectral waveband selection for on-line measurement of grain cleanness,’’Biosyst. Eng. vol. 104, no. 1, pp. 1–7, Sep. 2009.[5] J. Mahirah, K. Yamamoto, M. Miyamoto, N. Kondo, Y. Ogawa, T. Suzuki,H. Habaragamuwa, and U. Ahmad, ‘‘Monitoring harvested paddy duringcombine harvesting using machine vision-Double lighting system,’’ Eng.Agricult. Environ. Food. vol. 10, no. 2, pp. 140–149, Apr. 2017.[6] M. A. Shahin and S. J. Symons, ‘‘Detection of fusarium damage in cana-dian wheat using visible/near-infrared hyperspectral imaging,’’ J. FoodMeas. Characterization vol. 6, nos. 1–4, pp. 3–11, Dec. 2012.[7] C. B. Singh, D. S. Jayas, J. Paliwal, and N. D. G. White, ‘‘Fungal dam-age detection in wheat using short-wave near-infrared hyperspectral anddigital colour imaging,’’ Int. J. Food Properties vol. 15, no. 1, pp. 11–24,Jan. 2012.[8] M. Ebrahimi, M. Khoshtaghaza, S. Minaei, and B. Jamshidi, ‘‘Vision-based pest detection based on SVM classiﬁcation method,’’ Comput. Elec-tron. Agricult. vol. 137, pp. 52–58, May 2017.[9] T. Liu, W. Chen, W. Wu, C. Sun, W. Guo, and X. Zhu, ‘‘Detection of aphidsin wheat ﬁelds using computer vision technique,’’ Biosys. Eng. vol. 141,pp. 82–93, Jan. 2016.[10] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for imagerecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,Jun. 2016, pp. 770–778.[11] Y. H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga,and G. Toderici, ‘‘Beyond short snippets: Deep networks for video clas-siﬁcation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) ,Jun. 2015, pp. 4694–4702.[12] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,V. Vanhoucke, and A. Rabinovich, ‘‘Going deeper with convolutions,’’inProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) Boston,MA, USA, Jun. 2014, pp. 1–9.[13] G. E. Hinton, S. Osindero, and Y. W. Teh, ‘‘A fast learning algorithm fordeep belief nets,’’ Neural Comput. vol. 18, no. 7, pp. 1527–1554, Jul. 2014.[14] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks forlarge-scale image recognition,’’ in Proc. Comput. Vis. Pattern Recognit. ,Sep. 2014, pp. 1409–1556.[15] C. Szegedy, A. Toshev, and D. Erhan, ‘‘Deep neural networks for objectdetection,’’ Adv. Neural Inf. Process. Syst. vol. 26, no. 3, pp. 2553–2561,May 2013.[16] W. Ding and G. Taylor, ‘‘Automatic moth detection from trap imagesfor pest management,’’ Comput. Electron. Agricult. vol. 123, pp. 17–28,Apr. 2016.[17] B. Mandal, L. Li, G. S. Wang, and J. Lin, ‘‘Towards detection of bus driverfatigue based on robust visual analysis of eye state,’’ IEEE Trans. Intell.Transp. Syst. vol. 18, no. 3, pp. 545–557, Mar. 2017.[18] R. Singh and H. Om, ‘‘Newborn face recognition using deep con-volutional neural network,’’ Multimedia Tools Appl. vol. 76, no. 18,pp. 19005–19015, Sep. 2017.[19] A. Krizhevshy, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classiﬁcationwith deep convolutional neural networks,’’ Commun. Acm vol. 60, no. 6,pp. 84–90, Jun. 2017.[20] P. S. Yuan, W. Li, S. G. Ren, and H. L. Xu, ‘‘Identiﬁcation of chrysanthe-mum ﬂower types and varieties based on convolutional neural network,’’Trans. Chin. Soc. Agricult. Eng. vol. 34, no. 5, pp. 152–158, May 2018.VOLUME 7, 2019 162217Y. Shen et al. Image Recognition Method Based on an Improved Convolutional Neural Network[21] J. Amara, B. Bouaziz, and A. Algergawy, ‘‘A deep learning-based approachfor banana leaf diseases classiﬁcation,’’ in BTW Workshops in LectureNotes in Informatics, B. Mitschang, Eds. Bonn, Germany, Jan. 2017,pp. 79–88.[22] K. Yang, H. Liu, P. Wang, Z. Meng, and J. Chen, ‘‘Convolutional neuralnetwork-based automatic image recognition for agricultural machinery,’’Int. J. Agricult. Biol. Eng. vol. 11, no. 4, pp. 200–206, Aug. 2018.[23] S. P. Huang, C. Sun, L. Qi, X. Ma, and W. J. Wang, ‘‘Detection method ofrice blast based on deep convolutional neural network,’’ Trans. Chin. Soc.Agricult. Eng. vol. 33, no. 20, pp. 169–176, Oct. 2017.[24] S. P. Mohanty, D. P. Hughes, and M. Salathé, ‘‘Using deep learning forimage-based plant disease detection,’’ Frontiers Plant Sci. vol. 7, p. 1419,Sep. 2016.[25] A. dos Santos Ferreira, D. M. Freitas, G. G. da Silva,H. Pistori, and M. T. Folhes, ‘‘Weed detection in soybean cropsusing ConvNets,’’ Comput. Electron. Agricult. vol. 143, pp. 314–324,Dec. 2017.[26] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature vol. 521,pp. 436–444, May 2015.[27] Z. Q. Lin, S. Mu, F. Huang, K. A. Mateen, M. Wang, W. Gao, andJ. Jia, ‘‘A uniﬁed matrix-based convolutional neural network for ﬁne-grained image classiﬁcation of wheat leaf diseases,’’ IEEE Access vol. 7,pp. 11570–11590, 2019.[28] A. Ramcharan, K. Baranowski, P. McCloskey, B. Ahmed, J. Legg, andD. P. Hughes, ‘‘Deep learning for image-based cassava disease detection,’’Frontiers Plant Sci. vol. 8, p. 1852, Oct. 2017.[29] Y. Lu, S. Yi, N. Zeng, Y. Liu, and Y. Zhang, ‘‘Identi?cation of Rice diseasesusing deep convolutional neural networks,’’ Neurocomputing. vol. 267,pp. 378–384, Dec. 2017.[30] Z. H. Huang, L. Huang, Q. Li, T. Zhang, and N. Sang, ‘‘Framelet regu-larization for uneven intensity correction of color images with illumina-tion and reﬂectance estimation,’’ Neurocomputing vol. 314, pp. 154–168,Nov. 2018.[31] X. Zhang, Y. Qiao, F. Meng, C. Fan, and M. Zhang, ‘‘Identiﬁcation ofmaize leaf diseases using improved deep convolutional neural networks,’’IEEE Access vol. 6, pp. 30370–30377, 2018.[32] S. C. Satapathy, N. S. M. Raja, V. Rajinikanth, A. S. Ashour, and N. Dey,‘‘Multi-level Image Thresholding using Otsu and Chaotic Bat Algorithm,’’Neural Comput. Appl. vol. 29, no. 12, pp. 1285–1307, Jun. 2018.[33] J. Peng, Y. H. Chen, B. Liu, D. J. He, and C. Liang, ‘‘Real-time detectionof apple leaf diseases using deep learning approach based on improvedconvolutional neural networks,’’ IEEE Access vol. 7, pp. 59069–59080,2019.[34] K. Thenmozhi and U. S. Reddy, ‘‘Crop pest classiﬁcation based on deepconvolutional neural network and transfer learning,’’ Comput. Electron.Agricult. vol. 164, Sep. 2019, Art. no. 104906.[35] Y. Altunta≥, Z. Cömertb, and A. F. Kocamaza, ‘‘Identiﬁcation of haploidand diploid maize seeds using convolutional neural networks and transferlearning approach,’’ Comput. Electron. Agricult. vol. 163, Aug. 2019,Art. no. 104874.[36] M. M. Ghazi, B. Yanikoglu, and E. Aptoula, ‘‘Plant identiﬁcation usingdeep neural networks via optimization of transfer learning parameters,’’Neurocomputing vol. 235, pp. 228–235, Apr. 2017.[37] L. N. Smith, ‘‘Cyclical learning rates for training neural networks,’’inProc. IEEE Winter Conf. Appl. Comput. Vis. (WACV) Mar. 2017,pp. 464–472.[38] Y. Sun, W. Zhang, H. Gu, C. Liu, S. Hong, W. Xu, J. Yang, and G.Gui, ‘‘Convolutional neural network based models for improving super-resolution imaging,’’ IEEE Access vol. 7, pp. 43042–43051, 2019.[39] D. P. Kingma and J. Ba, ‘‘Adam: method for stochastic opti-mization,’’ Jul. 2015, arXiv:1412.6980 [Online]. Available: https://arxiv.org/abs/1412.6980YIN SHEN was born in Sichuan, China, in 1987.He received the master’s degree in agricul-tural engineering from Southwest University,Chongqing, China, in 2014, where he is currentlypursuing the Ph.D. degree in agricultural electriﬁ-cation and automation. He has authored more thanten articles in related journals. His current researchinterests include computer vision, deep learning,and image recognition.YANXIN YIN received the Ph.D. degree fromChina Agriculture University, Beijing, China,in 2014. He is currently an Associate with theNational Research Center of Intelligent Equip-ment for Agriculture. He has participated in theprojects 863 Program. His current research inter-ests include wireless sensor networks and vehicu-lar ad hoc networks.CHUNJIANG ZHAO received the Ph.D. degreefrom China Agriculture University, Beijing,China. He is currently Doctoral Supervisor TutorAcademician with the Chinese Academy of Engi-neering, expert enjoying the special governmentallowance, China’s expert with great contributions.He has more than 30 years of industry and univer-sity experience, as fellow, Chief Scientist, and aDistinguished Member of Technical Staff. He hasauthored more than 400 articles in related journals.His current research interests include image analysis, intelligent agriculture,deep learning, and agricultural information technology.BIN LI received the Ph.D. degree from ChinaAgriculture University, Beijing, China. He was aMaster Tutor. He is currently an Associate Fel-low. He has authored more than 30 articles inrelated journals. His current research interestsinclude terahertz spectroscopy, image recognition,and machine vision technology.JUN WANG received the Ph.D. degree from theDepartment of Electronic and Computer Engi-neering, Hanyang University, Seoul, South Korea,in February 2011. He was Postdoctoral Fel-low with the University of Wisconsin–Madison,Madison, WI, USA, from 2011 to 2012. He hasbeen an Associate Professor with the College ofElectronics and Information Engineering, SichuanUniversity, Chengdu, China, since 2012. He haspublished more than 50 publications includingmore than 30 SCI indexed articles. His current research interests includeimage encryption, holographic 3D display, GPU parallel computing, deeplearning, real-time image, and video processing. He is currently an AssociateEditor of IEEE CCESS journal.GUANGLIN LI received the Ph.D. degree fromSouthwest University, Chongqing, China, wherehe is currently Professor and Doctoral Super-visor with the School of Engineering and Tech-nology. He served as the Dean of the Schoolof Engineering Technology, Senior Member,and Teaching Guidance Committee with theChongqing Hilly Mountain Agricultural Equip-ment Key Laboratory. He has published more than60 articles and 20 patents. His current researchinterests include the application of parallel computing in image processing,image analysis, and the Internet of Things key technology.ZIQIANG ZHANG was born in Tengzhou County,China, in 1993. He is currently pursuing the M.S.degree in software engineering with the Informa-tion Engineering College, Capital Normal Univer-sity, China. His current research interests includedeep learning for agricultural image classiﬁcationand deep transfer learning for object recognition.162218 VOLUME 7, 2019AnnotationsImage recognition method based on an improved convolutional neural network to detect impurities in wheatShen, Y; Yin, Y; Zhao, C; Li, B; Wang, J; Li, G; Zhang, Z01Amirhossein Zaji Page 231/7/2020 18:01