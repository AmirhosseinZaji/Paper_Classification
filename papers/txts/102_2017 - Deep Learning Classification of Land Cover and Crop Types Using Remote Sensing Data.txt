778 IEEE GEOSCIENCE AND REMOTE SENS ING LETTERS, VOL. 14 NO. 5, MAY 2017Deep Learning Classiﬁcation of Land Cover andCrop Types Using Remote Sensing DataNataliia Kussul, Mykola Lavreniuk, Sergii Skakun, and Andrii ShelestovAbstract Deep learning (DL) is powerful state-of-the-arttechnique for image processing including remote sensing (RS)images. This letter describes multilevel DL architecture thattargets land cover and crop type classiﬁcation from multitempo-ral multisource satellite imagery. The pillars of the architectureare unsupervised neural network (NN) that is used for opticalimagery segmentation and missing data restoration due to cloudsand shadows, and an ensemble of supervised NNs. As basicsupervised NN architecture, we use traditional fully connectedmultilayer perceptron (MLP) nd the most commonly usedapproach in RS community random forest, and compare themwith convolutional NNs (CNNs). Experiments are carried outfor the joint experiment of crop assessment and monitoring testsite in Ukraine for classiﬁcation of crops in heterogeneousenvironment using nineteen multitemporal scenes acquired byLandsat-8 and Sentinel-1A RS satellites. The architecture withan ensemble of CNNs outperforms the one with MLPs allowingus to better discriminate certain summer crop types, in particularmaize and soybeans, and yielding the target accuracies more than85% for all major crops (wheat, maize, sunﬂower, soybeans, andsugar beet).Index Terms Agriculture, convolutional neuralnetworks (CNNs), crop classiﬁcation, deep learning (DL),joint experiment of crop assessment and monitoring (JECAM),Landsat-8, remote sensing (RS), Sentinel-1, TensorFlow, Ukraine.I. NTRODUCTIONTHE last several years and onward could be called theyears of Big Free Data in remote sensing (RS). During the2013–2016 period, several optical and synthetic aperture radar(SAR) RS satellites were launched with high spatial resolution(10–30 m), in particular Sentinel-1A/B and Sentinel-2A withinthe European Copernicus program [1], [2], and Landsat-8within the Landsat Project, joint initiative between the U.S.Geological Survey (USGS) and the National Aeronautics andSpace Administration [3]. These data sets are freely availableon operational basis. This ope ns unprecedented opportunitiesfor wide range of preoperationa and operational applicationsin the environment and agricultural domains taking advantageof high temporal resolution data sets and advances in theManuscript received February 17, 2017; accepted March 6, 2017. Date ofpublication March 31, 2017; date of current version April 20, 2017.N. Kussul and M. Lavreniuk are with the Department of Space InformationTechnologies and Systems, Space Research Institute, National Academyof Sciences of Ukraine and SSA Ukra ine, 03680 Kyiv, Ukraine (e-mail:inform@ikd.kiev.ua; nataliia.ku ssul@gmail.com; nick_93@ukr.net).S. Skakun is with the Department of Geographical Sciences, University ofMaryland, College Park, MD 20742 USA (e-mail: skakun@umd.edu).A. Shelestov is with the Department of Information Security, NationalTechnical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute,”03056 Kyiv, Ukraine (e-mail: andrii.shelestov@gmail.com).Color versions of one or more of the ﬁgures in this letter are availableonline at http://ieeexplore.ieee.org.Digital Object Identiﬁer 10.1109/LGRS.2017.2681128multisources data fusion techniques [4], [5]. Land cover andcrop type maps are one of the most essential inputs whendealing with environmental and agriculture monitoring tasks[6]–[8]. Multitemporal multisource satellite imagery is usuallyrequired in order to capture speciﬁc crop growth stages andthus being able to discriminate different crop types. For exam-ple, multispectral optical im agery only might not be enoughto discriminate summer crops in complex and heterogeneousenvironment. For this, SAR-derived information adds an addedvalue that allows discrimination of particular crop types [9],[10].A comprehensive study on the state-of-the-art supervisedpixel-based methods for land cover mapping was performedby Khatami et al. [11]. They found that support vectormachine (SVM) was the most efﬁcient for most applicationswith an overall accuracy (OA) of about 75%. The secondmethod with approximately the same efﬁciency (74% of OA)was neural network (NN)-based classiﬁer. In that study,classiﬁcation was done only for single date image. At thesame time, SVM is too much resource consuming to beused for big data applications and large area classiﬁcationproblems. Another popular approach in the RS domain is therandom forest (RF)-based approach [12]. However, multiplefeatures should be engineered to feed the RF classiﬁer for theefﬁcient use.Over the past few years, the most popular and efﬁcientapproaches for multisensor and multitemporal land coverclassiﬁcation are ensemble-based [13]–[16] and deep learn-ing (DL) [17]–[20]. These techniques are found to outperformthe SVM [21]–[23]. DL is powerful machine learningmethodology for solving wide range of tasks arising in imageprocessing, computer vision, signal processing, and naturallanguage processing [24]. The main idea is to simulate thehuman vision to deal with big data problem, use all the dataavailable and provide the semantic information at the output.Plenty of models, frameworks and benchmark databases of ref-erence imagery are available or image classiﬁcation domain.Over past years, more and more studies have been using DLfor processing of RS imagery [25], [26]. DL proved to beefﬁcient for processing both optical (hyperspectral and mul-tispectral imagery) and radar images, in extracting differentland cover types such as road extraction, buildings extrac-tion [17], [27], [28]. In terms of particular DL architectures,convolutional NNs (CNNs), deep autoencoders, deep beliefnetworks, and recurrent NN with long short-term memorymodel have already been explored for RS tasks [17], [28]–[31].It should be noted that most studies with DL for RS utilizea single date image for classiﬁcation purposes, e.g., land1545-598X 2017 EEE. Personal se is perm itted, but republication/redistri bution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.KUSSUL et al. DL CLASSIFICATION OF LAND COVER AND CROP TYPES 779TABLE IDATES OF ACQUISITION OF LANDSAT -8AND SENTINEL -1AFOR THE KYIVREGION IN 2015cover or object detection. However, multitemporal images areusually required to reliably identify speciﬁc land cover classessuch as crop types.When providing large scale crop mapping using multi-temporal satellite imagery, th following challenges shouldbe addressed while using DL. First, pixels of satelliteimage contain physical values. In particular, each pixel ofthe optical imagery contains spectral reﬂectance values inmultiple spectral bands, and can be contaminated with cloudsand shadows, while each pixel of the spaceborne SAR imageryis characterized by backscatter intensity and phase in multiplepolarizations. Both of the data sources have multitemporalnature and different spatial resolutions. That is why, DL imple-mentation for land cover and crops classiﬁcation based ondata fusion of multitemporal multisensor satellite data is achallenge.In this letter, we propose multilayer DL architecturethat is targeted for classiﬁcation of multisource multitemporalRS images, both optical and SAR, at pixel level. The coreof the architecture is an ensemble of CNNs. The proposedarchitecture is applied for crop classiﬁcation using Landsat-8and Sentinel-1A time-series and provides accuracy highenough >85%) to be considered for operational context atthe national level [9].II. STUDY AREA AND MATERIALSWe address the problem of land cover and crop clas-siﬁcation for Kyiv region of Ukraine using multitemporalmultisource images acquired by Landsat-8 and Sentinel-1Asatellites. The study area is classiﬁed into eleven classesincluding major agricultural crops (water, forest, grassland,bare land, winter wheat, winte rapeseed, spring cereals,soybeans, maize, sunﬂowers, and sugar beet). It is rather largearea (28 000 square km) with big diversity of different landcover types and agricultural crops. The territory is big enoughto be considered as representative one for the extensionof the technology to the entire country. Such technologyis particularly important taking into account national leveldemonstration within the ESA “Sentinel for Agriculture”project (Sen2Agri), started in 2015.For the 2015 vegetation season (since October 2014 tillSeptember 2015) four Landsat-8 and ﬁfteen Sentinel-1 imageswere acquired for the study area (Table I). Atmosphericallycorrected Landsat-8 images do wnloaded from the USGS earth-Explorer system were used in this letter [32]. The Landsat-8product is provided with cloud and shadow masks [33].These regions were masked as areas without data. Sentinel-1Aimages went through preprocessing procedure that cov-TABLE IINUMBER AND AREA OF POLYGONS COLLECTED DURINGGROUND SURVEYS FOR THE KYIVREGION IN 2015ered calibration, multilooking (with ×2 window), speckleﬁltering (3 ×3 window with Reﬁned Lee algorithm), andterrain correction using The Shuttle Radar Topography Mis-sion (SRTM) digital elevation model (DEM). time-seriesof six spectral bands from each Landsat-8 scene and twobands with combinations of vertical transmit and verticalreceive and combinations of vertical transmit and horizontalreceive polarizations from each Sentinel-1 scene is used as aninput to the classiﬁcation model. In the study for the 2015vegetation season, we used four images acquired by Landsat-8 and 15 images acquired by Sentinel-1A. Ground truth datawere collected during ground surveys in May–July of 2015to generate training and testing sets to train and validate theproposed classiﬁer, respectively. Ground surveys were con-ducted along the road as an adopted sampling design for thejoint experiment of crop assessment and monitoring (JECAM)experiments [34]. In total, 547 polygons of different classeswere collected (Table II). These polygons were randomlydivided into training (calibration) set (50%) and validationset (50%).III. METHODOLOGYA. General Architecture OverviewA four-level architecture is proposed for classiﬁcation ofcrop types from multitemporal satellite imagery. These levelsare preprocessing, supervised classiﬁcation, postprocessing,and geospatial analysis (Fig. 1).Since optical satellite imagery can be contaminated withclouds and shadows, one have to deal with missing values inthe imagery. Most classiﬁers accept only valid pixel valuesas an input, and therefore preprocessing step should beperformed to impute (or ﬁll gaps) missing values. This pro-cedure is performed within level of the architecture. Thenext step is supervised classiﬁcation level II which is thecore of this letter. We propose different CNNs architectures,namely, 1-D and 2-D, to explore spectral and spatial features,respectively. To our best knowledge, this is the ﬁrst attemptto apply CNNs to multisource multitemporal satellite imageryfor crop classiﬁcation. The CNNs architecture is comparedto the existing methods such as an ensemble of multilayerperceptrons (MLPs) (ENN) and RF classiﬁer. Levels III and IV780 IEEE GEOSCIENCE AND REMOTE SENS ING LETTERS, VOL. 14 NO. 5, MAY 2017Fig. 1. Four-level hierarchical DL model for satellite data classiﬁcationand land cover/land use changes analysis (I—preprocessing for dealingwith missing data on optical images due to clouds/shadows; II—supervisedclassiﬁcation; III—postprocessing usin additional geospatial data to improveclassiﬁcation maps; IV—geospatial analy sis for high-level product, e.g., croparea estimation).are aimed at improving the resulting classiﬁcation map withavailable geospatial layers and building high-level products.The latter can be crop area estimation and crop rotation areaestimation. All these levels of the architecture are describedin more detail in the following sections.B. Level I: PreprocessingFor preprocessing, we utilize self-organizing Kohonenmaps (SOMs) for optical images segmentation and subse-quent restoration of missing data in time-series of satelliteimagery [35]. SOMs are trained for each spectral band sepa-rately using nonmissing values. Missing values are restoredthrough special procedure that substitutes input sample’smissing components with neuron’s weight coefﬁcients. Pixelsthat have been restored are masked, the number of cloud-freescenes available for each pixel from optical imagery is calcu-lated, and these two layers are used for further postprocessingprocedure (at level III to improve the resulting classiﬁcationmap [16]. The detailed description of the restoration algorithmis given in [35] and [36].C. Level II: Supervised Classiﬁcation1) General Overview: The core element of the model isthe supervised classiﬁcation, which is performed at the secondstage level II ). We explore two different paradigms: state-of-the-art methods (RF an ENN) and compare those classiﬁerswith proposed ensemble of CNNs. Each MLP represents aclassical fully connected NN with single hidden layer. TheMLP transforms an input into feature space by hiddenlayer, and features are subsequently used to discriminateclasses by the output layer. The CNN, in turn, builds ahierarchal set of features through local convolution and down-sampling. The resulting feature maps are used to discriminateFig. 2. Deep CNN architecture. (Top) 1-D CNN architecture.(Bottom) 2-D CNN architecture.classes with fully connected output layer. The detailed descrip-tion of the ENN approach is given in [34] and [36], while theCNN architecture is described in the next section. We usedthe Orfeo Toolbox implementation for the RF classiﬁer.2) Supervised Classiﬁcation With Convolutional NeuralNetworks: The two bands from each of the ﬁfteenSentinel-1A scenes and the six bands from each of the fourLandsat-8 scenes form CNN input feature vector withdimension size 54 (15 ×2+4×6). Traditional CNNs (2-D) takeinto account spatial context of an image and provide higheraccuracy comparing to per pixel -based approach. However,in this case, CNN smooths not only some misclassiﬁed pixelbut also small objects like roads, and forest “stripes” andclear cuts within the forest (with linear dimensions of severalpixels) are missed. In this letter, we compare two differentCNN architectures: 1-D CNN with convolutions in the spectraldomain [37], and 2-D CNN with convolutions in the spatialdomain. Each CNN in the corresponding ensemble consistsof two convolutional layers, each of them followed by maxpooling and two fully connected layers in the end (Fig. 2).We used rectiﬁed linear unit (ReLU) function that is oneof the most popular and efﬁcient activation functions for deepNNs. There are advantages of using ReLU such as biologicalplausibility, efﬁcient computation, and gradient propagation.Therefore, ReLU function is faster and more effective fortraining CNNs comparing to sigmoid function. In botharchitectures, there are ﬁve CNNs in the ensemble. Each ofthe CNNs has the same convolution and max-pooling structurebut differs in the trained ﬁlters and number of neurons in thehidden layer being 60, 70, 80, 90, and 100 for ﬁve CNNs,respectively.Tikhonov’s L2 regularizati on, dropout with probability of0.5 and learning rate exponen tial decay techniques are usedto prevent the overﬁtting problem and to generalize the lossfunction. For loss function optimization, we used advancedadaptive moment estimation method that is combinationof AdaGrad and RMSProp methods and has faster con-vergence comparing to the well-known methods such asgradient descent, stochastic gradient descent, AdaGrad andRMSProp [38]. Batch learning technique with sample size 32is used to speed-up the NN training phase. Multiclass crossKUSSUL et al. DL CLASSIFICATION OF LAND COVER AND CROP TYPES 781entropy function is used as loss function and softmaxfunction is used to provide the posteriori probability foreach class. Ensembles of 1-D and 2-D CNNs are implementedusing the Google’s library TensorFlow [39].The proposed 1-D CNN architecture is able to provide clas-siﬁcation for each pixel of the input image. In turn, classical2-D CNN provides class for window with size of ×7pixels. In this case, there is popular approach with directup-scaling to match the input size that will actually reducethe spatial resolution of the classiﬁcation map (at the order offour times for two down-scaling layers with ﬁlter size ×2).In our experiment, we utilized sliding window technologywith 1 pixel step and the resulting class was assigned tothe central pixel of the sliding window. Therefore, the outputclassiﬁcation map had the same spatial resolution, but even inthis case some small objects were smoothed and misclassiﬁed.D. Level III and IV: Postprocessing and Geospatial AnalysisTo improve the quality of the resulting map, we developedseveral ﬁltering algorithms, based on the available informationon quality of input data and ﬁel ds boundaries, for exampleparcels [16]. Those ﬁlters take pixel-based classiﬁcationmap and speciﬁcally designed rules to account for severalplots (ﬁelds) within the parcel. In the result, we obtained aclear parcel-based classiﬁcation map. The ﬁnal level of dataprocessing provides data fusion with multisourced heteroge-neous information, in particular, statistical data, vector geospa-tial data, socio-economic information, and so on. It allowsinterpreting the classiﬁcation results, solving applied problemsfor different domains, and providing the support informationfor decision makers. For example, classiﬁcation map coupledwith area frame sampling approach can be used to estimatecrop areas [40].IV RESULTSOverall classiﬁcation accur acies for RF, ENN, ensemble of1-D and 2-D CNNs were 88.7%, 92.7%, 93.5%, and 94.6%,respectively (Table III). User’s and producer’s accuracies(UA and PA) provided by an ensemble of 2-D CNNs were thehighest for all classes. The RF classiﬁer provided the lowestaccuracies comparing to NN-ba sed approaches. Accuracy forwinter rapeseed, spring crops, sunﬂower, forest, and waterdid not vary signiﬁcantly with different approaches. At thesame time, major improvements of using CNNs comparing toRF were achieved for maize, sugar beet, soybeans, grassland,and bare land (Fig. 3). Usually, the main confusion in cropclassiﬁcation map for Ukraine territory is confusion betweenmaize and soybeans. Using the ensemble of 2-D CNNs, wewere able to discriminate these classes more reliably: maize(PA=94.6%, UA =93.6%) and soybeans (PA =86.9%,UA=89.1%).All these experiments were executed on computer withIntel Core i7-4770 processor and RAM 32 Gb. Trainingof ensemble of MLPs took up to 10 min at the sametime ensemble of 1-D CNNs trained approximately h and2-D CNNs training takes about 12 h.V. CONCLUSIONIn this letter, we proposed multilevel DL approach forland cover and crop types classiﬁcation using multitemporalFig. 3. Example of classiﬁcation result for the Kyiv region for 2015 basedon all Landsat-8 and Sentinel-1A images.TABLE IIICOMPARISON OF PA, UA, AND OA FOR THE RF, ENN,1-D CNNs, AND 2-D CNNs FOR KY VR G O NI 2015multisource satellite imagery. The architecture uses both unsu-pervised and supervised NNs for segmentation and subsequentclassiﬁcation of satellite imagery, respectively. In this letter,we used Landsat-8 and Sentinel-1A images over the JECAMtest site in Ukraine. Ensemble of 1-D and 2-D CNNs outper-formed the RF classiﬁer and an ensemble of MLPs allowingus to better discriminate summer crops, in particular maizeand soybeans. In general, the use of CNN allowed us to reachthe target accuracy of 85% for major crops (wheat, maize,sunﬂower, soybeans, and sugar beet) thus making foundationfor further operational use of RS data for the whole territoryof Ukraine within the Sentinel-2 for Agriculture project. Themain advantage of using CNNs over MLP and RF is thatit enables to build hierarchy of local and sparse featuresderived from spectral and temporal proﬁles while MLP and782 IEEE GEOSCIENCE AND REMOTE SENS ING LETTERS, VOL. 14 NO. 5, MAY 2017RF build global transformation of features. The 2-D CNNsoutperformed the 1-D CNNs, but some small objects in theﬁnal classiﬁcation map provided by 2-D CNNs were smoothedand misclassiﬁed.REFERENCES[1] M. Drusch et al. “Sentinel-2: ESA’s optical high-resolution missionfor GMES operational services,” Remote Sens. Environ. vol. 120,pp. 25–36, May 2012.[2] R. Torres et al. “GMES Sentinel-1 mission,” Remote Sens. Environ. ,vol. 120, pp. 9–24, May 2012.[3] D. P. Roy et al. “Landsat-8: Science and product vision for terrestrialglobal change research,” Remote Sens. Environ. vol. 145, pp. 154–172,Apr. 2014.[4] J. Zhang, “Multi-source remote sensing data fusion: Status and trends,”Int. J. Image Data Fusion vol. 1, no. 1, pp. 5–24, Nov. 2010.[5] M. D. Mura, S. Prasad, F. Paciﬁci, P. Gamba, J. Chanussot, andJ. A. Benediktsson, “Challenge and opportunities of multimodalityand data fusion in remote sensing,” Proc. IEEE vol. 103, no. 9,pp. 1585–1601, Sep. 2015.[6] A. Kolotii et al. “Comparison of biophysical and satellite pre-dictors for wheat yield forecasting in Ukraine,” Int. Arch. Pho-togramm., Remote Sens. Spatial Inf. Sci. vol. 40, no. 7, p. 35, 2015,doi: 10.5194/isprsarchives-XL-7-W3-39-2015.[7] F. Kogan et al. “Winter wheat yield forecasting: comparative analysisof results of regression and biophysical models,” J. Autom. Inf. Sci. ,vol. 45, no. 6, pp. 68–81, 2013.[8] F. Kogan et al. “Winter wheat yield forecasting in Ukraine based onEarth observation, meteorological data and biophysical models,” Int.J. Appl. Earth Observat. Geoinf. vol. 23, pp. 192–203, Aug. 2013.[9] H. McNairn, A. Kross, D. Lapen, R. Caves, and J. Shang, “Early seasonmonitoring of corn and soybeans with TerraSAR-X and RADARSAT-2,”Int. J. Appl. Earth Observat. Geoinf. vol. 28, pp. 252–259, May 2014.[10] S. Skakun, N. Kussul, A. . Shelestov, M. Lavreniuk, and O. Kussul,“Efﬁciency assessment of multitemporal C-band Radarsat-2 intensityand Landsat-8 surface reﬂectance satellite imagery for crop classiﬁcationin Ukraine,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. ,vol. 9, no. 8, pp. 3712–3719, Aug. 2016.[11] R. Khatami, G. Mountrakis, and S. . Stehman, “A meta-analysis ofremote sensing research on supervised pixel-based land-cover imageclassiﬁcation processes: General guidelines for practitioners and futureresearch,” Remote Sens. Environ. vol. 177, pp. 89–100, May 2016.[12] P. O. Gislason, J. A. Benediktsson, and J. R. Sveinsson, “Random Forestsfor land cover classiﬁcation,” Pattern Recognit. Lett. vol. 27, no. 4,pp. 294–300, 2006.[13] M. Han, X. Zhu, and W. Yao, “Remote sensing image classiﬁcationbased on neural network ensemble algorithm,” Neurocomputing vol. 78,no. 1, pp. 133–138, 2012.[14] X. Huang and L. Zhang, “An SVM ensemble approach combiningspectral, structural, and semantic features for the classiﬁcation of high-resolution remotely sensed imagery,” IEEE Trans. Geosci. Remote Sens. ,vol. 51, no. 1, pp. 257–272, Jan. 2013.[15] M. S. Lavreniuk et al. “Large-scale classiﬁcation of land coverusing retrospective satellite data,” Cybern. Syst. Anal. vol. 52, no. 1,pp. 127–138, 2016.[16] N. Kussul, G. Lemoine, F. J. Gallego, S. . Skakun, M. Lavreniuk, andA. . Shelestov, “Parcel-based crop classiﬁcation in Ukraine usingLandsat-8 data and Sentinel-1A data,” IEEE J. Sel. Topics Appl. EarthObserv. Remote Sens. vol. 9, no. 6, pp. 2500–2508, Jan. 2016.[17] . Chen, Z. Lin, X. Zhao, G. Wang, and . Gu, “Deep learning-basedclassiﬁcation of hyperspectral data,” IEEE J. Sel. Topics Appl. EarthObserv. Remote Sens. vol. 7, no. 6, pp. 2094–2107, Jun. 2014.[18] W. Zhao and S. Du, “Learning multiscale and deep representations forclassifying remotely sensed imagery,” ISPRS J. Photogramm. RemoteSens. vol. 113, pp. 155–165, Mar. 2016.[19] N. Kussul, N. Lavreniuk, A. Shelestov, B. Yailymov, and I. Butko, “Landcover changes analysis based on deep machine learning technique,”J. Autom. Inf. Sci. vol. 48, no. 5, pp. 42–54, 2016.[20] N. Kussul, A. Shelestov, R. Basarab, S. Skakun, O. Kussul, andM. Lavrenyuk, “Geospatial intelligence and data fusion techniquesfor sustainable development problems,” in Proc. ICTERI 2015,pp. 196–203.[21] J. Ding, B. Chen, H. Liu, and M. Huang, “Convolutional neural networkwith data augmentation for SAR target recognition,” IEEE Geosci.Remote Sens. Lett. vol. 13, no. 3, pp. 364–368, Mar. 2016.[22] F. J. Huang and . LeCun, “Large-scale learning with SVM andconvolutional for generic object categorization,” in Proc. IEEEComput. Soc. Conf. Comput. Vis. Pattern Recognit. Jun. 2006,pp. 284–291.[23] T. Ishii, R. Nakamura, H. Nakada, . Mochizuki, and H. Ishikawa,“Surface object recognition with CNN and SVM in Landsat images,”inProc. 14th IAPR Int. Conf. Mach. Vis. Appl. (MVA) May 2015,pp. 341–344.[24] . LeCun, . Bengio, and G. Hinton, “Deep learning,” Nature vol. 521,pp. 436–444, May 2015.[25] F. Zhang, B. Du, and L. Zhang, “Sa liency-guided unsupervised featurelearning for scene classiﬁcation,” IEEE Trans. Geosci. Remote Sens. ,vol. 53, no. 4, pp. 2175–2184, Apr. 2015.[26] F. Zhang, B. Du, and L. Zhang, “Scene classiﬁcation via gradi-ent boosting random convolutional network framework,” IEEE Trans.Geosci. Remote Sens. vol. 54, no. 3, pp. 1793–1802, Mar. 2016.[27] . Mnih and G. E. Hinton, “Learning to detect roads in high-resolution aerial images,” in Proc. Eur. Conf. Comput. Vis. 2010,pp. 210–223.[28] J. Geng, J. Fan, H. Wang, X. Ma, B. Li, and F. Chen, “High-resolutionSAR image classiﬁcation via deep convolutional autoencoders,” IEEETrans. Geosci. Remote Sens. vol. 12, no. 11, pp. 2351–2355,Nov. 2015.[29] . Chen, X. Zhao, and X. Jia, “Spect ral–spatial classiﬁcation of hyper-spectral data based on deep belief network,” IEEE J. Sel. Topics Appl.Earth Observ. Remote Sens. vol. 8, no. 6, pp. 2381–2392, Jun. 2015.[30] H. Liang and Q. Li, “Hyperspectra imagery classiﬁcation using sparserepresentations of convolutional neural network features,” Remote Sens. ,vol. 8, no. 2, p. 99, 2016.[31] H. Lyu, H. Lu, and L. Mou, “Learni ng transferable change rule froma recurrent neural network for la nd cover change detection,” RemoteSens. vol. 8, no. 6, p. 506, 2016.[32] E. Vermote, C. Justice, M. Claverie, and B. Franch, “Preliminaryanalysis of the performance of the Landsat 8/OLI land surfacereﬂectance product,” Remote Sens. Environ. vol. 185, pp. 46–56, 2016,doi: 10.1016/j.rse.2016.04.008.[33] Z. Zhu, S. Wang, and C. E. Woodcock, “Improvement and expansionof the Fmask algorithm: Cloud, cloud shadow, and snow detectionfor Landsats 4–7, 8, and Sentinel images,” Remote Sens. Environ. ,vol. 159, pp. 269–277, Mar. 2015.[34] F. Waldner et al. “Towards set of agrosystem-speciﬁc cropland map-ping methods to address the global cropland diversity,” Int. J. RemoteSens. vol. 37, no. 14, pp. 3196–3231, 2016.[35] S. . Skakun and R. M. Basarab, “Reconstruction of missing data intime-series of optical satellite images using self-organizing Kohonenmaps,” J. Autom. Inform. Sci. vol. 46, no. 12, pp. 19–26, 2014.[36] N. Kussul, S. Skakun, A. Shelestov, M. Lavreniuk, B. Yailymov, andO. Kussul, “Regional scale crop mapping using multi-temporal satelliteimagery,” Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. vol. 40,no. 7, pp. 45–52, 2015.[37] W. Hu, . Huang, L. Wei, F. Zhang, and H. Li, “Deep convolu-tional neural networks for hyperspe ctral image classiﬁcation,” J. Sens. ,vol. 2015, art. no. 258619, 2015.[38] D. P. Kingma and J. Ba. “Adam: method for stochastic optimization.”Unpublished paper, 2014. [Online]. Available: https://arxiv.org/abs/1412.6980[39] M. Abadi et al. “TensorFlow: Large-scale machine learning on het-erogeneous distributed systems.” Unpublished paper, 2016. [Online].Available: https://arxiv.org/abs/1603.04467[40] F. J. Gallego, N. Kussul, S. Skakun, O. Kravchenko, A. Shelestov, andO. Kussul, “Efﬁciency assessment of using satellite data for crop areaestimation in Ukraine,” Int. J. Appl. Earth Observat. Geoinf. vol. 29,pp. 22–30, Jun. 2014.