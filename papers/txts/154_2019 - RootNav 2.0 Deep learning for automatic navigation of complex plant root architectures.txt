GigaScience ,8 ,2 1 ,1 1 6doi: 10.1093/gigascience/giz123ResearchRESEARCHRootNav 2.0: Deep learning for automatic navigationof complex plant root architecturesRobail Yasrab1, Jonathan A. Atkinson2,D r e nM .W l s2,Andrew P. French1,2, Tony P. Pridmore1and Michael P. Pound1,*1School of Computer Science, University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham NG81BB, UK and2School of Biosciences, Sutton Bonington Campus, University of Nottingham, Nottingham LE125RD, UK∗Correspondence address. Michael P. Pound, School of Computer Science, University of Nottingham, Jubilee Campus, Wollaton Road, Nottingham NG81BB, UK. Email: michael.pound@nottingham.ac.uk http://orcid.org/0000-0002-5016-1078AbstractBackground: In recent years quantitative analysis of root growth has become increasingly important as way to explore thein/f_luence of abiotic stress such as high temperature and drought on plant’s ability to take up water and nutrients.Segmentation and feature extraction of plant roots from images presents signi/f_icant computer vision challenge. Rootimages contain complicated structures, variations in size, background, occlusion, clutter and variation in lightingconditions. We present new image analysis approach that provides fully automatic extraction of complex root systemarchitectures from range of plant species in varied imaging set-ups. Driven by modern deep-learning approaches,RootNav 2.0 replaces previously manual and semi-automatic feature extraction with an extremely deep multi-taskconvolutional neural network architecture. The network also locates seeds, /f_irst order and second order root tips to drive asearch algorithm seeking optimal paths throughout the image, extracting accurate architectures without user interaction.Results: We develop and train novel deep network architecture to explicitly combine local pixel information with globalscene information in order to accurately segment small root features across high-resolution images. The proposed methodwas evaluated on images of wheat Triticum aestivum L.) from seedling assay. Compared with semi-automatic analysis viathe original RootNav tool, the proposed method demonstrated comparable accuracy, with 10-fold increase in speed. Thenetwork was able to adapt to different plant species via transfer learning, offering similar accuracy when transferred to anArabidopsis thaliana plate assay. /f_inal instance of transfer learning, to images of Brassica napus from hydroponic assay,still demonstrated good accuracy despite many fewer training images. Conclusions: We present RootNav 2.0, newapproach to root image analysis driven by deep neural network. The tool can be adapted to new image domains with areduced number of images, and offers substantial speed improvements over semi-automatic and manual approaches. Thetool outputs root architectures in the widely accepted RSML standard, for which numerous analysis packages exist(http://rootsystemml.github.io/ ), as well as segmentation masks compatible with other automated measurement tools. Thetool will provide researchers with the ability to analyse root systems at larget scales than ever before, at time when largescale genomic studies have made this more important than ever.Keywords: convolutional neural network (CNN); plant phenotyping; computer vision; encoder-decoder; root systemReceived: 19 July 2019; Revised: 23 August 2019; Accepted: 22 September 2019C/circlecopyrtThe Author(s) 2019. Published by Oxford University Press. This is an Open Access article distributed under the terms of the Creative CommonsAttribution License http://creativecommons.org/licenses/by/4.0/ ), which permits unrestricted reuse, distribution, and reproduction in any medium,provided the original work is properly cited.1Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 20202 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesFigure 1: An example of the challenge root phenotyping presents for computervision. a) sample input image of Brassica napus seedling grown on germina-tion paper. This plant phenotype exhibits single primary root and numerouslateral roots. b) Cluttered scenes make segmentation challenging. c) Complex oc-clusion and intersection makes extracting root topology dif/f_icult. d) Many smallimage features, such as root tips, occur in close proximity, making identi/f_icationdif/f_icult.BackgroundPlant phenotyping plays key role in plant science research,underpinning large-scale genetic discovery and the breeding ofmore resilient traits 1]. This innovation makes fundamentalcontribution to the push for global food security. In recent yearsquantitative analysis of root growth has become increasinglyimportant as way to explore the in/f_luence of abiotic stressessuch as high temperate and drought on plant’s ability to takeup water and nutrients 2]. Segmentation and feature extrac-tion of plant roots from images presents signi/f_icant computervision challenge. Root images contain complicated structures,variations in size, background, occlusion, clutter, and variationin lighting conditions. Fig. 1shows an exemplar root image cap-tured on germination paper. Even straightforward imaging as-say presents numerous challenges to classic computer visionpipeline.In recent years machine learning has driven advancesthroughout many computer vision domains 3]. Indeed, muchof the recent progress in plant phenotyping has also been drivenby new and so-called deep learning techniques, branch of ar-ti/f_icial intelligence, often centring around convolutional neuralnetworks (CNNs) 4–6]. The sharp increase in the availability ofperformant techniques in image analysis has coincided with anincrease in the availability of genomic information in plant biol-ogy, providing an opportunity for robust and high-throughputsolutions. The scale of the data challenge seen within plantscience means that now, all but the truly fully automatic ap-proaches will quickly become bottlenecks that hinder progress[7].Analysis of root system architecturesIn this article, we focus on the analysis of root systems whereimprovements promise increases to water and nutrient use ef/f_i-ciency 8]. Historically, automated root phenotyping has provenchallenging, owing partly to the concealed nature of roots in thesoil, but also to the architectural complexity and variability ofroot systems between species, and even individuals. Progresshas been made through combination of innovative approachesand tools 9,10], and new imaging technologies such as X-ray andmagnetic resonance imaging 11,12].The prevailing methodologies in root image analysis can bebroadly categorized on the basis of the level of automation theyprovide. Fully automated tools attempt to quantify the traits of aroot system without human guidance, often through processof image segmentation followed by post-processing. These arewhat might be termed ”bottom-up” approaches, which performsuccessive /f_iltering over images in order to best distinguish be-tween the foreground root material, and the background. Toolssuch as DIRT, GiA Roots, IJ Rhizo, and EZ-Rhizo 13–16 offer afamiliar pipeline in which an image is /f_irst segmented into 2classes, root system and background, before noise removal (suchas image /f_ilters and morphology 17]) and skeletonization tech-niques 18]a eu e dt oc e nt ei a e .T e et o st e nq a -tify the distribution of root mass within an image, providingsummary statistics such as root system width, height, and morecomplex measures such as density. Some tools, e.g., EZ-Rhizo,will measure root width at each location, providing more de-tailed analysis of the distribution of roots of different sizes.A limitation of automated systems such as these is that er-rors propagate from early processing stages through to measure-ment. Noisy images or unexpected phenotypes will lead to er-rors in thresholding, which are challenging to remove and maylead to incorrect measurement of the root system. For this rea-son, most automated tools have placed heavy focus on cruderorgan-scale measurements such as the total width of the rootsystem because these are most robust to small errors in im-age segmentation. Owing to the challenge of reliably segment-ing and analysing root systems automatically, many tools placestrict requirements on the type of image they will analyse. Rhi-zoScan 19], for example, offers an automatic pipeline similar tothe above, based on the OpenAlea platform 20], but supportsonly root systems grown on Petri plates.Beyond the problem of low-level image analysis, by framingthe problem as one of identifying root pixels at low level, thesetools struggle to extract high-level root architectural informa-tion. More detailed phenotypic traits such as the number of lat-eral roots are out of reach of many existing tools simply becausedisambiguating the category of root within system may proveimpossible in the presence of noise, especially once growth is ata mature stage where roots begin to overlap. Semantically un-tangling such root system requires higher-level understand-ing of the image than pixel-based processing methods provide.Manual root analysis tools such as ImageJ’s polyline func-tion 21]a dD R T[ 22] offer an entirely different approach. Theyplace reliance on an expert human annotator to successfullyidentify the structure of the root system by asking the expertto label each root by hand. The advantage here is that if suf/f_i-ciently well trained, an annotator could conceivably reconstructan entire root system, using their advanced knowledge to clearup disambiguation in cluttered areas of the image. The obvi-ous drawback to this approach is that this is an extremely time-consuming process. In practice, many experiments will there-fore have to severely limit the number of measurements cap-tured per image, such as by focusing on primary root length, tobring the time required into reasonable range. Some tools, e.g.,RootScape 23], have been designed with this in mind, requiringthat user highlight only 20 key landmarks on root system.These landmarks are then used to explore phenotypic differ-ences between genotypes via principal component analysis. Inthose instances where detailed analysis is required, the burdenDownloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 3on annotators is huge, and the cost of mistakes may be high.Outside of plant science, obtaining cheap and ef/f_icient annota-tion has become widely researched topic in and of itself 24,25].In plant science, noisy and low-cost annotation may not be ac-ceptable, depending on the experimental requirements, and ul-timately offers few bene/f_its over the automated tools describedabove.Alongside the development of manual and automated tools,as l c i no fw d l yu e ds m - u o a i ct o sh v eb e nr -leased. These approaches aim to bridge the gap between speedand accuracy, offering compromise acceptable for many usecases. Tools such as RootReader 26] perform similar auto-matic function to the tools above but provide the user with theability to manipulate some of the output to correct mistakes.Most of the tools in this category are not bottom up, and in-stead model the root system in some way, guided by the user,in order to better understand the image on which they are run.Smartroot 10], plugin for the popular ImageJ tool 21], oper-ates by tracing along each root in guided way, at each stepsearching for the optimal direction in which to travel based onthe current orientation of the root at that point. Smartroot issemi-automatic, with initiation of roots and correction of errorsoften requiring human intervention. Nevertheless, with someuser effort Smartroot can potentially be used to reconstruct fullroot system architectures. RootNav 9], precursor to the workpresented here, offers point-to-point path search between la-belled seed locations and root tips. Images are /f_irst segmentedinto background/foreground classes, before user is required tolabel root tip and seed locations. Shortest path search is usedto trace between key organ landmarks, resulting in completereconstruction of the root system. However, RootNav does notinclude reliable method for detecting seeds and root tips (theuser must perform this step), nor is the segmentation step ro-bust to image noise. This means that signi/f_icant user interactionis still required to guide the software, but as with Smartroot, theoutput is full and architecturally correct root system architec-ture. Many tools that are able to output root system architec-tures have been adapted to provide output in the popular RSMLformat 10]. RSML is an XML-based standard for the sharing ofroot system architectures, including information on geometry,and relative position within the system. Numerous tools existto read and write RSML /f_iles, allowing customized pipelines be-tween tools, and the ability to decouple the image analysis fromthe ultimate measurement of traits, as well as view the /f_inal ar-chitecture labelling.Deep learning for root systemsThe prevailing methodology when working with images in deeplearning is the CNN. CNNs improve upon traditional machinelearning via their ability to learn not only solutions to prob-lems but also the most effective way in which to transformdata to make this goal easier. This representation learning pro-vides CNNs with unparalleled discriminative power and hasseen them quickly move into dominant position within the/f_ield of computer vision 3]. CNN is layered structure that per-forms successive image-/f_iltering operations that transform animage from traditional RGB input into new feature represen-tation. This transformation is learned during training and pro-vides the /f_inal layers of the CNN with the best possible view ofthose data from which to base decisions. The deeper into CNNdata /f_lows, the more abstracted and powerful the representationbecomes. While the initial layers may compute simple primi-tives such as edges and corners, deeper into the network fea-ture maps may highlight groups of primitives. Deeper still, fea-ture maps may contain complex arrangements of features rep-resenting real-world objects 5]. These features are learnt by theCNN training algorithms and are not hand-coded, meaning thatwith suf/f_icient training data any number of different problemscan be addressed. Within the biosciences, such networks havebeen used to perform variety of tasks ranging from classi/f_ica-tion, assigning discrete labels to images and objects 27], throughto regression problems; i.e., of directly predicting values 28]. Forroot systems, Pound et al. 29] used deep classi/f_ication networkto scan an image for probable root tip locations in 32 ×32 pixeltiles. Despite promising results, the drawback of this approachis that using small /f_ield of view, customarily called ”receptive/f_ield” within the machine learning literature, is computationallyless ef/f_icient and may produce additional false-positive resultswhere the small /f_ield of view is not suf/f_icient to distinguish trueroots from image noise. This system also only currently detectsroot tips, which means more complex traits involving other or-gans cannot be computed.Image segmentation and feature localizationThe measurement of complex phenotypic traits requires anal-ysis at /f_iner scale than that of whole-root-system traits butsensitive to more than only small selection of plant featuressuch as just root tips. To address this, the research communityhas begun to move towards networks that output richer ar-ray of information. Recent work has been based around newerCNN designs in what we term an encoder-decoder con/f_igura-tion, aimed at segmentation of images, or the location of keyfeature points. Traditional CNNs perform spatial downsamplingsuch that by the end of the network, features spatially corre-spond to the entire image, i.e., they have lost location resolution.This is ideal for classi/f_ication tasks, where decision must bemade on an image scale. This is not appropriate, however, for sit-uations in which 2D segmentation result is required. Encoder-decoders therefore upsample again from the feature space, backinto spatially high-resolution image (Fig. 2). This process canbe thought of as combining CNN with second, reversed CNNthat learns to produce images once again; these images mightbe trained to predict the locations of objects, or to segment pix-els into background and foreground classes. Encoder-decodersare being used in plant science to, among other tasks, segmentplant shoots 30,31], other plant organs 32], and /f_ill gaps in rhi-zotron images of root systems 33]. Pound et al. 5] /f_irst intro-duced the concept of heat map regression to the plant pheno-typing domain, in which segmentation output is replaced bya heat map showing likely target locations. Our development inthis article combines both of these approaches, simultaneouslysegmenting root system and predicting the likely locations ofroot tips and seeds.Automated root phenotypingWe present here new tool for the automatic analysis of rootsystems that is designed to work across wide variety ofplants and imaging conditions. Our pipeline is driven by deepencoder-decoder network, similar to that presented by Pound etal. [5] but adapted to handle higher-resolution images. The net-work is trained to simultaneously segment root material, clas-sify root type, and locate key features from which root geome-try can be derived. To our knowledge this is the /f_irst use of deeplearning to perform multi-task segmentation and localization inplant phenotyping. The output of the network is re/f_ined using anDownloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 20204 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesFigure 2: simpli/f_ied example illustrating the major components of CNN in an encoder-decoder con/f_iguration. The encoder performs combination of /f_ilteringoperations including convolutional /f_ilters, spatial downsampling, and normalization. These layers convert the original image into high-dimensi onal feature spacebut with very low spatial resolution. The decoding network performs similar layer operations but replaces downsampling with upsampling to return th ef a u erepresentation back into spatially high-resolution image.A∗shortest path algorithm to determine the most likely path ofeach root, connecting located second-order roots to appropriate/f_irst-order roots, and /f_irst-order roots back to the seed location.Full root geometry is extracted per plant and is robust to multi-ple plants and highly varied architectures. The tool outputs thestandard RSML format 34], widely supported by the community,from which root system architecture (RSA) traits can be derived.The tool also outputs the underlying segmentation masks for/f_irst- and second-order roots, from which global traits can be de-rived. An overview of the tool can be seen in Fig. 3. The system/f_irst performs pixel-wise segmentation of the image and heatmap regression to locate key features; it next extracts the roottopology via series of guided shortest-path searches before /f_i-nally extracting the entire root architecture into portable RSMLformat.We /f_irst demonstrate the performance of the tool on largewheat dataset grown on germination paper. We perform quan-titative comparison with traits measured using the originalsemi-automatic RootNav tool 9], hereby referred to as RootNav1.0, in which an expert performed detailed manual interventionto ensure accuracy. We next demonstrate the ability of RootNav2.0 to adapt to new image types with much smaller training set.We retrained the network on 200 images of Arabidopsis thalianagrown on agar plates, in which up to plants appear per im-age. We again compare quantitatively against human-labelledimages generated using RootNav 1.0. Finally, we transfer learnonce more using an even smaller, rapeseed dataset, comprisingonly 91 training images. Beyond accuracy measures, we have as-sessed our system’s performance in terms of inference time andresource ef/f_iciency to provide comparative analysis of user bur-den for root architecture analysis. The trained networks, tool,and all training datasets have been made publicly available.Data descriptionPrimary datasetOur primary dataset is composed of images of wheat Triticumaestivum L.) seedlings totalling 3,630 images of 1,900 ×2,000 pixelresolution. Images include those released by Pound et al. 5],plus additional images captured using the same methodology.Images were captured as per Atkinson et al. 35]; seeds weresieved to uniform size, sterilized, and pre-germinated beforetransfer to growth pouches in controlled environment cham-ber (12-hour photoperiod: 20◦C day, 15◦C night, with light in-tensity of 400 µmol m−2s−1photosynthetically active radia-tion). After days (with plants at the 2-leaf stage), individualpouches were transferred to copy stand for imaging using aNikon D5100 DSLR camera controlled using NKRemote software(Breeze Systems Ltd, Camberley, UK). Ground truth annotationsfor all plants were obtained using the original RootNav 1.0 soft-ware 9] and stored in RSML format 34]. Each annotation wasprovided by an expert user, and because we intended to useRootNav 1.0 as quantitative baseline for accuracy, emphasiswas placed on accuracy over speed during this process.Ground truth images for network training and validationwere generated from these RSML /f_iles by rendering appropriatesegmentation masks and heat maps. The dataset was split intotraining and validation sets totalling 2,864 and 716 images, re-spectively. An additional 50 images were held back as /f_inaltesting set. More details on this methodology can be found inthe Methods section. Example images can be found in Fig. 4a.Transfer learning datasetsOur second dataset is composed of images of Arabidopsis thalianagrown on agar plates as detailed by Wilson et al. 36]. Imagesof individual plates were acquired using near-infrared imagingutilizing the system described by Wells et al. 37]. In this sys-tem, multiple seeds are sown on each plate, and thus, unlike theprimary dataset, each image typically contained up to plants(Fig. 4b). This dataset is considerably smaller, totalling 277 im-ages, and is used as demonstration of transfer learning withour approach despite limited annotated data. The dataset wassplit into training and validation sets of 200 and 27 images, re-spectively, and as with the primary dataset, 50 holdout test im-ages were used for /f_inal quantitative evaluation.Our /f_inal dataset is composed of images of rapeseed Bras-sica napus seedlings, grown in the same system as used in theprimary dataset above. This dataset is small, containing only 120images of individual plants. Our hypothesis was that despite thereduced size, transfer learning from network trained on boththe wheat (similar image background) and Arabidopsis (similarroot system organization) datasets would lead to suf/f_icient ac-curacy. The dataset was split into training and validation sets of91 and 14 images, respectively. We used 15 holdout test imagesfor the /f_inal quantitative evaluation. Example images for the 2transfer learning datasets can be found in Fig. 4ba dc .AnalysesThis section will present comprehensive performance anal-ysis of RootNav 2.0, including quantitative evaluation ofboth the underlying segmentation approach and the root ar-chitecture extraction. We evaluate segmentation accuracy via3 common metrics, mean average pixel classi/f_ication accuracy(both global and class averages) and mean intersection overunion (mIoU). We compare the segmentation performance ofour approach against the well-known benchmark architecturesVGG 38], FCN 39], SegNet 40], UNet 41], and DeepLab 42]. WeDownloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 5Figure 3: An overview of RootNav 2.0. The input enters CNN that performs both segmentation of the root structure and localization of key points. These are post-processed to extract information for path-/f_inding algorithm. ∗search then extracts likely paths taken by each root, generating an entire architecture for an arbitrarynumber of plants in an image. All roots are resampled as smooth splines, before all topology and geometry are output into an RSML /f_ile. Segmentation mask sf r/f_i s -and second-order roots are also saved.Figure 4: Example images from each of the datasets used during this work.(a) Wheat Triticum aestivum L.). (b) Arabidopsis (Arabidopsis thaliana ). (c) Rapeseed(Brassica napus ). Scale bars are 50 mm long.then evaluate the automatic reconstruction of root systems us-ing comparison of common root phenotypic traits such asthe dimensions of the root system, and root counts. For groundtruth, we use semi-automatic measurements obtained throughexpert annotation using RootNav 1.0. Finally, we perform thesame experiments to outline the accuracy on the additionaldatasets, which contain fewer training images, to demonstratethe ef/f_icacy of transfer learning to new species and imagingmodalities.Table 1: Quantitative comparison: quantitative analysis of train-able parameters and memory requirements of different benchmarkarchitectures used during experimentsCNN modelTrainable parameters,input (3 ×256×256pixels)GPU memoryrequirements (bytes)VGG-16 38]1 8 3 7 5 4 , 5 , 4 , 2 0FCN 39]1 4 8 5 9 4 , 6 , 5 , 6 0SegNet 40]2 , 7 , 5 1 6 3 2 2 7 4UNet 41]1 , 9 , 2 1 2 6 1 6 9 2Stacked Hourglass 43]6 7 0 1 2 , 0 , 8 , 9 2LinkNet 44]1 , 4 , 4 533,725,184PSPNet 45]6 , 8 , 3 1 9 4 6 2 7 0DeepLab-V3 42]5 , 4 , 0 5 6 6 9 7 4RootNav 2.0 1,595,782 892,338,176The input size was set at constant ×256×256 pixel size for this comparison.Root image segmentationRootNav 2.0 is driven by deep network that segments imagesof root systems into classes: background, /f_irst-order roots, andsecond-order roots. Crucial to the accuracy of any subsequentpath-/f_inding approach is reliable segmentation. Segmentingwhole-root images is important in order to provide suf/f_icientcontext when distinguishing /f_irst- or second-order roots. Split-ting images into ef/f_icient tiles reduces memory consumption butmakes distinguishing root type problematic. With this in mind,we designed the network to be ef/f_icient by reducing the num-ber of trainable parameters, intermediate feature sizes, and thusoverall memory requirements. This allows larger 1,024 ×1,024resolution input. Table 1shows comparison of the memory re-quirements and parameter sizes of commonly used segmenta-tion networks, and our own architecture.We trained each network on the wheat dataset as describedin the Methods. To provide fair comparison of each network,we allocated Nvidia GPUs with >11 GB onboard memory eachfor training each network, then trained using consistent hyper-parameters such as learning rates, and equal batch sizes. ImageDownloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 20206 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesTable 2: quantitative comparison of the segmentation performanceon root images of RootNav 2.0 against other commonly used CNNarchitecturesCNN architectureGlobalaverageClassaverage mIoUVGG 38]3 . 2 8 5 1 1 1FCN 39]4 . 8 8 5 3 . 7SegNet 40]6 . 8 0 2 4 . 5UNet 41]7 . 7 9 6 5 . 8DeepLab-V3 42]8 . 8 . 0 4 0RootNav.2.0 99.6 95.1 66.1Performance is measured using global average accuracy, class average accuracy,and mean intersection over union. The classes evaluated are background (noroot), /f_irst-order, and second-order roots.resolution was maximized for each network depending on itsresource requirements. Accuracy was measured using stan-dard metrics: Global average accuracy, class average accuracy,and mIoU. Global average accuracy measures the performanceof segmentation over all pixels in the validation set. High val-ues indicate that the majority of pixels have been classi/f_ied cor-rectly. Because most pixels are background in root images, highvalues indicate few false-positive results but do not necessar-ily demonstrate good root segmentation. Class average accuracymeasures the performance of each class separately, before com-puting /f_inal average. High values here represent good perfor-mance across all classes. Finally, mIoU represents the percent-age of overlap between each class and the ground truth. Highervalues indicate predictions closer to that of the ground truth la-belling.Example image output from each network can be found inFig. 5, with quantitative results for all tested networks acrossthe validation set shown in Table 2.T el r e rn t o k sc n -tain more features, which while in some cases may improve per-formance of deep network, here hinders the ability of eachnetwork to resolve /f_iner detail because they cannot operate at1-megapixel image resolution. The strong performance of Root-Nav 2.0 in this experiment can be attributed to its ef/f_icient use offeatures throughout the network, lower memory requirements,and thus larger 1-megapixel input sizes.Extraction of root system architectureAfter segmentation and feature localization, segmentationmasks are converted into weighted graph structure amenableto traversal with shortest-path algorithm. RootNav 2.0 ex-tracts full root architecture by performing series of heuris-tic searches across the image. First, shortest paths are foundbetween all /f_irst-order root tips and the most appropriate seedlocation (de/f_ined as the seed /f_irst reached during heuristicsearch). This generates series of /f_irst-order roots, to whichsecond-order root paths are found from all second-order roottips. The output of this process is complete root architec-ture description, stored in RSML format, from which phenotypictraits can be derived. We compare the output of RootNav 2.0against ground truth measurements captured using RootNav 1.0in collaboration with an expert user. Quantitative traits weremeasured directly using the RSML output by both tools; resultsare presented in Fig. 6.We chose range of root traits that are both representative ofthe measurements commonly used in the root phenotyping lit-erature but also ones that exercise various aspects of our partic-ular approach. For example, we include traits that measure theaccuracy of feature detection (e.g., total root count) and thosethat also measure the accuracy of the shortest-path approach(e.g., total root length). In Fig. 6it can be seen that there is strongagreement between the results of RootNav 2.0 and the groundtruth measurements. Measurements based on the extremitiesof the root system (maximum depth, maximum width, and con-vex hull area) produced values very close to those in the groundtruth, with r2values >0.99. Traits that summarize the entire rootsystem, such as centroid depth, provided r2values in the range0.64–0.72.The prediction of /f_irst- and second-order root countsachieved r2values of 0.641 and 0.724, respectively. For /f_irst-orderroots, we observed that the majority of incorrect predictionswere either count higher or count lower than the groundtruth and that these confusions often occurred near the seedposition, where seminal root may visually appear similar toa second-order root that emerges near the seed, or vice versa.Other failures were produced by roots leaving the /f_ield of viewof the camera, but that had been annotated by the expert, orwhere root tips grew in very close proximity (within few pix-els). Second-order roots were typically much shorter and often inclose proximity. Some missed root tips would be caused by non-maximal suppression, when the R-Tree data structure is used toremove possible duplicates. We also found that the contrast onsecond-order roots was lower because they were usually thinner,which might account for some missed tips in this class. Errors inthe detection of root tips will also propagate errors into the totalroot length measurements because these roots will not be de-tected. For second-order roots, we found that most of the errorin root length can be attributed to missed roots rather than er-rors in path /f_inding. For primary roots, path /f_inding was usuallyrobust, except in cases where roots grow side by side. RootNav1.0 handled these errors by allowing user to intervene and cor-rect any mistakes; in RootNav 2.0 we wish the process to remainfully automatic, so we do not explicitly correct for this. However,the occurrence of this type of growth is in the minority, in ourexperience. Centroid depth is measured as the mean position ofall roots and so is in/f_luenced by the detection and path /f_indingof every root.An understanding of where and how RootNav 2.0 may pro-duce errors provides insight into these results. An accurate mea-surement of maximum depth depends on only variables: thelocation of the seed, and the location of the /f_irst-order root tipthat is lowest (in terms of y-position) in the image. The graphof maximum depth in Fig. 6re/f_lects the fact that these fea-tures were successfully found in every case. Similarly, maximumwidth depends only on the left- and right-most roots, and con-vex hull only on the outermost roots throughout the architec-ture. missed second-order root within root system will notaffect these traits, so these results are robust even where someroots have been missed. This tells us that for the majority of im-ages, the locations of the seeds, lowest tips, and outermost rootsare detected successfully and that these traits that measure theextremities of the root system are robust.Transfer learning to new species and imagesTo demonstrate the adaptability of our approach to differentspecies and imaging modalities, we retrained the network /f_irston an Arabidopsis dataset, comprising ∼277 images of A. thalianagrown on agar plates. We then trained once more from the wheatdataset to the rapeseed dataset, comprising 120 images of B.napus on germination paper. In both cases we extracted RSMLDownloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 7Figure 5: Example image output from each trained network architecture. (a) An example hydroponic wheat image. (b) VGG 38]. (c) FCN 39]. (d) SegNet 40]. (e) UNet[41]. (f) DeepLab-V3 42]. (g) RootNav 2.0.Figure 6: Numerical results showing range of root system traits measured in RootNav 2.0 against ground truth measurements on the wheat test set. For each traitwe also /f_it linear regression model and report the r2value.root descriptions and quanti/f_ied these in the same way as thewheat dataset. We also trained both networks from randomlyinitialized weights, rather than transfer learning, and found thatthe datasets were too small to train effectively (SupplementaryFigs 6and 7).Arabidopsis thalianaThe Arabidopsis dataset contains marked differences from thewheat data. The dataset contains many fewer images, whichmakes transfer learning essential to reduce over/f_itting. Thisspecies has taproot structure that contains single primaryroot, from which lateral roots emerge, rather than multiple/f_irst-order roots in the form of primary and multiple semi-nal roots. This dataset is also imaged under infrared illumina-tion and so contains no colour information, and very differentbackground arrangement consisting of plastic plate contain-ing semi-transparent growth medium instead of blue germina-tion paper. Finally, each plate typically contains plants ratherthan single plant. We found that the network and heuris-tic searches adapted well to this new domain. We made minormodi/f_ications to the path-/f_inding approach to support multipleplants, which are discussed further in the Methods. Quantita-tive results are shown in Fig. 7, with full results found in Sup-plementary Fig. 1and example image output in SupplementaryFig.4.Despite the smaller number of images available for training,the results show good performance after transfer learning tothe new data. Over/f_itting on the smaller dataset led to nois-ier segmentation and feature detection on some instances ofthe test data, examples of which can be found in Supplemen-tary Fig. 8. Not every plant was successfully detected; missingor additional primary root tip or seed location would mean thatthe number of plants was under- or overestimated. In 60% ofimages examined, the tool correctly identi/f_ied the same num-ber of plants as were marked in the ground truth. In 6% of theimages, single plant was missed, usually owing to the plantbeing extremely underdeveloped but having been annotated bythe user anyway. We found only single instance in imagethat contained well-established plant that had not been iden-ti/f_ied by our network. Overcounting of plants was more com-mon, with 20% of images identifying an additional plant and14% identifying more beyond this. In the majority of cases wefound that these errors were caused by unusual angles in theleaves and germinated seeds at the top of the plant, producingfalse-positive detections. This is something that would likely beDownloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 20208 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesFigure 7: Numerical results showing sample of root system traits measured inRootNav 2.0 against ground truth measurements on the Arabidopsis test set. Eachimage contains up to plants, and results are presented per plant. For each traitwe also /f_it linear regression model and report the r2value.Figure 8: Numerical results showing sample of root system traits measured inRootNav 2.0 against ground truth measurements on the Brassica napus test set.For each trait we also /f_it linear regression model and report the r2value.corrected with additional training data; remember, we are us-ing very small amount of training data for this image class,versus the wheat images. Where duplicate plants were found,they were often extremely close to, or even above, an existingplant location. These duplicates could be removed easily viapost-processing; this is something we do not address in this ar-ticle.Of the plants that were successfully identi/f_ied, the traits cap-tured by the tool offer good agreement with the ground truth.As with the wheat dataset, measures of the extremities of theroot system such as maximum depth performed with the high-est accuracy, but we also found that total /f_irst-order root lengthwas very close to the ground truth in the majority of cases. Er-rors here usually indicated second primary root incorrectly de-tected alongside an existing one, feature that we do not yet re-move in post-processing as with duplicate plants, although thiswould be possible. The detection of second-order roots was alsohighly correlated with the ground truth measurements, and thetotal length of all second-order roots (measured per plant) cor-related with the ground truth with an r2of 0.91.Brassica napusThis dataset uses the imaging format of blue germination pa-per with single plants (like the wheat dataset) but contains thesame species root structure as Arabidopsis (a single taproot fromwhich all other roots derive). This dataset contains the fewestimages, with only 90 images used for training. We use this smalldataset as demonstration of the ef/f_icacy of transfer learning,but we also note that training over slightly larger dataset inpractice would be worthwhile for improving robustness. Resultscan be found in Fig. 8, and in full in Supplementary Fig. 2.A swith the Arabidopsis dataset, the low number of training imagesproduces some over/f_itting that results in noisier output than thewheat dataset on some of the test data. Examples can be seenin Supplementary Fig. 9.It can be seen that the correlation between RootNav 2.0 andground truth ranges from r2of 0.539 (/f_irst-order root length) to0.941 (convex hull area). Because this is such small dataset,the test set contains only 15 images, meaning that there isinevitably more noise in the results than in the previous ex-periments. Nevertheless, the results are promising, particularlygiven the tiny size of this dataset compared with typical stan-dards for deep learning. As with both previous datasets, convexhull and other extremity-based measures provided the most re-liable results. Accuracy of the total length and root count met-rics had lower r2than the other datasets, caused we believe bythe smaller training set, meaning that the approach is slightlyless robust to noise. We found that in few images the longestlateral tips were incorrectly classi/f_ied as /f_irst order, causing er-roneously high measures of /f_irst-order root length. We believethat this occurred where these laterals are mistaken for wheatseminal roots, on which the network was originally trained.This occurred on the minority of test images and is prob-lem that we are con/f_ident would be resolved with more trainingdata. It would be possible to use prior domain knowledge, e.g.,the knowledge that rapeseed has single taproot, to clean theoutput during post-processing; as with the Arabidopsis dataset,we did not perform any post-processing of this kind in thiswork.After segmentation, the structure of these root systems wasquite amenable to traversal using shortest-path approach. Inmany cases the longest roots grow close together, which causeserrors where search may travel along the same path as anotherroot. We found that this did not substantially increase the errorin total root length because many of these roots grew in closeproximity and were of similar length. Nevertheless, dealing withroot overlap in an ef/f_icient and automatic way is topic worthexploring in future work.Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 9Figure 9: Sample training image and ground truth, zoomed sections and colour added for clarity. (a) An example wheat image. (b) Segmentation mask for /f_irst-orde rroots. (c) Segmentation mask for second-order roots. (d–f) Heat maps for seed, /f_irst-, and second-order tip locations.Performance analysesWe measured the time taken for both tools to complete the fullpipeline, from image to RSML output. We timed RootNav 1.0 byannotating random images from each test set from scratch; thetotal time taken to annotate 10 images of wheat and Arabidop-sisand of rapeseed was recorded, and averages computed. Theannotation was performed by an expert user who ha many yearsof familiarity with the tool. For RootNav 2.0, we processed eachtest set and then calculated the average inference time per im-age. For all tests each image was annotated in suf/f_icient detailto measure the traits seen in Figs 6–8. Results can be found inTable 3.In both systems more complex root architecture typicallyleads to longer analysis time. In RootNav 1.0 this is due to thehuman input required; with RootNav 2.0 the path /f_inding takeslonger if there are more lateral roots or roots are longer. On someimages such as those in the rapeseed dataset, RootNav 1.0 wouldtake less time to process each image if only course traits suchas /f_irst-order root length were required because signi/f_icant usertime is taken in annotating and correcting second-order root po-sitions. We anticipate that RootNav 2.0 will be used to measureas many traits as possible automatically and so present herea like-for-like comparison where both tools are used to mea-sure the same features of each root architecture. In this com-parison, RootNav 2.0 offers substantial speed advantage overthe original tool on each dataset. It should also be noted thatthe time presented here for RootNav 1.0 requires that the userengage with the software continuously. Because RootNav 2.0is fully automatic, the human time cost is essentially zero be-cause images could be batch-processed overnight. This test wasalso run on single CPU and GPU, where additional computa-tional resource would linearly scale the speed of the system.If performance were serious consideration, dedicated paral-lel hardware set-up could streamline RootNav 2.0 performanceconsiderably.DiscussionIn this article we have introduced RootNav 2.0, state-of-the-art,fully automated root-phenotyping tool. It is powered by deepCNN in an encoder-decoder con/f_iguration, designed to performsegmentation ef/f_iciently in high-resolution images. The networksegments root from background and can distinguish /f_irst- andsecond-order roots. This deep learned root segmentation pro-vides strong foundation upon which users can derive commonarchitectural traits, such as those based on RSA skeletonization.We have adapted the network, however, to simultaneously pre-dict the location of key root architectural features: the seed loca-tion, and /f_irst- and second-order root tips. This knowledge thendrives heuristic search that reconstructs the entire root sys-tem. This topology is represented as spline curves, and outputin RSML format.A quantitative analysis of RootNav 2.0 shows that it offerscomparable accuracy against the original RootNav on large train-ing sets. Over range of standard trait measurements the newtool produced highly correlated results against the ground truth,with r2values ranging from 0.64 to 1. Performance on traitsrepresenting the bounds of the root system yielded among thehighest r2values. On smaller datasets, we have demonstratedthat transfer learning produces accurate results despite manyfewer training examples. This adaptability is key advantagefor those within the research community looking to use Root-Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 202010 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesTable 3: Performance comparison of RootNav 1.0 against RootNav 2.0DatasetAverage processing time (s)RootNav 1.0 RootNav 2.0Wheat 68.8 8.2Arabidposis 109.8 7.5Brassica 132.4 14.0The time to process random sample of images from each test set was measured, and an average time per image calculated.Nav 2.0; those who use different growth conditions, image cap-ture approaches, or require the analysis of different species canadapt one of our existing trained models with minimum of ef-fort using transfer learning.While the accuracy of the fully automatic approach here doesnot yet match human-annotated approach across all traits, webelieve that this system still offers bene/f_it over all existing ap-proaches for forward genetic screens on root system architec-ture, such as quantitative trait locus (QTL) analysis or genome-wide association studies (GWAS). RootNav 2.0 is substantiallymore convenient to use than previous semi-automated tools,with no human interaction required at any point during thepipeline. The entire process requires <15 seconds processingtime per image. The output can be analysed using the Root-Nav viewer tool or any compatible RSML analysis pipeline. Train-ing the original network took few days on suitable hardware,with transfer learning to new dataset typically taking abouthalf day. The speed of the system allows many more imagesto be processed, images that may represent additional speciesor more replicates. Previous work has shown that larger sam-ple sizes have positive effect on the performance of both QTLanalysis and GWAS 46,47]a dt a tg o dr s l sc nb eo t i e don even simple automatic measures compared with existingsemi-automatic approaches 29]. We believe that RootNav 2.0will prove to be key milestone in root phenotyping, further en-couraging the uptake of machine learning in addressing theseimportant challenges.In future work, we will continue to adapt this approach tonew and varied datasets, maximizing the potential for use in theresearch community. We will also explore the use of more ro-bust heuristic searches, combined with appropriate segmenta-tion output from the network, to address the challenge of cross-ing and intersecting root systems. We will also continue to ex-plore deep network developments at the core of the tool, with aview to closing the gap between automatic and semi-automaticapproaches on the most challenging traits. To encourage com-munity uptake and engagement, we will also develop mecha-nisms to ease the sharing of network models, and indeed theretraining process required to adapt them to speci/f_ic scenarios.Potential implicationsWe believe that RootNav 2.0 offers substantial increase in accu-racy over bottom-up approaches to root image analysis. It alsooffers an increase in throughput over existing semi-automatictools. Importantly, results on the Arabidopisis dataset suggestthat the approach will be applicable to images obtained withother phenotyping systems such as rhizotrons. With continuedcommunity support, RootNav 2.0 has the potential to be the/f_irst true species—and platform—agnostic analysis tool in theplant sciences. This will provide researchers with the ability toanalyse root systems at larger scales than ever before, at timewhen large-scale genomic studies have made this more impor-tant than ever.MethodsTraining, validation, and test image preparationFor each image we obtained ground truth annotations using theoriginal RootNav 1.0 software. This software is semi-automaticand allows users to manually intervene to correct errors in eithersegmentation or RSA extraction. We used these data as groundtruth, rather than to evaluate the accuracy of RootNav 1.0, andas such annotators were instructed to spend suf/f_icient time oneach image to correct all mistakes that they could identify. Thissemi-automatic process often requires large amount of humaninteraction and is time consuming, but the approach has pro-vided very reliable ground truth annotations. All ground truthwas stored in RSML format.RSML data for each image were converted into series ofsegmentation masks and feature heat maps for use in training.Segmentation masks were created separately for both /f_irst- andsecond-order roots by rendering them as polylines over blankimage. RootNav 1.0 does not measure diameter information forroot systems, but the seedlines are sufUbb/f_iciently young thatroot diameter is quite consistent across species and images. Werendered each root with width of pixels. For heat map out-put, the seed location and /f_irst- and second-order root tip loca-tions were rendered as in Pound et al. 5], as separate images ofblurred Gaussian points of standard deviation 1.0 pixels. The re-sult of these processes is that for each input image there were5a s c a e do t u ti a e ,2s g e t t o nm s sf r/f_i s -a dsecond-order roots, and heat maps for seed position and /f_irst-and second-order root tips (Fig. 9).At this point, we have constructed suitable training sets ofimages based on manual annotations. The next task is to con-struct suitable encoder-decoder architecture capable of seg-menting these images and locating root features.CNN DesignInput and output resolutionWe used the PyTorch 48]f a e o kt od v l pt en t o k ,training, and validation code that drives our segmentation ap-proach. The network is based around an encoder-decoder ar-chitecture (Fig. 2)b th sb e na a t dt oh n l et eh g e -resolution images seen in the datasets. Encoder-decoder CNNsare memory intensive, particularly at points towards the startand end of the network where the spatial resolution is high.Each layer calculates many features, each of which exists as animage stored in memory. Over many layers, the computationalcost becomes prohibitive. Previous work, such as that by Poundet al. 5], used small input and output sizes of 256 ×256 pixels.Other commonly used networks such as VGG-FCN 39]a dU -Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 11Table 4: The proposed CNN’s layersLayer architecture Dimensions Feature mapsInput (RGB) 1,024 ×1,024 3Convolution (7 ×7) 512 ×512 64Residual block 512 ×512 128Maximum pooling 256 ×256 128Residual block 256 ×256 128Maximum pooling 128 ×128 128Residual plock 128 ×128 256Hourglass (Block =1) 128 ×128 256Transposed convolution 256 ×256 256ResNet block 256 ×256 256Transposed convolution 512 ×512 256ResNet block 512 ×512 128Convolution (1 ×1) 512 ×512 128Network splitConvolution (1 ×1) Convolution(1×1)512×512 3Softmax Convolution(1×1)512×512 3Batch normalization layers and ReLU activation functions are used between lay-ers and within residual blocks. The hourglass used is equivalent to single stackof the type used by Pound et al. 5].Net 41] use similar input sizes. Root images pose challenge inthis situation because roots may be only few pixels in diame-ter but exist as part of large, connected architecture coveringmany megapixels. Shrinking the image to convenient size willmake processing simpler but also badly degrade the quality ofthese small features. In scenarios such as this, where shrinkingthe input this far may represent signi/f_icant loss in quality, itis common to tile the input into small cropped sections and runthe network repeatedly. This is the approach taken by Pound etal. [5], in which wheat images are tiled, processed, and then re-constructed. The drawback of tiling images is that each tile isthen considered in isolation, removing vital context on its posi-tion in the wider image. In the root datasets, for example, /f_irst-and second-order roots often appear identical when not viewedas part of larger architecture.In this work, we limit the size of input images to 1,024 ×1,024 pixels. For the wheat and rapeseed datasets, this neces-sitated downsampling of the input and output images, but onlyby moderate amount, in which /f_ine root detail is preserved. FortheArabidopsis plate images, no downsampling was required be-cause they were already of suitable size. Upon completion ofthe deep learning, images are returned to native resolution toensure that the output measurement scale is preserved.Network architectureOur complete network operates on input images of 1,024 ×1,024pixels and outputs segmentation masks and regression maps of512×512 pixels. diagrammatic overview of the network can befound in Fig. 10, with description of the layers in Table 4.T ecore of the network is an hourglass architecture similar to thoseused by Pound et al. 5] and Newell et al. 43], but here we use arestricted number of features throughout and do not use stackedstructure (repeated encoder-decoders after one another). Thesealterations to the network allow it to successfully process the1-megapixel input size without reaching the limit of availablememory. We also perform additional downsampling and upsam-pling at either end of the network. Initial strided convolutionallayers with large /f_ilter sizes of ×7a eu e dt oe t a tf a -tures and downsample the image size, before interleaved resid-ual blocks and maximum-pooling operations are used to furtherreduce the spatial size of the input to 128 ×128 pixels. The hour-glass architecture performs the primary encoder-decoder role,with downsampling performed using maximum pooling and up-sampling performed using bilinear interpolation. The output ofthe hourglass is set of 128 ×128 pixel feature maps, after whichlearned deconvolutional /f_ilters and residual blocks are used toreturn to 512 ×512 pixel spatial resolution. Finally, pathsare used to separately predict segmentation masks and featureheat maps. Each branch comprises ×1 convolutional layers forprediction, with the segmentation output also passed througha sigmoid output, as required by the binary cross entropy lossfunction.Loss functionsThe output of our network is divided into paths with differ-ent objectives. The /f_irst outputs segmentation masks contain-ing locations for /f_irst- and second-order roots. Each of these isa 2D binary output and is trained using cross-entropy loss. Itis common in root images that the number of background pix-els heavily outweigh the foreground. Calculating loss over anunbalanced dataset such as this is likely to cause bias towardsbackground pixels, causing error and undersegmentation of theforeground. We apply class-balancing approach to the stan-dard cross-entropy loss, based on median frequency balancing[49]. Weights are assigned to each class inversely proportional tothe median frequency in which that class appears throughoutthe entire training set. This reduces the weight for classes thatappear more often, in this case background, and increases theweight of foreground classes such as /f_irst-order roots. The accu-racy of classes with higher weights is prioritized during training.The proposed loss L1is given in Equation 1).L1=αcN/summationdisplayNn=1/summationdisplayWx=1/summationdisplayHy=1/bracketleftbigggnxylog/parenleftBigˆgnxy/parenrightBig+/parenleftBig1−gnxy/parenrightBiglog/parenleftBig1−ˆgnxy/parenrightBig/bracketrightbigg,(1)where for Nfeatures, gnxyis the predicted class output at loca-tion x,y)a d gnxyis the ground truth prediction at that location.The weight of each class is scaled by its frequency relative to themedian frequency of all classes by αc,g v nb yαc=median freqfreq( c), (2)where freq( c) is the frequency of occurrences of pixels of classcdivided by the number of pixels in any image containing thatclass and median freq is the median of these frequencies overall classes.The second path is responsible for predicting key feature lo-cations on the root system, speci/f_ically, the seed location, /f_irst-order root tips, and second-order root tips. The output is three2D outputs, trained using mean squared error loss, predictinglikely locations for root features, represented by 2D Gaussianscentred at each feature location. The proposed loss L2is depictedin Equation 3).L2=αcN/summationdisplayNn=1/summationdisplayWx=1/summationdisplayHy=1/vextenddouble/vextenddouble/vextenddouble/vextenddoublepnxy−ˆpnxy/vextenddouble/vextenddouble/vextenddouble/vextenddouble2, (3)Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 202012 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesFigure 10: The network used is an extended encoder-decoder architecture. The input is /f_iltered and downsampled into an ef/f_icient size, before an hourglass networ k[43]performs the remaining encoding and some decoding. Finally, series of learned deconvolutional layers upsample back to 50% of the original input siz e. The networkis split into fully convolutional branches at the output, which are responsible for learning the segmentation and heat map regression outputs separ ately.where for each of Nfeatures, pnxyis the predicted feature likeli-hood at pixel x,y)a d pnxyis the expected ground truth at thesame location.The /f_inal loss L=L1+L2trains the network end to end, bal-ancing the objectives of both paths. We found that additionalscaling factors applied to the loss of either path were not neces-sary for accurate training.TrainingBeginning with the wheat dataset, the network was trained endto end from scratch using the rmsprop optimizer. The initiallearning rate was set to e−4and reduced by factor of 10 af-ter 50,000 iterations. The network was trained using batch sizeof for 500,000 iterations, although we found that performanceplateaued after approximately 400,000–450,000 iterations. Dur-ing training, we selected the best-performing model from thevalidation set.We applied random augmentation to the training set to re-duce potential over/f_itting. We added random horizontal /f_lippingwith 50% probability during training, as well as random ro-tation in the range −30◦,3 0◦]. We experimented with randomcropping as in Pound et al. 5] but found that cropping oftencaused the removal of parts of the root system, sacri/f_icing con-text crucial, e.g., in distinguishing /f_irst- and second-order roots.For this reason we did not use random cropping during theseexperiments.Transfer learningTransfer learning is the process of training on new data by begin-ning with an existing trained network’s parameters, rather thanrandomly initialized weights. We began by training the wheatnetwork to completion. This is large dataset, with more thansuf/f_icient images to train network reliably from scratch. Asnoted in the above section, successful training simply means ac-ceptable performance on the validation images. We began withthe existing wheat network and retrained on the A. thalianadataset. This is smaller dataset, but the use of pre-trainedweights allows network to make use of any useful image /f_il-ters learned during the initial training. We experimented withtraining from scratch on the smaller dataset but found thatwe were unable to train network that performed reliably onthe validation set (Supplementary Figs 6and 7). The same pro-cess of training and validation was used to complete trainingon the new dataset, except that we limited training durationto 120,000 iterations. Normally minor modi/f_ications of the un-derlying deep network would be required to support single-channel near-infrared images rather than RGB. However, to en-sure compatibility between models, in particular to simplifytransfer learning between images that may move between RGBand single channel, we chose to /f_ix the network and use in-put channels in all cases. For the Arabidopsis dataset, we dupli-cate the grayscale channel into RGB channels prior to use. Thecomputational overhead of this only affects the weights in the/f_irst layer of very deep network and is marginal. The result isthat the Arabidopsis model does not make use of the additionalchannel information in the /f_irst layer but remains structurallyidentical to the other models.Finally, we repeated transfer learning from the wheat datasetto the B. napus data, which have the fewest images of thedatasets we use. We explored training from scratch, as well asusing pre-trained weights from either the wheat or arabidopsisdatasets, and found that the wheat network offered the mostreliable starting point, fact we attribute to the similar back-ground and foreground colours, and scales for both datasets. Asabove, we trained for 120,000 iterations and selected the modelwith highest validation performance.Post-processingDense CRFEach segmentation mask was passed to dense conditional ran-dom /f_ield (CRF) to improve smoothness and maximize agree-ment between similar neighbouring pixels. We found that thisapproach had subtle but helpful effect on the separation be-tween roots growing in close proximity, and the smoothness ofthe boundaries of segmented roots. We used the dense CRF pro-posed by Kr ¨ahenb ¨uhl and Koltun 50], in which each pair of ran-dom variables (pixels) are connected by an edge 51], weightedusing Gaussian pairwise potential. The effect was smoothingof con/f_licting regions of pixels where the image was cluttered;but where segmentation was already successful, the approachhad no notable negative effect on the results.Feature localization and non-maximal suppressionThe heat map regression output contains the probable loca-tions of the seed, as well as /f_irst- and second-order root tips.These are represented as 2D Gaussian distributions, the cen-tre of which lies on likely feature locations. We obtain dis-crete location for each feature via non-maximal suppression(NMS) approach 52], which suppresses all predicted pixels ex-cept those that are greater than their surrounding neighbours.Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 13For Gaussian-based distribution, this has the effect of locat-ing the centre of the distribution. Our implementation of NMSuses pixel-wise search, immediately discounting any pixel out-put below pre-de/f_ined threshold (in our case, 0.7), for speed.Each pixel is then compared with its neighbour pixels in 3 ×3window, where the central pixel “c” is non-maximal; if anotherpixel of greater or equal intensity is discovered in its neighbour-hood, the algorithm skips to the next pixel in the scan line 53].For the majority of root tips in isolation, NMS will success-fully return single location for each true root tip. This re-lies on the heat map regression layers of the network return-ing well-formed Gaussian distributions in all instances, whichwhile likely, may not occur in the presence of image clutter,confusing root hairs, or multiple tips in close proximity. Toavoid positions being returned for single underlying rootfeature, we identify and suppress neighbouring features. Weuse an R-Tree data structure to ef/f_iciently query for neigh-bours within close proximity. When NMS returns new posi-tion on the image, the R-Tree is searched for nearby featuresthat have already been added and prevents locations from be-ing added twice. In our experiments, we considered new po-sition duplicate if it fell within pixels radius of an exist-ing feature, which is derived from the scale of the roots in ourdatasets.Root architecture reconstructionAfter successful pixel-wise segmentation of the complete rootsystem architecture and extraction of the tips and seed loca-tions, we are now able to reconstruct the whole root skeleton.This procedure is similar to the original RootNav 1.0 tool 9], ex-cept it is now driven by more accurate class-aware segmenta-tion, rather than error-prone root likelihood estimations. Root-Nav 2.0 can place more reliance on the accuracy of the segmen-tation and make use of each segmentation map separately toensure that roots are not traversed over the wrong material,e.g., that /f_irst-order roots prioritize image locations of that class.We establish an 8-way connected graph structure throughoutthe image, where the weights travelling to neighbouring pix-els are calculated as function of their class, the path we aretrying to /f_ind, and the distance between them. Each segmenta-tion mask is converted into distance map of values [0, 1] in-dicating the distance from any background pixel. We then con-vert this distance into weighting that prioritizes paths alongroot centres; the maximum weight we assign to any root pixel is0.1, for pixels near the root edge. The weight decreases towardsthe centre of the root, to minimum of 0.01. Because the graphincludes diagonal connections, these are weighted by an addi-tional cost of√2 to account for the longer distance. Finally, anypixel that does not belong to the speci/f_ic class being traversed,e.g., /f_irst-order root only, is assigned weight of 10.0, represent-ing much stronger penalty for traversing these pixels. UnlikeRootNav 1.0, we use separate graphs and searches for /f_irst- andsecond-order roots. value of 10.0 was chosen simply as verylarge increase in weight when compared to the minimum costfor any segmented root material. Different weight values are ef-fective, as long as they are large enough relative to root materialto avoid the shortest path taking shortcuts across backgroundpixels where this is unnecessary. In practice, these weights areonly traversed if there is gap in the segmentation for true rootmaterial.A∗search 54] is path-/f_inding algorithm that in our imple-mentation seeks to /f_ind path of minimal cost between loca-tions on root system. It is an extension of Dijkstra’s shortest-Figure 11: Example output from RootNav 2.0. (a) Input image. (b) Colour-codedsegmentation mask. (c, d) Binary segmentation masks for /f_irst- and second-orderroots. (e) sample of the RSML /f_ile representing the entire architecture.path algorithm 55], and along with distance travelled also con-siders heuristic measure of the remaining distance to the goal.Pixels are explored based on the lowest cost /f_irst, in order to min-imize the functionf(p)=g(p)+h(p), (4)where g(p) is the sum of all weights to p,a d h(p)i st er m i i gdistance, which we calculate as the Manhattan distance, or L1-norm.In the case of RSA traversal, minimal cost paths between keyfeatures such as /f_irst-order root tips and seeds represent recon-structed roots. A∗searches are initialized from all /f_irst-order roottips, travelling along segmented roots until they reach any seedpoint. Upon reaching seed location, the entire path is recordedas /f_irst-order root. Once all /f_irst-order roots have been tra-versed, new series of searches are begun from second-orderroot tip locations, ending at any encountered /f_irst-order root.The second-order root searches use Dijkstra’s algorithm by notincluding remaining distance heuristic h(p), which would beinef/f_icient to calculate over many possible goal locations. Theoutput of each search is list of pixel coordinates representingthe individual roots within the RSA.Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 202014 RootNav 2.0: Deep learning for automatic navigation of complex plant root architecturesThe Arabidopsis dataset contains multiple plants per image,which can be handled through minor modi/f_ications to the path-/f_inding approach. Where the deep network detects multiple seedlocations, these are used to initialize multiple candidate plants.We use Dijkstra’s algorithm rather than ∗as with second-orderroot searches with multiple candidate goal locations. First-orderroot searches proceed as above but now terminate upon encoun-tering the /f_irst seed, which in Dijkstra search represents theshortest possible path between that tip and any of the seeds.This root path is then assigned to the corresponding plant, andthe process is repeated for each /f_irst-order tip location. Oncethe process is complete, any remaining plants whose seed lo-cations did not result in connection with /f_irst-order root areremoved.Spline /f_ittingThe use of distance map that prioritizes the centre lines ofroots generally acts to smooth the paths found throughout theroot system. This may not be the case where there is noise inthe segmentation output, or roots cross, and the distance map isless reliable. We smooth each root path using spline curve rep-resentation. Control points are sampled at equal spacing alongeach path, before the path is resampled using cubic splines. Eachspline includes tension parameter that we set at constant of0.5 for our experiments. Both the spline and polyline represen-tation are output into the /f_inal RSML /f_ile to ensure maximumcompatibility with other tools.RSML and outputThe RSA reconstruction approach in RootNav 2.0 does notperform phenotypic measurements itself; rather it extracts aroot topology along with segmented images from which traitscan be derived. The entire root system for each plant in animage is exported using the RSML format 34], providing astandard and interoperable format. RSML is an XML docu-ment speci/f_ically designed to store 2D and 3D root architec-tures. It also stores metadata and plant properties and is com-patible with numerous analysis tools. Some existing plant-phenotyping tools offer RSML import support, meaning thatthey may also load root systems created automatically us-ing RootNav 2.0. Our approach also outputs /f_irst- and second-order root segmentation images, representing an alternativesource of quantitative data. Many tools such as Ez-Rhizo op-erate on such images, but the segmentation masks gener-ated here contain very little image noise, making them moreamenable to further automated analysis. An example outputcan be seen in Fig. 11.F rt i sw r kw ep r o m dq a t /f_i a -tion entirely using the RSML output. Phenotypic measurementswere calculated from each RSML /f_ile using the existing Root-Nav Viewer tool, which has been extended and updated for thispublication.Availability of Source Code and RequirementsProject name: RootNav 2.0Project home page: https://github.com/robail-yasrab/RootNav-2.0SciCrunch RRID:SCR 015584biotoolsID: biotools:Rootnav 2.0Operating system(s): Platform independentProgramming language: Python, C# (RootNav Viewer)Other requirements: Python 3.6, PyTorch 1.0.1License: GNU GPLAny restrictions to use by non-academics: NoneAvailability of Supporting Data and MaterialsDatasets will be made available at https://plantimages.nottingham.ac.uk Snapshots of our code and other supporting data areavailable in the GigaScience repository, GigaDB 56].Additional FilesAdditional Figure S1: Extended plots showing quantitative re-sults on the Arabidopsis thaliana dataset.Additional Figure S2: Extended plots showing quantitative re-sults on the Brassical napus dataset.Additional Figure S3: Example output of RootNav 2.0 on thewheat Triticum aestivum L.) dataset.Additional Figure S4: Example output of RootNav 2.0 on the Ara-bidopsis thaliana dataset.Additional Figure S5: Example output of RootNav 2.0 on the Bras-sica napus dataset.Additional Figure S6: Example output on the Arabidopsisdataset showing the results of transfer learning vs. the resultswhen the network is trained from scratch (randomly initialisedweights).Additional Figure S7: Example output on the Brassica napusdataset showing the results of transfer learning vs. the resultswhen the network is trained from scratch (randomly initialisedweights).Additional Figure S8: Examples of incorrect segmentation androot system extraction on the Arabidopsis dataset.Additional Figure S9: Examples of incorrect segmenta-tion and root system extraction on the Brassica napusdataset.AbbreviationsCNN: convolutional neural network; CRF: conditional random/f_ield; DART: Data Analysis of Root Tracings; DIRT: Digital Imag-ing of Root Traits; DSLR: digital single-lens re/f_lex; FCN: fullyconvolutional network; GPU: graphical processing unit; GWAS:genome-wide association study; mIoU: mean Intersection overUnion; NMS: non-maximal suppression; QTL: quantitativetrait locus; RSA: root system architecture; RSML: Root SystemMarkup Language;Competing InterestsThe authors declare that they have no competing interests.FundingThis work was supported by the Biotechnology and Biologi-cal Sciences Research Council [grant numbers BB/P026834/1,BB/M019837/1].Authors’ ContributionsM.P.P., A.P.F. and T.P.P. designed the project; R.Y. implemented thesoftware and trained the deep networks under the supervisionof M.P.P. with support from A.P.F. and T.P.P.; M.P.P. wrote addi-tional code for release; J.A.A. and D.M.W. designed the protocolsfor and carried out the collection of the datasets. R.Y. and M.P.P.wrote the manuscript. All authors contributed to and approvedthe /f_inal manuscript.Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020Yasrab et al. 15AcknowledgementsWe thank Dr. Michael Wilson of the University of Nottinghamfor his contributions to testing and deployment of RootNav 2.0on variety of platforms and operating systems.References1. Pieruschka R, Schurr U. Plant phenotyping:past, present, and future. Plant Phenomics 2019,doi:10.34133/2019/7507131.2. Lynch JP. Steep, cheap and deep: an ideotype to optimizewater and acquisition by maize root systems. Ann Bot2013; 112(2):347–57.3. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature2015; 521(7553):436.4. Singh A, Ganapathysubramanian B, Singh AK, et al. Machinelearning for high-throughput stress phenotyping in plants.Trends Plant Sci 2016; 21(2):110–24.5. Pound MP, Atkinson JA, Wells DM, et al. Deep learn-ing for multi-task plant phenotyping. In: 2017 IEEEInternational Conference on Computer Vision Work-shops (ICCVW), Venice, Italy. IEEE; 2017:2055–63,doi:10.1109/ICCVW.2017.241.6. Ubbens JR, Stavness I. Deep plant phenomics: deep learn-ing platform for complex plant phenotyping tasks. FrontPlant Sci 2017; 8:1190.7. Minervini M, Scharr H, Tsaftaris SA. Image analysis: the newbottleneck in plant phenotyping [applications corner]. IEEESignal Proc Mag 2015; 32(4):126–31.8. Atkinson JA, Pound MP, Bennett MJ, et al. Uncovering the hid-den half of plants using new advances in root phenotyping.Curr Opin Biotech 2019; 55:1–8.9. Pound MP, French AP, Atkinson JA, et al. RootNav: navi-gating images of complex root architectures. Plant Physiol2013; 162(4):1802–14.10. Lobet G, Pag `es L, Draye X. novel image-analysis toolboxenabling quantitative analysis of root system architecture.Plant Physiol 2011; 157(1):29–39.11. Mairhofer S, Zappala S, Tracy S, et al. Recovering com-plete plant root system architectures from soil via X-ray µ-computed tomography. Plant Methods 2013; 9(1):8.12. Schulz H, Postma JA, van Dusschoten D, et al. Plant rootsystem analysis from MRI images. In: Csurka G, Kraus M,Laramee RS et al., eds. Computer Vision, Imaging and Com-puter Graphics: Theory and Application. Berlin and Heidel-berg: Springer; 2013:411–25.13. Das A, Schneider H, Burridge J, et al. Digital imaging of roottraits (DIRT): high-throughput computing and collabora-tion platform for /f_ield-based root phenomics. Plant Methods2015; 11(1):51.14. Galkovskyi T, Mileyko Y, Bucksch A, et al. GiA Roots: soft-ware for the high throughput analysis of plant root systemarchitecture. BMC Plant Biol 2012; 12(1):116.15. Pierret A, Gonkhamdee S, Jourdan C, et al. IJ Rhizo: an open-source software to measure scanned images of root samples.Plant Soil 2013; 373(1–2):531–9.16. Armengaud P, Zambaux K, Hills A, et al. EZ-Rhizo: in-tegrated software for the fast and accurate measure-ment of root system architecture. Plant 2009; 57(5):945–56.17. Haralick RM, Sternberg SR, Zhuang X. Image analysis usingmathematical morphology. IEEE Trans Pattern Anal Mach In-tell 1987; PAMI-9 (4):532–50.18. Lobregt S, Verbeek PW, Groen FC. Three-dimensional skele-tonization: principle and algorithm. IEEE Trans Pattern AnalMach Intell 1980; PAMI-2 (1):75–7.19. Berzin I, Cohen B, Mills D, et al. RHIZOSCAN: semi-automatic image processing system for characterizationof the morphology and secondary metabolite concentra-tion in hairy root cultures. Biotechnol Bioeng 2000; 70(1):17–24.20. Pradal C, Dufour-Kowalski S, Boudon F, et al. OpenAlea: vi-sual programming and component-based software platformfor plant modelling. Funct Plant Biol 2008; 35(10):751–60.21. Abr `amoff MD, Magalh ˜aes PJ, Ram SJ. Image processing withImageJ. Biophotonics Int 2004; 11(7):36–42.22. Le Bot J, Serra V, Fabre J, et al. DART: software to analyseroot system architecture and development from capturedimages. Plant Soil 2010; 326(1–2):261–73.23. Ristova D, Rosas U, Krouk G, et al. RootScape: landmark-based system for rapid screening of root architecture in Ara-bidopsis .P a tP y i l2 1 ; 161(3):1086–96.24. Settles B, Craven M, Friedland L. Active learning with realannotation costs. In: Proceedings of the NIPS Workshop onCost-Sensitive Learning, Vancouver, BC, Canada. 2008:1–10.25. Rolnick D, Veit A, Belongie S, et al. Deep learning is robust tomassive label noise. arXiv 2017:1705.10694.26. Clark RT, Famoso AN, Zhao K, et al. High-throughput two-dimensional root system phenotyping platform facilitatesgenetic analysis of root growth and development. Plant CellEnviron 2013; 36(2):454–66.27. Mohanty SP, Hughes DP, Salath ´e M. Using deep learningfor image-based plant disease detection. Front Plant Sci2016; 7:1419.28. Itzhaky Y, Farjon G, Khoroshevsky F, et al. Leaf counting:multiple scale regression and detection using deep CNNs. In:Proceedings of the British Machine Vision Conference, New-castle, UK, 2018. BMVA Press; 2018:328.29. Pound MP, Atkinson JA, Townsend AJ, et al. Deep machinelearning provides state-of-the-art performance in image-based plant phenotyping. Gigascience 2017; 6(10):gix083.30. Aich S, Stavness I. Leaf counting with deep convolutionaland deconvolutional networks. In: Proceedings of the IEEEInternational Conference on Computer Vision; 2017:2080–9.31. Keller K, Kirchgessner N, Khanna R, et al. Soybean leafcoverage estimation with machine learning and threshold-ing algorithms for /f_ield phenotyping. In: Proceedings of theBritish Machine Vision Conference, Newcastle, UK, 2018.BMVA Press; 2018:32.32. Atanbori J, Chen F, French AP, et al. Towards low-cost image-based plant phenotyping using reduced-parameter CNN. In:Proceedings of the British Machine Vision Conference, New-castle, UK, 2018. BMVA Press; 2018:32633. Chen H, Giuffrida MV, Tsaftaris SA, et al. Root gap cor-rection with deep inpainting model. In: Proceedings ofthe British Machine Vision Conference, Newcastle, UK, 2018.BMVA Press; 2018:325.34. Lobet G, Pound MP, Diener J, et al. Root System Markup Lan-guage: toward uni/f_ied root architecture description lan-guage. Plant Physiol 2015; 167(3):617–27.35. Atkinson JA, Wingen LU, Grif/f_iths M, et al. Phenotypingpipeline reveals major seedling root growth QTL in hexaploidwheat. Exp Bot 2015; 66(8):2283–92.36. Wilson MH, Holman TJ, Sørensen I, et al. Multi-omics analy-sis identi/f_ies genes mediating the extension of cell walls intheArabidopsis thaliana root elongation zone. Front Cell DevBiol 2015; 3:10.Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 202016 RootNav 2.0: Deep learning for automatic navigation of complex plant root architectures37. Wells DM, French AP, Naeem A, et al. Recovering the dynam-ics of root growth and development using novel image acqui-sition and analysis methods. Philos Trans Soc Lond BiolSci 2012; 367(1595):1517–24.38. Simonyan K, Zisserman A. Very deep convolutionalnetworks for large-scale image recognition. arXiv 2014:1409.1556.39. Long J, Shelhamer E, Darrell T. Fully convolutional net-works for semantic segmentation. In: Proceedings of the IEEEConference on Computer Vision and Pattern Recognition.2015:3431–40.40. Badrinarayanan V, Kendall A, Cipolla R. Segnet: deep con-volutional encoder-decoder architecture for image segmen-tation. arXiv 2015:1511.00561.41. Ronneberger O, Fischer P, Brox T. U-net: convolutionalnetworks for biomedical image segmentation. In: In-ternational Conference on Medical Image Computingand Computer-Assisted Intervention. Springer; 2015:234–41.42. Chen LC, Zhu Y, Papandreou G, et al. Encoder-decoder withatrous separable convolution for semantic image segmenta-tion. In: Ferrari V, Hebert M, Sminchisescu , et al., eds.Proceedings of the European Conference on Computer Vision(ECCV). Cham, Springer; 2018:801–18.43. Newell A, Yang K, Deng J. Stacked hourglass networks for hu-man pose estimation. In: Liebe B, Mata J, Sebe , et al., eds.European Conference on Computer Vision. Cham: Springer;2016:483–99.44. Chaurasia A, Culurciello E. Linknet: exploiting encoder repre-sentations for ef/f_icient semantic segmentation. In: 2017 IEEEVisual Communications and Image Processing (VCIP). IEEE;2017:1–4.45. Zhao H, Shi J, Qi X, et al. Pyramid scene parsing network. In:Proceedings of the IEEE Conference on Computer Vision andPattern Recognition. 2017:2881–90.46. Li H, Hearne S, ¨anziger M, et al. Statistical properties of QTLlinkage mapping in biparental genetic populations. Heredity2010; 105(3):257.47. Hong EP, Park JW. Sample size and statistical power cal-culation in genetic association studies. Genomics Iinform2012; 10(2):117.48. Paszke A, Gross S, Chintala S, et al. Automatic differentiationin pytorch. In: NIPS-W. 2017.49. Eigen D, Fergus R. Predicting depth, surface normals and se-mantic labels with common multi-scale convolutional ar-chitecture. In: Proceedings of the IEEE International Confer-ence on Computer Vision, Santiago, Chile. IEEE; 2015:2650–8,doi:10.1109/ICCV.2015.304.50. Kr ¨ahenb ¨uhl P, Koltun V. Ef/f_icient inference in fully con-nected CRFs with Gaussian edge potentials. In: Proceedingsof the 24th International Conference on Neural InformationProcessing Systems, Grenada, Spain. 2011:109–117.51. Desmaison A, Bunel R, Kohli P, et al. Ef/f_icient continuous re-laxations for dense CRF. In: European Conference on Com-puter Vision. Springer; 2016:818–33.52. Lindeberg T. Edge detection and ridge detection with auto-matic scale selection. Int Comput Vision 1998; 30(2):117–56.53. Pham TQ. Non-maximum suppression using fewer than twocomparisons per pixel. In: International Conference on Ad-vanced Concepts for Intelligent Vision Systems. Springer;2010:438–51.54. Doran JE, Michie D. Experiments with the Graph Tra-verser program. Proc Soc Lond Math Phys Sci1966; 294(1437):235–59.55. Dijkstra EW. note on two problems in connexion withgraphs. Numer Math 1959; 1(1):269–71.56. Yasrab R, Atkinson JA, Wells DM, et al. Supporting datafor “RootNav 2.0: Deep learning for automatic navigationof complex plant root architectures. GigaScience Database2019; http://dx.doi.org/10.5524/100651 .Downloaded from https://academic.oup.com/gigascience/article-abstract/8/11/giz123/5614712 by guest on 14 July 2020