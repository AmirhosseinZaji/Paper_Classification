Xiong etal. Plant Methods (2019) 15:150 https://doi.org/10.1186/s13007-019-0537-2RESEARCHTasselNetv2: in-field counting ofwheat spikes withcontext -augmented local regression networksHaipeng Xiong1, Zhiguo Cao1, Hao Lu1* Simon Madec2, Liang Liu1 and Chunhua Shen3Abstract Background: Grain yield of wheat is greatly associated with the population of wheat spikes, i.e., spike number m−2 To obtain this index in reliable and efficient way, it is necessary to count wheat spikes accurately and automatically. Currently computer vision technologies have shown great potential to automate this task effectively in low-end manner. In particular, counting wheat spikes is typical visual counting problem, which is substantially studied under the name of object counting in Computer Vision. TasselNet, which represents one of the state-of-the-art counting approaches, is convolutional neural network-based local regression model, and currently benchmarks the best record on counting maize tassels. However, when applying TasselNet to wheat spikes, it cannot predict accurate counts when spikes partially present.Results: In this paper, we make an important observation that the counting performance of local regression net -works can be significantly improved via adding visual context to the local patches. Meanwhile, such context can be treated as part of the receptive field without increasing the model capacity. We thus propose simple yet effective contextual extension of TasselNet—TasselNetv2. If implementing TasselNetv2 in fully convolutional form, both training and inference can be greatly sped up by reducing redundant computations. In particular, we collected and labeled large-scale wheat spikes counting (WSC) dataset, with 1764 high-resolution images and 675,322 manually-annotated instances. Extensive experiments show that, TasselNetv2 not only achieves state-of-the-art performance on the WSC dataset 91.01% counting accuracy) but also is more than an order of magnitude faster than TasselNet (13.82 fps on 912×1216 images). The generality of TasselNetv2 is further demonstrated by advancing the state of the art on both the Maize Tassels Counting and ShanghaiTech Crowd Counting datasets.Conclusions: This paper describes TasselNetv2 for counting wheat spikes, which simultaneously addresses two important use cases in plant counting: improving the counting accuracy without increasing model capacity, and improv -ing efficiency without sacrificing accuracy. It is promising to be deployed in real-time system with high-throughput demand. In particular, TasselNetv2 can achieve sufficiently accurate results when training from scratch with small networks, and adopting larger pre-trained networks can further boost accuracy. In practice, one can trade off the performance and efficiency according to certain application scenarios. Code and models are made available at: https ://tinyu rl.com/Tasse lNetv 2.Keywords: Wheat spikes, Object counting, Convolutional models, Local regression networks, Context fusion© The Author(s) 2019. This article is licensed under Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statu-tory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view copy of this licence, visit http://creat iveco mmons .org/licen ses/by/4.0/.BackgroundIn agricultural production, crop yield is one of the key factors when monitoring crop growth status. Wheat is one of the top three cereal crops in the world. Its Open AccessPlant Method s*Correspondence: poppinace@hust.edu.cn1 National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, People’s Republic of ChinaFull list of author information is available at the end of the articlePage of 14 Xiongetal. Plant Methods (2019) 15:150 grain yield is mainly associated to spike number m−2 grain number m−2 and thousand grain weight [1]. Among these traits, spike number m−2 is the most important index [2 3]. Conventional manual approaches to counting wheat spikes are tedious and labor-intensive. The counting results are also error-prone and unrepresentative due to small sampling areas used. To meet the need of large-scale analyses in the era of intelligent agriculture and to obtain the index of spike number m−2 accurately in real time, counting wheat spikes must be automated in reliable way, and possibly with low cost.With the rapid development of recent deep learn-ing technologies, large-scale visual databases and cost-effective graphical processing units, image-based approaches appear to be promising alternatives to auto-mate the task of wheat spikes counting.Counting wheat spikes is typical object counting problem in Computer Vision, and currently convolu -tional neural network (CNN)-based local regression models have shown remarkable performance in count-ing crowd [4 5], vehicles [6 ], cells [7 ], animals [8 ], and plants [9 –12]. However, when turning to the scenario of counting wheat spikes in the wild, things are much difficult due to the non-rigid nature of spikes and sub-stantial visual challenges. As shown in Fig. 1, these challenges are:• Wheats planted in different regions show significant visual differences, due to differences in varieties and geographical environment (Fig.1a);• The color, size and shape of wheat spikes vary greatly and unevenly at different growth stages of wheats (Fig.1b);• If the imaging equipment lacks manual maintenance, or fog droplets and dust cover the lens, images will be blurred (Fig.1c);• Dramatic illumination changes result in completely different visual characteristics of wheat (Fig.1d);• The intensive cultivation of wheats gives rise to extremely dense distributions and severe occlusions (Fig. 1e). In these extremely dense areas, even an expert has to count wheat spikes for multiple times to obtain reliable measure;• The perspective changes due to the imaging angle. Some wheats may be perpendicular to the lens and only occupy small number of pixels in the image, which renders difficulties to distinguish wheat spikes from background. This also leads to large size varia-tions of wheat spikes (Fig.1f).Above visual challenges make wheat spikes count -ing good study case for counting non-rigid objects. Recent literatures emerge on counting wheat spikes but are mainly based on detection. [13–16] first segment the wheats using the RGB images, and then detect each object based on the segmentation result. After detec-tion, the wheat counts can be easily inferred from the objects detected. [17] fuses multi-sensor information (RGB images and multispectral images) to help segmen-tation. [18] and [19] utilize R-CNN [20] to detect wheat spikes. However, the camera is close to the wheat spikes in these methods, which allows for capturing high-reso-lution images and obtaining accurate detection but leads to small observation areas. The efficiency of R-CNN processing high-resolution images is also an issue. [21] benefits from active learning to reduce human labe-ling efforts and use RetinaNet [22] for detecting and counting sorghum head in UAV-based images in large region. In order to meet the need of high-throughput plant phenotype analysis over large area, we leverage images captured from fixed platform (4 m/5 above the ground) for counting. These images cover wheat spikes over around 30 m2 However, wheat spikes pre -sent extremely dense distributions and severe overlaps in such images. We notice that non-maximum suppression is regularly used at the end of detection-based methods, which makes it hard to distinguish overlapping objects. Furthermore, there are more than 10,000 wheat spikes in just one image, which makes the bounding boxes annota-tion nearly impossible. Overall, these counting-by-detec -tion methods render difficulties for counting dense wheat spikes within large area.Current state-of-the-art counting approaches typically pursue the idea of local regression with CNNs. Images are often divided into small local patches, and these patches are then processed by the networks individually. Fig. Challenges of counting wheat spikes in the wild. different planting regions, various growth stages, degraded image quality due to blurring, visual differences caused by changing illumination, extremely dense spatial distributions and severe occlusions, size and pose variationsPage of 14 Xiongetal. Plant Methods (2019) 15:150 Most CNN-based local regression methods adopt density maps as the regression target [4, 5, 23–25]. These meth -ods intend to regress the per-pixel density maps, which is dense prediction problem. But the problem is that the ground-truth density map is associated with specific choices of Gaussian kernels. This means the ground-truth density map may not be initially accurate, and the error would be introduced before learning the model. To alleviate this problem, [9, 26] prove it is much easier to regress the local count than the density map. The ben-efit is that the ground truth is no longer sensitive to the exact choice of Gaussian kernels. Lu etal. [9] proposed local count regression network named TasselNet, which counts maize tassels much more accurate than other existing methods. We believe this idea should also be applicable to other non-rigid objects like wheat spikes.Albeit successful, we found that TasselNet cannot pre-dict correct counts when spikes partially present in local image patches. As shown in Fig. 2, it is not clear whether there are two wheat spikes or not when only looking at those visible regions. This situation is even more serious when spikes are occluded. In fact, wheats are planted far denser than maize plants, and the density of spikes typi-cally varies between 200/m2 and 600/m2 which means partial spikes would occur frequently in cropped local image patches and thus seriously limits the applicabil-ity of TasselNet. To address this, our intuition tells that we need the help of visual contextual information. This is in consistent with the fact that, when one cannot infer the exact number of partially occluded objects within local area, he may look further until supporting informa-tion, such as the border or other object parts, is identi -fied. This kind of supporting information in real world refers to the visual context in images, and it is kind of “weak context” for it only contains the local surroundings rather than all of remaining images. Therefore, simple way to tackle above problem is to enable TasselNet to receive both local images and their surrounding pixels, as shown in Fig. 3. This raises subsequent question: how to integrate the context into CNNs in principled way? One way is to use large convolutional kernels but at the cost of introducing extra parameters. In this paper, we show that much clever way is to include the context as part of the receptive field so that the model can keep the same number of parameters. This idea is particularly useful for local counting models, such as TasselNet, that do not make full use of their receptive field. As consequence, we make simple yet effective extension to TasselNet so that contextual information could be received, leading to an extended version of TasselNet—contextual TasselNet (TasselNetv2 for short).Another limitation of TasselNet is its low efficiency due to the need of densely sampling local image patches. This introduces many redundant computations. We won-der whether these redundant computations could be avoided in TasselNetv2. Inspired by Fast R-CNN [27], we show that one actually can first extract the features maps of the whole image and then densely sample the feature maps to obtain local features, rather than pro-cessing local patches individually. Based on this idea, we implement fully convolutional form of TasselNetv2, which is proven to be an order of magnitude faster than TasselNet. In particular, we created large-scale Wheat Spikes Counting (WSC) dataset to validate the effective-ness of TasselNetv2.Extensive experiments show that, TasselNetv2 reaches 91.01% relative counting accuracy and achieves the state-of-the-art performance on the WSC dataset, and nota -bly, can process images 13.21 times faster than TasselNet (13.82 fps for TasselNetv2 vs. 1.05 fps for TasselNet). Further experiments demonstrate that TasselNetv2 also reports state-of-the-art counting performance on the Maize Tassels Counting (MTC) and ShanghaiTech Crowd Counting datasets [5], which confirms good gen-erality of TasselNetv2. Several interesting ablative studies are conducted to justify the effectiveness and necessity to include the context for better counting performance.Overall, the main contributions of this paper are:Fig. Three examples of incomplete objects when only looking at the local patches. White parts are invisible contextual regions for the current visible patches. Wheat spikes annotated with black dots indicate the spike is partly within the visible area, and red dots represent spikes with severe occlusions. In both cases, accurate wheat numbers are just hard to obtain without the help of local visual contextFig. A high-level overview of the approach utilizing local visual context information. The red dashed box indicates local patch ready for counting, and the part outside the box refers to the contextPage of 14 Xiongetal. Plant Methods (2019) 15:150 We introduce principled way to supplement the local visual context into convolutional models by treating it as part of the receptive field, which can improve the counting performance without increas-ing extra parameters;• We propose simple yet effective extension of Tas -selNet to its contextual version TasselNetv2. Tas -selNetv2 not only improves the counting perfor -mance but also speeds up the computation with an order of magnitude;• We collect and annotate large-scale WSC dataset with 1764 high-resolution images and 675,322 man-ually-labeled instances;• We report state-of-the-art counting performance on the WSC, MTC and ShanghaiTech datasets.MethodImage acquisitionField wheat images in the WSC dataset are collected from three experimental fields of Gucheng, Hebei, Zhengzhou, Henan, and Tai’an, Shandong, containing seven sequences from 2011 to 2013. Due to the differ-ent local geology and climate conditions, three culti -vars were planted, respectively, including Zimai No. 24 in Taian, Jimai No. 22 in Gucheng, and Zhengmai No. 366 in Zhengzhou.Figure shows the image capturing device, main components include high-resolution CCD digital camera (E450 Olympus), low-resolution monitoring equipment, 3G wireless data transmission system, and several solar panels for power supply. The CCD digital camera is set with height of m above the ground, and the focal length is fixed to 16mm. From a.m. to 17 p.m., images are captured from perspective oblique to the ground once an hour. After images are acquired, wheat images are transmitted to the remote server through the 3G wireless network, and then we can access the image data. For detailed information of the image capturing equipment, readers can refer to [28].Wheat spikes counting datasetThere are tens of thousands of wheat spikes in the wheat images, and they present high degree of similarity when the time interval is short, which makes the annotations for all of the captured images costly and needless. This means only subset of images is essential to build the dataset, but this subset should be large enough to cover wheat spikes in various scenarios. We pick out this sub-set with two-stage selection strategy. At the first stage, we choose images according to the date, after the head-ing stage of wheat. Before obvious emergence of spikes, the sampling interval is set to days. After wheat spikes emerge, the number of wheat spikes changes rapidly, and thus the sampling interval is shortened to days. At the second stage, 10 candidate images collected in each day (from a.m. to 17 p.m.) are taken into account. Consid-ering the illumination characteristics in one day, three images are chosen from three time periods, i.e., morning (8 a.m. to 11 a.m.), noon (12 a.m. to 14p.m.), and after-noon (15 p.m. to 17 p.m.), to maintain the diversity of the dataset.Finally, total of 196 images, with the resolution of 3648×2736 were chosen. The number of wheat spikes varies from to over 10, 000. Since the image resolu -tion is very high, and wheat spikes are extremely dense (it brings tremendous difficulties for the annotation pro-cess), each original image is cropped to sub-images with resolution of 1216×912 Thus, 1764 images in all are used to construct the dataset. Table presents the information of each sequence in the dataset.With seven sequences in the WSC dataset, the train -ing set, validation set and test set are divided, as shown in Table 2. Images from the Shandong Taian (2012–2013 Camera 1) sequence exhibit relatively clear distinction Fig. Imaging device in the Zhengzhou, Henan Province. The main components include high resolution CCD digital camera (E450 Olympus) and low-resolution monitoring equipment. The camera is set m high above the groundTable Constitution oftheWSC datasetImages denote the number of images in each sequence. Spikes refer to the number of wheat spikes in each sequence. Min and Max indicate the minimum and maximum number of wheat spikes per imageSequence Images Spikes Min MaxHebei Gucheng (2011–2012) 324 82,578 661Henan Zhengzhou (2011–2012) 234 118,022 1462Henan Zhengzhou (2012–2013) 171 104,847 1331Shandong Taian (2011–2012 Camera 1) 279 97,695 1010Shandong Taian (2011–2012 Camera 2) 261 78,887 908Shandong Taian (2012–2013 Camera 1) 234 94,454 1090Shandong Taian (2012–2013 Camera 2) 261 98,839 971Total 1764 675,322 1462Page of 14 Xiongetal. Plant Methods (2019) 15:150 between spikes and background. Spikes in this sequence also appear to have high density and are with dramatic changes caused by illumination. In the Henan Zhengzhou (2012–2013) sequence, it is hard to distinguish the spikes from the background. The presence of severe occlusions makes this task even more challenging. Evaluations on these sequences can sufficiently show the adaptability and robustness of the counting method. Local visual con-text may be helpful for identifying overlapped objects, as shown in Fig. 2. We embed local visual context in Tas-selNetv2 to alleviate such problem.Following [9], dotted annotation is adopted where point is marked at the location of each wheat spike. Figure shows an example of annotated image. Six colleagues in our laboratory first participated in the annotation process. After the dataset is annotated, we double-checked the annotations and corrected some missing and wrong annotations. Especially for the second round checking, we trained TasselNet to predict counts and identified the areas with high counting errors. With this kind of auxiliary information, particular attentions are paid to these areas for careful checking further, and other areas are also checked again.Design ofTasselNetv2We first highlight the concepts of “input image” “input patch” and “input patch with context” in Fig. 7. They are prerequisites for readers to better understand TasselNetv2.Local patches from an image may have severe overlaps due to dense sampling, but TasselNet requires extracting the local feature from each patch first and then mapping it to the local count. In this paradigm, many redundant calculations appear during feature extraction. Inspired by Fast R-CNN [30], redundant calculations can be avoided by first extracting the feature maps of the whole image, then densely sampling the feature maps to obtain local features and finally mapping them to local counts in light-weight manner.Notice that fully-connected layers in TasselNet can also be implemented as convolutional layers with 1×1 ker-nels [31]. When the convolutional kernel slides over the image and manipulates local area of pixels at time, it performs form of dense sampling. This inspires us to replace the explicit dense sampling with convolution.MotivationThe local visual context, in the framework of local regres -sion, refers to the surrounding pixels of local sampling patches. In Fig. 2, if the visible parts belong to local sam-pling patches, those invisible parts represent the con -text. Unfortunately, since the context is not within local patches, it remains invisible to local regression networks like TasselNet. If network can see the context, overlap-ping objects or part of objects may be inferred easily and counted accurately. The high-level idea is thus to enable the network to process both local patches along with the context, as shown in Fig.3.Adding contextThe main idea of TasselNetv2 is to process local patches with the context. Notice that there is massive waste of the receptive field in TasselNet. It is natural to think how to reduce such waste. In this paper, we show that one can cancel zero paddings to enable the network receiving extra context and to make full use of the receptive field. The way to achieve this is simply to delete paddings in all of convolutional layers, as shown in Fig.6.We explain why this simple modification makes sense through visualizing analysis of the receptive field in Fig. 7, and brief introduction about computing the receptive field is also provided in Additional file 1. Assume TasselNet and TasselNetv2 regress the local count of the 64×64 local area. TasselNet (a) receives the local area without the context. It has zero paddings in all convolutional layers, and these paddings cause the zero area in the receptive field outside borders. However, if removing all the zero paddings, TasselNet (b) can lever-age the wasted receptive field to receive extra context and keep the same amount of parameters.Table Training set (train), validation set (val) andtest set (test) settings oftheWSC datasetSequence Train Val TestHebei Gucheng (2011–2012) /check /checkHenan Zhengzhou (2011–2012) /check /checkHenan Zhengzhou (2012–2013) /checkShandong Taian (2011–2012 Camera 1) /check /checkShandong Taian (2011–2012 Camera 2) /check /checkShandong Taian (2012–2013 Camera 1) /checkShandong Taian (2012–2013 Camera 2) /check /checkFig. An example of dotted annotation. red dot is marked at each location of the wheat spikePage of 14 Xiongetal. Plant Methods (2019) 15:150 It is worth noting that, though the network processes 94×94 patches, it still regresses local counts aggre -gated from the central 64×64 areas. Many counting approaches assume that CNNs are able to identify each object within their local receptive fields [26, 29], while we argue that one should treat part of the local receptive field as additional context towards accurate counting. This is what makes TasselNetv2 quite different from existing CNN-based local regression models.Improving efficiencyInspired by the idea of fully convolutional networks (FCNs) [32], we implement TasselNetv2 into fully con-volutional form, which speeds up both training and infer -ence significantly, as shown in Fig. 6. In what follows, we further explain in detail how TasselNetv2 works and improves efficiency.TasselNetv2 is composition of convolutional layers. If skipping the activation functions, the composition of con-volutional layers can be view as convolutional layer with large kernel, and the filter size equals to the size of the receptive field. As shown in Fig. 7, the size of the recep-tive field of the output remains 94×94 so TasselNetv2 can be seen as large 94×94 convolutional layer and maps each 94×94 local area (local patch with context) to local count. Meanwhile, since four layers are with stride of 2, this large convolutional filter slides with stride of 24=16 which is equivalent to densely sampling the input image with stride of 16. As consequence, TasselNetv2 adds context into TasselNet in FCN-like manner. It is worth noting that the context is naturally exploited in FCNs by most local areas. Only the context close to image borders is partially utilized by TasselNetv2, e.g., the local area in the upper left corner only has the lower right part of the context. In order to keep the size of these local areas to be 94×94 we supplement 15 zero paddings around the image borders. An elegant way to embed this pre-pro -cessing in TasselNetv2 is to use the accumulation of zero paddings from the first five layers (these zero paddings accumulate to 15 zero paddings around the input image).The calculations performed in CNNs are mainly Float-ing Point Operations (FLOPs), and FLOPs are also widely adopted in evaluating the computation complexity of CNNs 33, 34] from the view of computation amount. We remark the efficiency of TasselNetv2 using FLOPs during testing in Table 3. The first five convolution layers extract feature maps, and the following three layers map features to local counts. As mentioned in [9 ], dense sampling is essential to generate adequate training samples for Tas-selNet. However, 10× extra calculations are needed in this paradigm, compared to sampling non-overlapping patches. This is due to the redundant computations in both feature extraction and feature mapping. Instead, Tas-selNetv2 directly extracts the feature maps of the whole image, densely samples local features from the feature map and maps them to local counts simultaneously. In this way, TasselNetv2 avoids redundant calculations dur-ing feature extraction and is thus much more efficient than TasselNet. It can directly process the whole image and regress all local counts with single forward pass.Inference ofTasselNetv2Here we formally introduce the processing pipeline of TasselNetv2 during inference, as shown in Fig. 8. Tas-selNetv2 directly processes the whole image of arbitrary size (in this paper, the whole image refers to the image of size 1216×912 and regresses all local counts at the same time. However, since individual local areas have overlaps, the global image count cannot be acquired by summing over the whole count map directly. Following the aggregation and normalization strategy mentioned in [9], all local counts are merged to obtain the normalized count map. After normalization, the global image count can then be reflected by integrating over the count map.Implementation detailsWe implement TasselNetv2 based on MatConvNet [35]. During training, we use 1359 images in the train-ing and validation sequences of the WSC dataset. 90% images are randomly chosen for training, while the rest 2x2 max pool1, 0, /23x3 co nv1, 16, 12x2 max pool2, 0, /23x3 co nv2, 32, 13x3 co nv3, 64, 13x3 co nv4, 64, 13x3 co nv5, 64, 164x64x3 In put PatchTasselNet2x2 max pool3, 0, /2FC1, 128 FC2, 128 FC3, 1 Local Count94x94x3 In put Patch2x2 max pool1, 0, /23x3 co nv1, 16, 02x2 max pool2, 0, /23x3 co nv2, 32, 03x3 co nv3, 64, 03x3 co nv4, 64, 03x3 co nv5, 64, 0TasselNetadded context2x2 max pool3, 0, /2FC1, 128 FC2, 128 FC3, 1 Local Count912x 1216x Input Image2x2 max pool1, 0, /23x3 co nv1, 16, 12x2 max pool2, 0, /23x3 co nv2, 32 13x3 co nv3, 64 13x3 co nv4, 64 13x3 co nv5, 64 1TasselNetv254x73 All Local Counts1x1 co nv8, 1, 01x1 co nv7, 12 8, 08x8 co nv6, 12 8, 0, /22x2 max pool2, 0, /2Fig. The structure of TasselNet, TasselNet added context and TasselNetv2. All of the networks adopt AlexNet-like architectures. The definition of the convolutional and pooling layers is in the format: fliter size layer name, number of channels, padding, /stride. Fully connected layers are defined in the format: layer name, number of nodes. The different settings are highlighted in redPage of 14 Xiongetal. Plant Methods (2019) 15:150 for validation. Before learning, mean subtraction is pre -processed (the mean is computed from the training set). It is worth mentioning that, no data augmentation is per -formed because the WSC dataset already contains wheat spikes under various scenarios.We initialize networks with the improved Xavier method [36]. The standard stochastic gradient descent is applied to optimize the parameters of the net-work. The learning rate is initially set to 0.1 and is decreased when the training error stagnates. To speed up and stabilize the error convergence process, batch normalization layer [37] is attached after each convolu-tional layer before ReLU.The training time of TasselNetv2 on the WSC data -set varies from h to days depending on the network architecture used (4 hours for the Alex-like architec-ture, and days when the pretrained VGG–16 is used). When training TasselNet on the WSC dataset, the training time varies between days and weeks accord-ing to the network capacity used (Matlab 2017a, OS: Window10 Home 64-bit, CPU: Intel i7-7700 3.60GHz, GPU: Nvidia GeForce GTX 1070 (8GB), RAM: 16 GB).64x64x 3Conv1+Po ol1C onv4 FC232x32x161281281Inpu PatchC onv364x64 4x4Recep/g415 ve ﬁeldLocal CountFC1 Conv2+Po ol2C onv5+P ool5Conv 1:3x 3Pool1:2x 2Conv 2:3x 3Pool2:2x 2Conv 3:3x 3C onv4:3x316x16x3216x16x64Conv5:3x 3Pool5:2x 216x16x64 8x8x6410x10 18x1826x2638x38 94x94ba94x94x346x46x161x1x12 81x1x1 28194x9 4x4Recep/g415v ﬁeldLocal CountConv1:3x3Pool1:2x 2Conv2:3x 3Pool2:2x 2Conv3:3x 3C onv4:3x322x22x3220x20x64Conv 5:3x 3Pool5:2x218x18x648x8x6410x101 8x18 26x263 8x38Target Local AreaContext6464646494x94Inpu Patch with contex tC onv1+P ool1 Conv2+Po ol2C onv3 Conv4C onv5+P ool5 FC1F C2912x12 16x3Input ImageRecep/g415 ve ﬁeld912x12 16Conv 1:3x 3Pool1:2x2Conv1+P ool1 Conv2+Po ol2C onv3 Conv4C onv5+P ool5 Conv6C onv7All Loca CountsConv 2:3x 3Pool2:2x 2Conv 3:3x 3C onv4:3x3 Conv5:3x 3Pool5:2x 2Conv6:8x 8C onv7:1x 1C onv8:1x 1456x60 8x16228x30 4x32 228x30 4x64 228x30 4x64 114x15 2x64 54x73x12 85 4x73x12854x734x464x6410x10 18x18 26x26 38x38 94x94cFig. Feature maps and the corresponding receptive field of TasselNet and TasselNetv2. For TasselNet, for adding context to TasselNet via canceling zero-paddings and for TasselNetv2. The above line are feature maps of each layer in the network, numbers below feature maps are in the format: height×width×channel numbers The following line is the corresponding receptive fields, where black dotted boxes represents the target local area to be counted, the blue rectangular areas represents the input area, and the pink area represents the receptive field of the bottom left element in the feature map (the part of the receptive field beyond the input area denotes zero area). Since the last few layers have receptive fields of the same size, we use orange lines to point to the corresponding receptive fieldsPage of 14 Xiongetal. Plant Methods (2019) 15:150 Results anddiscussionExtensive experiments are conducted to demonstrate the effectiveness and efficiency of TasselNetv2. First, we perform experiments on the WSC dataset to search opti-mal hyper parameters. After obtaining these, we verify the effect of adding context in TasselNetv2. Next, Tas-selNetv2 is further compared against other state-of-the-art approaches on the WSC dataset. To demonstrate the generality of TasselNetv2, we also evaluate it on the MTC [9] and ShanghaiTech datasets [5].Mean absolute error (MAE and root mean squared error (RMSE are chosen to quantify the counting perfor-mance. They are defined as(1) MAE=1NN/summationdisplayi=1/vextendsingle/vextendsingle/vextendsingleCprei−Cgti/vextendsingle/vextendsingle/vextendsingle,(2) RMSE =/radicaltp/radicalvertex/radicalvertex/radicalbt1NN/summationdisplayi=1/parenleftBigCprei−Cgti/parenrightBig2,where denotes the number of images, Cprei denotes the predicted count of the i-th image, and Cgti denotes the corresponding ground-truth count. MAE meas -ures the accuracy of counting, and RMSE measures the stability. Lower MAE and RMSE imply better counting performance.Searching optimal parametersSince TasselNet is the direct baseline of TasselNetv2, we set the hyper parameters of TasselNetv2 same as the Tas -selNet, in order to demonstrate the superiority of Tas -selNetv2 w.r.t. TasselNet and the benefit of embedding context information. Hence, we first search the optimal parameters on the WSC dataset using TasselNet so that TasselNet can report the optimal performance, and we then apply the same parameters to TasselNetv2.Through extensive experiments, the optimal setting of hyper parameters for TasselNet on the WSC dataset is summarized in Table 4. Detailed procedures of searching optimal parameters are provided in Additional file1.Why adding context?Adding context iseffectiveWe first compare TasselNet trained with/without the context to highlight the pure effect of adding the context. Then, TasselNetv2 is evaluated to show its efficiency and accuracy beyond TasselNet.Quantitative results are presented in Table 5. We observe that, when forcibly adding the context into Tas-selNet during only inference (trained without context), the counting error increases notably, which suggests that TasselNet cannot utilize contextual information when trained without the context. This is the problem Table Comparison towards the floating point computations (FLOPs) when processing images with the resolution of 1216×912 Only the single-precision floating point multiplication are taken intoaccountTasselNet TasselNetv2Non-overlap Dense sampleconv1 4.70×1086.92×1094.79×108conv2 1.24×1091.83×10101.28×109conv3 1.22×1091.81×10101.28×109conv4 2.44×1093.61×10102.56×109conv5 2.44×1093.61×10102.56×109conv6(fc1) 5.17×1082.07×1092.07×109conv7(fc2) 1.75×1076.46×1076.46×107conv8(fc3) 1.26×1055.05×1055.05×105Total 8.34×1091.16×10111.03×1010Fig. The processing pipeline of TasselNetv2 at the test stage. Unlike TasselNet, TasselNetv2 directly processes the whole input image and outputs all local counts. And the final density map can be acquired by merging and normalizing all local countsTable TasselNet configurations ontheWSC datasetPatch size 64×64 Gaussian size 4Backbone of TasselNet AlexNet-like in Fig. 6Table The effect of context on the test set of the WSC dataset. “train” denotes adding context into TasselNet since training phase as Fig. 7b, while “test” denotes onlyadding context intoTasselNet inthetesting phaseAll networks are trained from scratch. Training time for one epoch is reported. The best performance is in italicsMethod Context MAE RMSE Train (s)TasselNet 61.35 99.27 3495.29TasselNet Test 79.42 126.18 3495.29TasselNet Train 50.17 82.16 4026.68TasselNetv2 /check 50.79 80.66 333.27Page of 14 Xiongetal. Plant Methods (2019) 15:150 we call information asymmetry. However, after embed -ding contextual information since the training phase, the MAE decreases more than 10 without increasing model parameters (compared to TasselNet). Adding the context is effective. It is worth noting that this significant perfor-mance improvement comes almost at no cost.It also can be observed that TasselNetv2 exhibits the same degree of improvement of adding the context. Meanwhile, TasselNetv2 is more than 10 times faster than TasselNet during the training stage. This is achieved by processing input images in FCN manner rather than densely sampling image patches, thus avoiding redun-dant computations in feature extraction, as analysed in Table 3. Now we can say that TasselNetv2 is much more efficient implementation of adding the context into TasselNet.We further analyze the error distributions in Fig. 9, and find that patch-based and image-based errors are more likely to shift towards zero with the help of context. So far, it can be concluded that lacking the context is the main drawback of TasselNet, and it is important to add the context during training.Adding context isnecessaryNotice that we treat the context as part of the recep -tive field and regress only the local count from the cen -tral region. One may wonder what if the network simply regresses the local count accumulated from the whole receptive field. Another baseline TasselNetv2 (del-c) is used to justify this point, where we delete the context of the input patch in TasselNetv2. Specifically, we alter the regression target of TasselNetv2 to the object count within the whole 94×94 receptive field (rather than the 64×64 central area in our proposition).According to the results in Table 6, we can see that the counting performance of TasselNetv2 (del-c) drops sig -nificantly (66.96 MAE), even worse than TasselNet. This implies network may not sense everything in its recep -tive field. possible explanation may be given from some recent findings on the effective receptive field. First, the effective receptive field is much smaller than the theo-retical receptive field [38]. According to [39], the effective receptive field empirically obeys Gaussian distribution, which means pixels close to the center of the receptive field have much larger impact on counting than mar-ginal pixels close to the boundary of the receptive field. network may not capture sufficient evidence to sup-port regressing counts at the border of the receptive field, while our empirical study shows that adding the context into part of the receptive field as auxiliary information can help to improve the counting of objects located in the center of receptive field.The above experiments justify that it is better to use portion of the receptive field as the context, instead of counting all objects within the whole receptive field [26].Comparison withstate oftheartAccording to the above evaluations, the optimal setting on the WSC dataset is shown in Table 4. Next, to com-pare TasselNetv2 with other state-of-the-art methods, several well-established baselines are chosen:• Segmentation method in [13]: This is the latest counting by segmentation method specially designed Fig. The distribution of absolute errors for local patches and test images. The left is the histogram of absolute error for local patches, and the right is the histogram of absolute error for test images. All networks are trained from scratch. “TasselNet (add-c)” denotes adding the context in TasselNet as per Fig. since the training phaseTable The necessity of adding context on the test set oftheWSC datasetAll networks are trained from scratch and with the same hyper parameters. The best performance is in italicsMethod MAE RMSETasselNet 61.35 99.27TasselNetv2 50.79 80.66TasselNetv2(del-c) 66.96 113.20Page 10 of 14 Xiongetal. Plant Methods (2019) 15:150 to count wheat spikes in the field. It first applies Laplacian frequency filtering to remove background, then utilizes the median filter to eliminate noise, and finally, finds the maximal to split individual wheat spikes;• Density map regression methods: CCNN [6] and MCNN [5] are two typical counting-by-regression methods, which aim to regress pixel-wise density maps. Their parameters are of the same order of magnitude as TasselNetv2. CSRNet [23] represents the state-of-the-art crowd counting approach and is composed of much deeper CNN (pretrained VGG16) as the front-end used for feature extraction. For fair comparison, we replace the feature extrac-tor in TasselNetv2 (the first convolutional layers) with all convolutional layers in VGG16 [40] and mark it as TasselNetv2† More details about TasselNetv2† can be found in Additional file1.• Local count regression method: TasselNet [9] regresses the local counts rather than density maps. This is our direct baseline and the most closely-related approach. brief introduction to TasselNet can also be found in Additional file.Results are listed in Table 7. We can make the following observations:• Segmentation method in [13] works poorly on the WSC dataset (317.19 MAE). Due to heavy depend-ency on the color information, this method is very sensitive to the illumination that significantly changes the color attributes. This also implies the problem of counting wheat spikes in the field-based environment cannot be addressed just by segmentation.• Density map regression methods, such as CCNN and MCNN, perform much better than the segmentation method, with 101.39 MAE and 97.08 MAE, respec-tively. It seems that these two CNN-based methods can adapt to the in-field environmental variations and the morphological variations of wheat spikes to certain degree. Nevertheless we remark that den-sity map prediction may not be suitable for counting wheat spikes, because the ground-truth density map cannot be generated accurately. This is also true for counting other non-rigid objects.• TasselNet outperforms CCNN and MCNN on the WSC dataset (61.35 MAE). It considerably reveals the benefit of local counts regression, which is important for object counting problems that have size variations.• CSRNet slightly outperforms TasselNetv2 (46.32 MAE versus 50.79 MAE). However, CSRNet not only has substantial parameters, more than an order of magnitude compared to TasselNetv2, but also is greatly benefited from the pre-trained model. Though with these unfair factors, TasselNetv2 still exhibits comparable performances against CSRNet. When TasselNetv2† uses the same pretrained VGG16, it outperforms CSRNet, with 44.27 MAE 91.01% rela -tive counting accuracy), reaching the state-of-the-art performance on the WSC dataset. As consequence, for time-sensitive applications, TasselNetv2 is still our recommended choice.Evaluation ontheMTC datasetTo show that TasselNetv2 is generic object counting method, particularly for the application in the agricul-ture scenario. We further evaluate the effectiveness Table Comparison with state-of-the-art counting approaches on the test set of WSC dataset. TasselNetv2 adopts anAlexNet-like architecture inFig.6 andistrained fromscratch† means the model is finetuned from the pretrained VGG16, and layer-by-layer settings can be found in Additional file. The best performance is italicsMethod Henan Zhengzhou (2012–2013) Shandong Taian (2012–2013 Camera1)Overall #ParametersMAE RMSE MAE RMSE MAE RMSESegmentation method in [13]387.09 436.84 268.03 345.78 317.19 386.22 ×CCNN [6] 168.41 214.41 52.40 72.78 101.39 149.91 5.70×105MCNN [5] 149.44 188.34 58.83 75.50 97.08 135.17 1.33×105CSRNet† [23] 64.19 88.96 33.26 46.19 46.32 67.63 1.63×107TasselNet [9] 94.97 137.24 36.79 57.37 61.35 99.27 6.38×105TasselNetv2 74.97 113.21 33.12 49.26 50.79 80.66 6.38×105TasselNetv2† 61.57 87.67 31.62 47.55 44.27 67.47 1.60×107Page 11 of 14 Xiongetal. Plant Methods (2019) 15:150 of TasselNetv2 on the Maize Tassels Counting (MTC) [9] dataset, following the same setting as [9 ]. Detailed results are shown in Table8 .TasselNet currently represents the state-of-the-art approach on the MTC dataset. According to the results, we found that TasselNetv2 outperforms Tas-selNet and further reduces the counting error by 18.2% (5.4 MAE versus 6.6 MAE). The context is also an important factor for maize tassels.With pre-trained model, TasselNetv2† only per -forms slightly better than TasselNetv2 but increases more than an order of magnitude of parameters. We conjecture the main reason is the lack of training sam-ples in the MTC dataset (only 186 training images). The potential of pre-trained models may not be fully exploited with such small dataset, while small net-work, such as TasselNetv2, can already produce sat -isfactory results. In this case, TasselNetv2 is effective and efficient, which seems to be better choice than TasselNetv2†.Evaluation ontheShanghaiTech datasetWe further evaluate TasselNetv2 on the ShanghaiTech dataset 5] to see its generality to crowd counting, follow-ing the same experimental setting in [5]. Results are listed in Table9.On both the part and part subsets, the benefit of adding the context can be reflected when comparing TasselNetv2 with TasselNet, but the improvement is marginal. When using pre-trained VGG-16 model, Tas-selNetv2† outperforms CSRNet and reaches the state-of-the-art performance. This suggests pre-trained models is necessary to fully exploit the benefit of context on the ShanghaiTech dataset.Some failure casesFigure 10 shows some qualitative results of TasselNetv2 on the WSC dataset. In most cases, TasselNetv2 predicts accurate counts (the first four rows). However, it exposes prominent under-estimate phenomena in some cases, particularly when severe overlapping and heavy blur-ring occur. These visual patterns raise huge challenge to discriminate spikes even for human expert. Efforts still should be paid to overcome these challenges. We leave this for future explorations.ConclusionsIn this work, we addressed an important and practi -cal problem of counting wheat spikes in the field-based environment using computer vision. We observe that, some existing CNN-based local regression models, such as TasselNet, suffer from the problem of lacking con-textual information, so they usually cannot predict cor -rect counts when objects partially present in local image patches. By integrating the context into the framework of the TasselNet, we proposed simple but effective exten-sion, i.e., TasselNetv2. large-scale WSC dataset, with 1, 764 images and 675, 322 annotated wheat spikes, is also created. The dataset is very challenging due to intrin-sic and extrinsic variations not only in spikes per se but also in environment, which makes it appropriate to be used as benchmark for counting non-rigid objects.Extensive experiments illustrate that, TasselNetv2 achieves state-of-the-art performance on the WSC dataset with 91.01% relative counting accuracy, and is also more than an order of magnitude faster than Tas -selNet. Further evaluations on the MTC and Shangha -iTech datasets demonstrate that TasselNetv2 can also push forward the state of the art. Sufficient analyses of potential issues effecting the practical application of TasselNetv2 are also described, including emphasiz-ing the role of the context in object counting, searching Table Evaluations of different methods on the MTC [9] dataset† means the model is finetuned from the pretrained VGG16. The best performance is in italicsMethod MAE RMSEJointSeg [41] 24.2 31.6mTASSEL [42] 19.6 26.1GlobalReg [43] 19.7 23.3DensityReg [44] 11.9 14.8CCNN [6] 21.0 25.5TasselNet [9] 6.6 9.6TasselNetv2 5.4 8.8TasselNetv2† 5.3 9.4Table Evaluations ontheShanghaiTech [5] dataset† means the model is fine-tuned from the pretrained VGG16. The best performance is in italicsMethod Part Part BMAE RMSE MAE RMSEMCNN [5] 110.2 173.2 26.4 41.3CP-CNN [25] 73.6 106.4 20.1 30.1ACSCP [24] 75.7 102.7 17.2 27.4CSRNet† [23] 68.2 115.0 10.6 16.0TasselNet [9] 87.0 138.9 16.7 28.1TasselNetv2 84.1 140.1 15.3 27.8TasselNetv2† 66.8 112.1 9.6 17.5Page 12 of 14 Xiongetal. Plant Methods (2019) 15:150 optimal parameters for local counts regression, and analyzing potential errors. We believe TasselNetv2 shows great potentials to be applied to other object counting domains.Albeit empirically effective, the reason why the con-text can improve the counting performance only stays at an intuitive level, and it remains unclear how the context interacts with the central receptive field as aux-iliary information. We hope such empirical findings in this paper could inspire others to uncover the mystery of the receptive field.Supplementary informationSupplementary information accompanies this paper at https ://doi.org/10.1186/s1300 7-019-0537-2.Additional file1. More details about the WSC dataset, experiment set -tings and results. brief introduction and analysis to the TasselNet [9] are also included.AcknowledgementsThe authors would like to thank the Wuxi Institute of Radio Science and Technology for providing the facilities and equipment, and X. Xiong, C. X. Liu, H. Z. Qi, W. X. Jiang, T. D. Yu, Z. H. Zhu for their assistance in annotating the WSC dataset.Author’s contributionsHX proposed the idea of TasselNetv2 and implemented the experiments. Both HX and HL drafted the manuscript, while LL and MS helped design the experiments and analyse the results. ZG and CS co-supervised the study and contributed in writing the manuscript. All authors read and approved the final manuscript.Fig. 10 Some ground truth density maps overlaid on original images on the test set of the WSC dataset and count maps generated by TasselNetv2 (finetuned with pre-trained VGG16). The number above each original image denotes the ground truth count number of wheat spikes, while that above each density map denotes prediction count number. The last line shows some unsuccessful predictions, and error maps of these images are also presented. An error map denotes the difference of the ground truth and predicted density map. Over-estimate is denoted by red, under-estimate by blue, and minor difference by gray. The darker the color is, the greater the errors are. We also zoom in some local areas with high counting errors. ’GT’ denotes ground-truth counts and ’Error’ denotes the difference compared to the ground truth. Further visualizations can be found in Additional file 1.Page 13 of 14 Xiongetal. Plant Methods (2019) 15:150 FundingThis work was supported by the Natural Science Foundation of China under Grant No. 61876211. CS’ participation was in part supported by the ARC industrial transformation research hub for driving farming productivity and disease prevention.Availability of data and materialsThe WSC dataset and other supporting materials are made available online at: https ://tinyu rl.com/Tasse lNetv 2.Ethics approval and consent to participateNot applicable.Consent for publicationNot applicable.Competing interestsThe authors declare that they have no competing interests.Author details1 National Key Laboratory of Science and Technology on Multi-Spectral Infor -mation Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan 430074, People’s Republic of China. INRA-EMMAH-CAPTE, 84914 Avignon, France. School of Computer Science, The University of Adelaide, Adelaide, SA 5005, Australia. Received: 23 April 2019 Accepted: December 2019References 1. Pask AJD, Pietragalla J, Mullan DM, Reynolds MP Physiological breeding ii: field guide to wheat phenotyping. Cimmyt. 2012;95–103. 2. Slafer GA, Calderini DF, Miralles DJ. Yield components and compensation in wheat: opportunities for further increasing yield potential. Increasing yield potential in wheat: breaking the barriers. 1996. 3. Ferrante A, Cartelle J, Savin R, Slafer GA. Yield determination, interplay between major components and yield stability in traditional and contemporary wheat across wide range of environments. Field Crops Res. 2017;203:114–27. 4. Zhang C, Li H, Wang X, Yang X. Cross-scene crowd counting via deep convolutional neural networks. In: Proc. IEEE international conference on computer vision (ICCV), 2015. p. 833–41. 5. Zhang Y, Zhou D, Chen S, Gao S, Ma Y. Single-image crowd counting via multi-column convolutional neural network. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2016. p. 589–97. 6. Oñoro-Rubio D, López-Sastre RJ. Towards perspective-free object count -ing with deep learning. In: Proc. European conference on computer vision (ECCV), 2016. p. 615–29. 7. Xie W, Noble JA, Zisserman A. Microscopy cell counting and detection with fully convolutional regression networks. Comput Methods Biomech Biomed Eng Imaging Vis. 2018;6(3):283–92. 8. Arteta C, Lempitsky V, Zisserman A. Counting in the wild. In: Proc. Euro -pean conference on computer vision (ECCV), 2016. p. 483–98. 9. Lu H, Cao Z, Xiao Y, Zhuang B, Shen C. TasselNet: counting maize tas-sels in the wild via local counts regression network. Plant Methods. 2017;13(1):79–95. 10. Aich S, Josuttes A, Ovsyannikov I, Strueby K, Ahmed I, Duddu HS, Pozniak C, Shirtliffe S, Stavness I. Deepwheat: Estimating phenotypic traits from crop images with deep learning. In: Proc. IEEE winter conference on applications of computer vision (WACV), 2018. p. 323–32. 11. Rahnemoonfar M, Sheppard C. Deep count: fruit counting based on deep simulated learning. Sensors. 2017;17(4):905. 12. Chen SW, Skandan SS, Dcunha S, Das J, Okon E, Qu C, Taylor CJ, Kumar V. Counting apples and oranges with deep learning: data driven approach. IEEE Robot Autom Lett. 2017;2(2):781–8. 13. Fernandez-Gallego JA, Kefauver SC, Gutiérrez NA, Nieto-Taladriz MT, Araus JL. Wheat ear counting in-field conditions: high throughput and low-cost approach using RGB images. Plant Methods. 2018;14(1):22. 14. Li Q, Cai J, Berger B, Okamoto M, Miklavcic SJ. Detecting spikes of wheat plants using neural networks with laws texture energy. Plant Methods. 2017;13(1):83. 15. Alharbi N, Zhou J, Wang W. Automatic counting of wheat spikes from wheat growth images. In: International conference on pattern recogni-tion applications and methods. 2018. p. 346–55. 16. Zhou C, Liang D, Yang X, Yang H, Yue J, Yang G. Wheat ears counting in field conditions based on multi-feature optimization and TWSVM. Front Plant Sci. 2018;8:1024. 17. Zhou C, Liang D, Yang X, Xu B, Yang G. Recognition of wheat spike from field based phenotype platform using multi-sensor fusion and improved maximum entropy segmentation algorithms. Remote Sens. 2018;10(2):246. 18. Hasan MM, Chopin JP Laga H, Miklavcic SJ. Detection and analysis of wheat spikes using convolutional neural networks. Plant Methods. 2018;14(1):100. 19. Madec S, Jin X, Lu H, De Solan B, Liu S, Duyme F, Heritier E, Baret F. Ear density estimation from high resolution RGB imagery using deep learning technique. Agric Forest Meteorol. 2019;264:225–34. 20. Girshick R, Donahue J, Darrell T, Malik J. Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2014. p. 580–7. 21. Ghosal S, Zheng B, Chapman SC, Potgieter AB, Jordan DR, Wang X, Singh AK, Singh A, Hirafuji M, Ninomiya S, et al. weakly supervised deep learning framework for sorghum head detection and counting. Plant Phenomics. 2019;2019:1525874. 22. Lin T-Y, Goyal , Girshick R, He K, Dollár . Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision, 2017. p. 2980–8. 23. Li Y, Zhang X, Chen D. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In: Proc. IEEE confer -ence on computer vision and pattern recognition (CVPR), 2018. p. 1091–100. 24. Shen Z, Xu Y, Ni B, Wang M, Hu J, Yang X. Crowd counting via adver -sarial cross-scale consistency pursuit. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR) 2018. 25. Sindagi VA, Patel VM. Generating high-quality crowd density maps using contextual pyramid cnns. In: Proc. IEEE international conference on computer vision (ICCV), 2017. p. 1879–88. 26. Cohen JP Boucher G, Glastonbury CA, Lo HZ, Bengio Y. Count-ception: Counting by fully convolutional redundant counting. In: Proc. IEEE international conference on computer vision workshop (ICCVW), 2017. p. 18–26. 27. Girshick R. Fast R-CNN. In: Proc. IEEE conference on computer vision and pattern recognition (ICCV), 2015. p. 1440–8. 28. Lu H, Cao Z, Xiao Y, Fang Z, Zhu Y. Toward good practices for fine-grained maize cultivar identification with filter-specific convolutional activations. IEEE Trans Autom Sci Eng. 2018;15(2):430–42. 29. Seguí S, Pujol O, Vitrià J. Learning to count with deep object features. In: Proc. IEEE conference on computer vision and pattern recognition workshops (CVPRW), 2015. p. 90–6. 30. Eggert C, Brehm S, Winschel A, Dan Z, Lienhart R. closer look: Small object detection in faster r-cnn. In: Proc. IEEE international conference on multimedia and expo (ICME), 2017. p. 421–6. 31. Lin M, Chen Q, Yan S. Network in network. In: Proc. International confer -ence on learning representations (ICLR) 2013. 32. Long J, Shelhamer E, Darrell T. Fully convolutional networks for seman-tic segmentation. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2015. p. 3431–40. 33. Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H. Mobilenets: Efficient convolutional neural net -works for mobile vision applications. arXiv preprint arXiv :1704.04861 2017. 34. Zhang X, Zhou X, Lin M, Sun J. Shufflenet: An extremely efficient con-volutional neural network for mobile devices. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2018. p. 6848–6856. 35. Vedaldi A, Lenc K. MatConvNet: Convolutional neural networks for MATLAB. In: Proc. ACM international conference on multimedia, 2015. p. 689–92.Page 14 of 14 Xiong etal. Plant Methods (2019) 15:150 fast, convenient online submission thorough peer review by experienced researchers in your ﬁeld• rapid publication on acceptance• support for research data, including large and complex data types• gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year At BMC, research is always in progress.Learn more biomedcentral.com/submissionsReady to submit our researc ? Choose BMC and benefit fr om: 36. He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: Proc. IEEE inter -national conference on computer vision (ICCV), 2015. p. 1026–34. 37. Ioffe S, Szegedy C. Batch normalization: Accelerating deep network train-ing by reducing internal covariate shift. In: Proc. international conference on machine learning (ICML), 2015. p. 448–56. 38. Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A. Object detectors emerge in deep scene CNNs. In: Proc. international conference on learn-ing representations (ICLR) 2014. 39. Luo W, Li Y, Urtasun R, Zemel R. Understanding the effective receptive field in deep convolutional neural networks. In: Advances in neural infor -mation processing systems (NIPS), 2016. p. 4898–906. 40. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. Computer Science 2014. 41. Lu H, Cao Z, Xiao Y, Li Y, Zhu Y. Region-based colour modelling for joint crop and maize tassel segmentation. Biosyst Eng. 2016;147:139–50. https ://doi.org/10.1016/j.biosy stems eng.2016.04.007. 42. Lu H, Cao Z, Xiao Y, Fang Z, Zhu Y, Xian K. Fine-grained maize tassel trait characterization with multi-view representations. Comput Electron Agric. 2015;118:143–58. https ://doi.org/10.1016/j.compa g.2015.08.027. 43. Tota K, Idrees H. Counting in dense crowds using deep features. CRCV 2015. 44. Lempitsky V, Zisserman A. Learning to count objects in images. In: Advances in neural information processing systems (NIPS), 2010. p. 1324–32.Publisher’s NoteSpringer Nature remains neutral with regard to jurisdictional claims in pub -lished maps and institutional affiliations.