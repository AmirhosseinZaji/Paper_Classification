remote sensing ArticleA New CNN-Bayesian Model for Extracting ImprovedWinter Wheat Spatial Distribution from GF-2 imageryChengming Zhang1,2, Yingjuan Han3, Feng Li4,*, Shuai Gao5, Dejuan Song1,2, Hui Zhao3,Keqi Fan1and Ya’nan Zhang11College of Information Science and Engineering, Shandong Agricultural University, 61 Daizong Road,Taian 271000, Shandong, China; chming@sdau.edu.cn (C.Z.); 2017120532@sdau.edu.cn (D.S.);fkq980810@hotmail.com (K.F.); zyn980113@hotmail.com (Y.Z.)2Shandong Technology and Engineering Center for Digital Agriculture, 61 Daizong Road, Taian 271000,Shandong, China3Key Laboratory for Meteorological Disaster Monitoring and Early Warning and Risk Management ofCharacteristic Agriculture in Arid Regions, CMA, 71 Xinchangxi Road, Yinchuan 750002, Ningxia, China;yjhan_nx@outlook.com (Y.H.); zhaohui_cau@outlook.com (H.Z.)4Shandong Provincal Climate Center, NO.12 Wuying Mountain Road, Jinan 250001, Shandong, China5Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Dengzhuangnan Road,Beijing 100094, China; gaoshuai@radi.ac.cn*Correspondence: lifengsd@outlook.com; Tel.: +86-156-6697-6552Received: 30 January 2019; Accepted: 12 March 2019; Published: 14 March 2019/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046Abstract:When the spatial distribution of winter wheat is extracted from high-resolution remotesensing imagery using convolutional neural networks (CNN), ﬁeld edge results are usually rough,resulting in lowered overall accuracy. This study proposed new per-pixel classiﬁcation modelusing CNN and Bayesian models (CNN-Bayesian model) for improved extraction accuracy. In thismodel, feature extractor generates feature vector for each pixel, an encoder transforms the featurevector of each pixel into category-code vector, and two-level classiﬁer uses the difference betweenelements of category-probability vectors as the conﬁdence value to perform per-pixel classiﬁcations.The ﬁrst level is used to determine the category of pixel with high conﬁdence, and the secondlevel is an improved Bayesian model used to determine the category of low-conﬁdence pixels. TheCNN-Bayesian model was trained and tested on Gaofen satellite images. Compared to existingmodels, our approach produced an improvement in overall accuracy, the overall accuracy of SegNet,DeepLab, VGG-Ex, and CNN-Bayesian was 0.791, 0.852, 0.892, and 0.946, respectively. Thus, thisapproach can produce superior results when winter wheat spatial distribution is extracted fromsatellite imagery.Keywords:winter wheat; convolutional neural network; Visual Geometry Group Network; Bayesianmodel; per-pixel classiﬁcation; high-resolution remote sensing imager; Gaofen image1. IntroductionWheat is the most important food crop in the world, comprising 38.76% of the total area cultivatedfor food crops and 29.38% of total food crop production in 2016 [1]. In China, these numbers are 21.38%and 21.00%, respectively [2]. Accurate estimations of crop spatial distribution and total cultivated areaare of great signiﬁcance for agricultural disciplines such as yield estimation, food policy development,and planting management, which are of great importance for ensuring national food security [3,4].Traditionally, obtaining crop area required large-scale ﬁeld surveys. Although this approachproduces high-accuracy results, it is time-consuming, labor-intensive, and often lacking in spatialinformation [5]. The use of remotely sensed data is an effective alternative that has been widely usedRemote Sens.2019,11, 619; doi:10.3390/rs11060619www.mdpi.com/journal/remotesensingRemote Sens. 2019,11, 619 of 21over the past few decades at regional or global scales 6–8]. As extraction of crop spatial distributionmainly relies on pixel-based image classiﬁcation, correctly determining pixel features for accurateclassiﬁcation is the basis for this approach 9–12].The spectral characteristics of low- and middle-resolution remote sensing images are usuallystable. Vegetation indexes are generally used as pixel features in studies using data from sourcesincluding the Moderate Resolution Imaging Spectroradiometer (MODIS) 6,13–16], Enhanced ThematicMapper/Thematic Mapper 13,17], and Systeme Probatoire d’ Observation de la Terre 7,10]. Theseindices include the normalized difference vegetation index (NDVI) 5,6,13–15], relationship analysis ofNDVI 8], and enhanced vegetation index (EVI) 3,18], which are extracted from band values. Commonclassiﬁcation methods include decision trees 5,11,13], linear regression 6], statistics 7], ﬁltration 13],time-series analysis 14,15], the iterative self-organizing data analysis technique (ISODATA) 16], andthe Mahalanobis distance 17]. Texture features can better describe the spatial structure of pixels, theGray-Level Co-Occurrence Matrix is commonly used texture feature 19], and Gabor 20] and wavelettransforms 19,21] are often used to extract texture features. Moreover, object-based image analysistechnology is also widely used in pre-pixel classiﬁcation 22,23]. Such methods can successfully extractthe spatial distribution of winter wheat and other crops, but limitations in spatial resolution restrictthe applicability of the results.The spatial resolution and precision of crop extraction can be signiﬁcantly improved by usinghigh-resolution imagery 8,24,25]. However, as the spectral characteristics of such imagery are not asstable as those of low- and middle-resolution imagery, traditional feature extraction methods struggleto extract effective pixel features 26,27]. Neural networks 28,29] and support vector machines 30,31]have been applied to this problem, but both are shallow-learning algorithms 32] that have difﬁcultyeffectively expressing complex features, producing unsatisfactory results.Convolutional neural networks (CNN) were developed from neural networks. The standardCNN follows an “image-label” approach, and its output is probability distribution over differentclass. Typical examples include AlexNet 33], GoogLeNet 34], Visual Geometry Group Network(VGG) 35], and Resnet 36]. Due to their strong feature extraction ability, these networks haveachieved remarkable results in camera image classiﬁcation 37,38]. The fully convolutional network,a “per-pixel-label” model based on standard CNNs, was proposed in 2015 39]. This network usesa multi-layer convolutional structure to extract pixel features, applies appropriate deconvolutionallayers to up-sample the feature map of the last convolution layer to restore it to the same size ofthe input image, and classiﬁed the up-sampled feature map pixel by pixel. Accordingly, series ofconvolution-based per-pixel-label models have been developed including SegNet 40], UNet 41],DeepLab 42], and ReSeg 43]. Of these, SegNet and UNet have the clearest and easiest-to-understandconvolution structures. DeepLab uses method called “Atrous Convolution”, which has strongadvantage in processing detailed images. ReSeg exploits local generic features extracted by CNNs andthe capacity of recurrent neural networks to retrieve distant dependencies. Each model has its ownstrengths and is adept at dealing with certain image types. As conditional random ﬁeld (CRF) havethe ability to learn the dependencies between categories of pixels, CRF can be used to further reﬁnesegmentation results 44].These convolution-based per-pixel-label models have been applied in remote sensing imagesegmentation with remarkable results. For example, researchers have used CNN to carry out remotesensing image segmentation and used conditional random ﬁelds to further reﬁne the output classmap 45–48]. To suit the characteristics of speciﬁc remote sensing imagery, other researchers haveestablished new convolution-based per-pixel-label models, such as multi-scale fully convolutionalnetworks 49], patch-based CNNs 50], and two-branch CNNs 51]. Effective work has also been carriedout in extracting information from remote sensing imagery using convolution-based per-pixel-labelmodels, e.g., extracting crop information for rice 52,53], wheat 54], leaf 55], and rape 56], as well astarget detection for weeds 57–59], diseases 60–62], and extracting road information using improvedFCN 63]. Some new feature extraction techniques are being applied to crop information extraction,Remote Sens. 2019,11, 619 of 21including 3D-CNN 64], deep recurrent neural networks 65], and CNN-LSTM 66], and RecurrentNeural Networks (RNN) was also used to correct satellite image classiﬁcation maps 67]. Some newtechniques are proposed to improve the segmentation accuracy, including structured autoencoders 68]and locality adaptive discriminant analysis 69]. Moreover, the research on how to automaticallydetermine the feature dimension that could be adaptive to different data distributions will help toobtain good performance in machine learning and computer vision 70].How to determine the optimal value of the parameters is an important problem in the use ofconvolutional neural networks. Stochastic gradient descent with momentum 45] is common andeffective training method. Data augmentation technology 33,35,41] and dropout technology 33] usedto prevent overﬁtting, so as to ensure that the model can obtain the optimal parameters. Practice hasproved that reasonable use of BN (Batch Normalization) layer is also helpful for model training toobtain the optimal parameters 42,43].At present, the CNN structure used in the pre-pixel classiﬁcation of remote sensing imagerygenerally includes two parts: feature extractor and classiﬁer. The former has been the focus of manyresearchers with good results. The convolution value acquired by the convolution kernel and pixelblock operations is regarded as feature of central pixels in the pixel blocks and is the commontechnique for existing feature extractors. However, with regard to classifying pixels with acquiredfeatures, most studies have only used classiﬁers with relatively ordinary functions. These classiﬁers usea set of linear regression functions to encode the features of pixels and obtained category-code vectors.The SoftMax function is then used to convert the category-code vector into category probabilityvector, and the category corresponding to the maximum probability value is taken as the pixel category.Previous experimental results 44–56] have shown that misclassiﬁed pixels are primarily locatedat the intersections of two land use types, such as ﬁeld edges or corners. This is because when thefeatures of pixels in these areas are acquired, the used pixel blocks usually contain more pixels ofother categories, resulting in the features often being different from the feature of inner pixels of theplanting area, which frequently cause classiﬁcation errors. By analyzing the probability vector ofthese misclassiﬁed pixels, it can be found that the difference between the maximum probability valueand the second-maximum probability value is generally small. These errors are due to the inherentstructure of the convolutional layer, which needs to be combined with the classiﬁer to be improved.The Bayesian model can synthesize information from different sources and improve the reliabilityof inferred conclusions 71,72]. Therefore, when judging the category of pixel whose differencebetween the maximum probability value and the second-maximum probability value is small, thespatial structure information of the pixels can be further introduced to improve the reliability of thejudgment by using the Bayesian model. In this study, we developed new CNN consisting of afeature extractor, encoder, and Bayesian classiﬁer, which we refer to as Bayesian ConvolutionalNeural Network (CNN-Bayesian model). We then used this model to extract winter wheat spatialdistribution information from Gaofen (GF-2) remote sensing imagery and compared the results withthose achieved by other methods.2. Study Area and Data2.1. Study AreaShandong Province is major wheat-producing area in China. The total planted area was 38,303km2in 2016 and 38,429 km2in 2017 (Figure 1)[73]. Zhangqiu County is located in North-centralShandong Province (36250–37090N, 117100–117350E). From south to north, the county’s terrainprogresses through mountainous, hilly, plain, and lowland regions, accounting for 30.8%, 25.9%, 30.7%,and 12.6% of the total area, respectively. The main food crops are wheat and corn 74]. Moreover, thecounty’s geographical and agricultural conditions are representative of broader regions within China,making it an appropriate study area.Remote Sens. 2019,11, 619 of 21Remote Sens. 2019, 11, FOR PEER REVIEW of 22 Figure 1. Regional distribution of wheat planting in Shandong Province, China, and the location of Zhangqiu County (red outline). 2.1. Data Sources 2.2.1. Remote Sensing Imagery We used 32 GF-2 images to cover the entire Zhangqiu County; 17 were captured on 14 February, 2017, and 15 on 21 January and March, 2018 (Figure 2a). Each GF-2 image is divided into multispectral and panchromatic image. The former is composed of four spectral bands (blue, green, red, and near-infrared), and the spatial resolution of each multispectral image is m, whereas that of the panchromatic image is m. Figure 1. Regional distribution of wheat planting in Shandong Province, China, and the location ofZhangqiu County (red outline).2.2. Data Sources2.2.1. Remote Sensing ImageryWe used 32 GF-2 images to cover the entire Zhangqiu County; 17 were captured on 14 February,2017, and 15 on 21 January and March, 2018 (Figure 2a). Each GF-2 image is divided into amultispectral and panchromatic image. The former is composed of four spectral bands (blue, green,red, and near-infrared), and the spatial resolution of each multispectral image is m, whereas that ofthe panchromatic image is m.The preprocessing of the GF-2 images involved four stages: geometric correction, radiometriccalibration, atmospheric correction, and image fusion. Using Python and the Geospatial DataAbstraction Library, we designed geometric correction program and completed this by combiningthe control points obtained from the ground investigation. Radiometric calibration converted theimages’ digital values to absolute at-sensor radiance values using Environment for Visualizing Images(ENVI) software (developed by Harris Geospatial Solutions, Broomﬁeld, Colorado, United States ofAmerica). The calibration parameters were obtained by calibration experiments in Chinese ﬁelds aspublished in CRESDA 9]. Atmospheric correction converted the radiance to reﬂectance using theFast Line-of-Sight Atmospheric Analysis of Spectral Hypercubes (FLAASH) model in ENVI with theInteractive Data Language. The related FLAASH parameters were obtained according to the acquisitiontime and imaging conditions. Subsequently, the ENVI pan-sharpening method was used to fuse themultispectral and panchromatic images. After preprocessing, each fusion image had four bands (blue,green, red, and near-infrared) with spatial resolution of m and size of 7300 ⇥6900 pixels.Remote Sens. 2019,11, 619 of 21Remote Sens. 2019, 11, FOR PEER REVIEW of 22 Figure 1. Regional distribution of wheat planting in Shandong Province, China, and the location of Zhangqiu County (red outline). 2.1. Data Sources 2.2.1. Remote Sensing Imagery We used 32 GF-2 images to cover the entire Zhangqiu County; 17 were captured on 14 February, 2017, and 15 on 21 January and March, 2018 (Figure 2a). Each GF-2 image is divided into multispectral and panchromatic image. The former is composed of four spectral bands (blue, green, red, and near-infrared), and the spatial resolution of each multispectral image is m, whereas that of the panchromatic image is m. Figure 2. Data sources: a) Gaofen remote sensing imagery of Zhangqiu County; and b) samplepoint locations within the county.2.2.2. Ground Investigation DataThe main land cover in Zhangqiu County during winter includes winter wheat, agriculturalbuildings, woodland, developed land, roads, water bodies, farmland, and bare ﬁelds. In fused GF-2images, bare ﬁelds, agricultural buildings, developed land, water bodies, farmland, and roads are allvisually distinct from each other and vegetated areas during winter. In order to accurately distinguishwhether vegetation area is winter wheat or woodland in visual interpretation, the sample informationof winter wheat areas and woodland areas should be obtained, so we conducted ground investigationsin 2017 and 2018, obtaining 367 sample points (251 winter wheat, 116 woodland); time, location, andland use were recorded for all points (Figure 2b).2.3. Image-Label DatasetsWe selected 305 non-overlapping region images from the GF-2 images described in Section 2.2toestablish the image-label dataset for training and test, and each image contained 1024 ⇥1024 pixels.The dataset covered all land use types of the study area, including winter wheat, agricultural buildings,woodland, developed land, roads, water bodies, farmland, and bare ﬁelds. We manufactured labelﬁle for each image, which was used to record the category number of each pixel on the image. Incombination with the ground investigation data described in Section 2.2.2, we used visual interpretationand ENVI software to establish the label ﬁle. Figure 3illustrates training image and correspondinglabel ﬁle.In the label ﬁles, winter wheat, agricultural buildings, woodland, developed land, roads, waterbodies, bare ﬁelds, and others were marked 1–8, respectively. In the test stage, 2–8 will be replaced by9, indicating that the corresponding pixel is non-winter wheat pixel.Remote Sens. 2019,11, 619 of 21Remote Sens. 2019, 11, FOR PEER REVIEW of 22 Figure 3. Example of image classification: (a) original Gaofen image and (b) classified by land use type. In the label files, winter wheat, agricultural buildings, woodland, developed land, roads, water bodies, bare fields, and others were marked 1–8, respectively. In the test stage, 2–8 will be replaced by 9, indicating that the corresponding pixel is non-winter wheat pixel. 3. Proposed CNN-Bayesian Model 3.1. Model Architecture The proposed CNN-Bayesian model consists of feature extractor used to generate feature vectors for each pixel, an encoder used to transform the feature vector of each pixel into category-code vector, and classifier used to determine the category of pixel (Figure 4). Figure 4. Architecture of the proposed CNN-Bayesian model; ReLU: rectified linear unit. 3.1.1. Feature Extractor The feature extractor’s network structure is based on VGG16 network [30] in that it consists of 13 layers (corresponding to the first 13 layers of VGG16); each layer includes convolution, batch Figure 3. Example of image classiﬁcation: a) original Gaofen image and b) classiﬁed by landuse type.3. Proposed CNN-Bayesian Model3.1. Model ArchitectureThe proposed CNN-Bayesian model consists of feature extractor used to generate feature vectorsfor each pixel, an encoder used to transform the feature vector of each pixel into category-code vector,and classiﬁer used to determine the category of pixel (Figure 4).Remote Sens. 2019, 11, FOR PEER REVIEW of 22 Figure 3. Example of image classification: (a) original Gaofen image and (b) classified by land use type. In the label files, winter wheat, agricultural buildings, woodland, developed land, roads, water bodies, bare fields, and others were marked 1–8, respectively. In the test stage, 2–8 will be replaced by 9, indicating that the corresponding pixel is non-winter wheat pixel. 3. Proposed CNN-Bayesian Model 3.1. Model Architecture The proposed CNN-Bayesian model consists of feature extractor used to generate feature vectors for each pixel, an encoder used to transform the feature vector of each pixel into category-code vector, and classifier used to determine the category of pixel (Figure 4). Figure 4. Architecture of the proposed CNN-Bayesian model; ReLU: rectified linear unit. 3.1.1. Feature Extractor The feature extractor’s network structure is based on VGG16 network [30] in that it consists of 13 layers (corresponding to the first 13 layers of VGG16); each layer includes convolution, batch Figure 4. Architecture of the proposed CNN-Bayesian model; ReLU: rectiﬁed linear unit.3.1.1. Feature ExtractorThe feature extractor’s network structure is based on VGG16 network 30] in that it consists of13 layers (corresponding to the ﬁrst 13 layers of VGG16); each layer includes convolution, batchnormalization, activation, and pooling layer. Like VGG16, the CNN-Bayesian model uses rectiﬁedRemote Sens. 2019,11, 619 of 21linear unit as an activation function. We added 10 convolution kernels (sized ⇥1⇥3) in the ﬁrstlayer to extract the color features of pixels.The input of the feature extractor is the fused GF-2 remote sensing images, each with four bands.The output is 3D matrix with size of ⇥n⇥l, where and are the number of rows and columnsrespectively, and is the length of the feature vector of each pixel. Each feature vector correspondingto one pixel consists of three parts. The ﬁrst is derived from the result of the convolution kernel, whichrepresents the color feature. The second is derived from the result of the ﬁrst layer, which representsthe low-level texture features. The third is derived from the output of the last layer, which representsthe semantic feature.Compared with the camera image, the pixels of the remote sensing image are continuous.Therefore, we used the extension method to cut out the training and test images and then extend somepixels on the four edges of each image, to ensure that the size of the last layer’s feature image was thesame as the original image.We improved the original pooling method of VGG16 using the following equation:as,t= maxi=s1,s,s+1j=t1,t,t+1bi,j, (1)where s,tdenotes the position of the pixel being calculated, adenotes the pooled result, and bdenotesthe feature map used in the pooled operation.We used step size of in the pooling operation. After feature map whose size is ⇥n hasbeen pooled, the size of the resulting matrix is (m 2)⇥(n2). Therefore, after each layer of featureextraction, the image size is reduced by four rows and four columns compared with the original image.Therefore, when we cut the training and test images, we extended 24 pixels outward on the four edgesof each image.3.1.2. EncoderThe encoder is used to transform the feature vector of pixel from the feature extractor into acategory-code vector, as shown below:266664r1r2...rm377775=266664w11w12 ··· w1nw21...wm1w22...wm2···...···w2n...wmn377775⇥hx1x2 ··· xniT+266664b1b2...bm377775, (2)where each row of the matrix windicates ﬁtting function for speciﬁed class, mdenotes the numberof classes, ndenotes the length of the feature vector of one pixel, vector xdenotes the feature vector,vector bdenotes the respective biases, and vector rdenotes the encoded value. The matrix wand vectorbare trained in the training stage.3.1.3. ClassiﬁerThe classiﬁer is divided into two levels, and B. The A-level classiﬁer transforms category-codevector r(corresponding to one pixel) into category-probability vector pas follows:266664p1p2...pm377775=2666664er1Âmierier2Âmieri...ermÂmieri3777775, (3)Remote Sens. 2019,11, 619 of 21where mdenotes the number of classes. Next, the conﬁdence level (CL) of each pis calculated asfollows:CL=pipj, (4)where pidenotes the max value in p, and pjdenotes the max value in pexcept pi. The category of agiven pixel is determined by:Aout =8><>:i;in train stage and iis the maxvalue in P1;in classification stage and CL d11;in classification stage and CL <d, (5)where Aout denotes the category number of one pixel, code indicates winter wheat, code 11indicates uncertainty, and dindicates the low threshold value of CL. The dvalue is selected anddetermined manually after the training has been completed, and the training results of all samples arestatistically analyzed.The B-level classiﬁer is used to determine the category of pixel whose CL<d, denoted by vPixel,by acquiring the maximum posterior probability of vPixel classiﬁed as winter wheat:vww=P(c=ww|?)=P(?|c=ww)P(c=ww)P(?), (6)where vwwdenotes the maximum posterior probability that the category of vPixel is winter wheat whentheCLvalue is ?,cis the category, wwdenotes winter wheat, ?denotes the CLvalue corresponding tothePof vPixel, p(?|c=ww) represents the probability that the CLvalue is equal to ?in winter wheatpixels, p(c=ww) indicates the probability of winter wheat, and p(?)indicates the probability that theCLvalue is equal to ?in all pixels. Next, the maximum posterior probability of vPixel classiﬁed asnon-winter wheat is acquired by:vnw=P(c=nw|?)=P(?|c=nw)P(c=nw)P(?), (7)where vnwdenotes the maximum posterior probability that the category vPixel is not winter wheatwhen the CLvalue is ?,c,?, and p(?)have the same meaning as in the Equation (6), nwdenotesnon-winter wheat, p(?|c=nw) indicates the probability that the CLvalue is equal to ?in non-winterwheat pixels, and p(c=nw) indicates the probability of non-winter wheat.In Equations (6) and (7), p(?|c=ww), p(c=ww), p( ?|c=nw), p(c=nw), and p( ?)are acquired bystatistical methods. When obtaining p(?|c=ww) andp(?|c=nw) all samples are statistics, reﬂectingthe global characteristics of the conﬁdence of certain classes. When obtaining p(c=ww), p(c=nw) andp(?), only samples in the maximum pixel block are used to extract the features of vPixel, reﬂecting thelocal characteristics of pixel spatial associations.The classiﬁer determines the ﬁnal pixel category as follows:out=8>>>>>>><>>>>>>>:1;(p1is the maxvalue in and CL d)or(CL<dand ww>vnw)or(CL<d,vww=vnwand 1is the maxvalue in )9;(p1is not the maxvalue in and CL d))or(CL<dand ww<vnw)or(CL<d,vww=vnwand 1is not the maxvalue in ), (8)where outrepresents the ﬁnal category number of one pixel, code indicates winter wheat, and code 9indicates non-winter wheat.Remote Sens. 2019,11, 619 of 213.2. Training ModelThe basic loss function calculation unit is the deﬁnition of cross entropy, expressed for onesample as:H(p,q)=Â8i=1qilog(pi), (9)where pis the predicted category probability distribution, qis the actual category probabilitydistribution, and iis the index of an element in the category probability distribution. On this basis, theloss function of the CNN-Bayesian model is deﬁned as:loss=1tsÂtsÂ8i=1qilog(pi), (10)where tsdenotes the pixel amount used in the training stage.We trained the CNN-Bayesian model in an end-to-end manner, B-level classiﬁer does notparticipate in the training stage. The parameters required for B-level classiﬁer to perform calculationsare obtained by statistics after training completed. The training stage consists of the following steps:1. Image-label pairs are input into the CNN-Bayesian model as training sample dataset, andparameters are initialized.2. Forward propagation is performed on the sample images.3. The loss is calculated and back-propagated to the CNN-Bayesian model.4. The network parameters are updated using the stochastic gradient descent 45] with momentum.Steps 2–4 are iterated until the loss is less than the predetermined threshold values.Table 1shows the hyperparameters setup we used to train our model. In the comparisonexperiments, the hyperparameters also applied to the comparison model.Table 1. The hyperparameters setup.Hyperparameter Valuemini-batch size 32learning rate 0.0001momentum 0.9epochs 20,0003.3. Work FlowFirst, set of ﬁxed-size pixel blocks are cut from the pre-processed remote sensing image setto form the image set for training and testing. The training images are labeled pixel by pixel usingvisual interpretation. These data are then used to train the CNN-Bayesian model (loss value of10–9in this study). The predicted category, actual category, and CL of each sample are output aftereach round of training. Subsequently, the training information of the last round is used to acquirethe conﬁdence threshold d(0.23 in this study) and the probability distributions p( ?|c=ww) andp(?|c=nw). Finally, the trained model is used to exact winter wheat spatial distribution informationform remote sensing images.4. Experimental Results4.1. Experimental SetupsThe proposed CNN-Bayesian model was implemented using Python 3.6 on Linux Ubuntu 16.04operating system and TensorFlow framework. The comparison experiments were performed on agraphics workstation with an NVIDIA GeForce Titan Graphics device with 12 GB graphic memory.The network architecture parameters of the feature extractor of CNN-Bayesian model and thedata dimensions of each layer are given in Table 2.Remote Sens. 2019,11, 619 10 of 21Table 2. Network architecture parameters of the feature extractor and data dimensions.Layer Operations Parameters1 Data Dimensionof InputData Dimensionof Output1Convolutional = ⇥3⇥3, = 1, = 64 748 ⇥748⇥3 746 ⇥746⇥64pooling = ⇥3, = 746 ⇥746⇥64 744 ⇥744⇥642Convolutional = ⇥3⇥64, = 1, = 64 744 ⇥744⇥64 742 ⇥742⇥64pooling = ⇥3, = 742 ⇥742⇥64 740 ⇥740⇥643Convolutional = ⇥3⇥64, = 1, = 64 740 ⇥740⇥64 740 ⇥740⇥64pooling = ⇥3, = 740 ⇥740⇥64 738 ⇥738⇥644Convolutional = ⇥3⇥64, = 1, = 128 738 ⇥738⇥64 736 ⇥736⇥128pooling = ⇥3, = 736 ⇥736⇥128 734 ⇥734⇥1285Convolutional = ⇥3⇥128, = 1, = 128 734 ⇥734⇥128 732 ⇥732⇥128pooling = ⇥3, = 732 ⇥732⇥128 730 ⇥730⇥1286Convolutional = ⇥3⇥128, = 1, = 128 730 ⇥730⇥128 728 ⇥728⇥128pooling = ⇥3, = 728 ⇥728⇥128 726 ⇥726⇥1287Convolutional = ⇥3⇥128, = 1, = 256 726 ⇥726⇥128 724 ⇥724⇥256pooling = ⇥3, = 724 ⇥724⇥256 722 ⇥722⇥2568Convolutional = ⇥3⇥256, = 1, = 256 722 ⇥722⇥256 720 ⇥720⇥256pooling = ⇥3, = 720 ⇥720⇥256 718 ⇥718⇥2569Convolutional = ⇥3⇥256, = 1, = 256 718 ⇥718⇥256 718 ⇥718⇥256pooling = ⇥3, = 718 ⇥718⇥256 716 ⇥716⇥25610Convolutional = ⇥3⇥256, = 1, = 512 716 ⇥716⇥256 714 ⇥714⇥512pooling = ⇥3, = 714 ⇥714⇥512 712 ⇥712⇥51211Convolutional = ⇥3⇥512, = 1, = 512 712 ⇥712⇥512 710 ⇥710⇥512pooling = ⇥3, = 710 ⇥710⇥512 708 ⇥708⇥51212Convolutional = ⇥3⇥512, = 1, = 512 708 ⇥708⇥512 706 ⇥706⇥512pooling = ⇥3, = 706 ⇥706⇥512 704 ⇥704⇥51213Convolutional = ⇥3⇥512, = 1, = 512 704 ⇥704⇥512 702 ⇥702⇥512pooling = ⇥3, = 702 ⇥702⇥512 700 ⇥700⇥512Output 700⇥700⇥586f denotes the size of the convolution/pooling kernel, represents the step length, and represents the number ofconvolution cores in this layer. Because the batch normalization and rectiﬁed linear unit layers do not change thesize of the data dimensions, they are not listed in the table.SegNet 35] and DeepLab 37] are classic semantic segmentation models for images that haveachieved good results in the processing of camera images. Moreover, the working principles of thesetwo models are similar to that of our study, and we therefore chose these as comparison models tobetter reﬂect the advantages of our model in feature extraction and classiﬁcation. We also removed thesecond-level classiﬁer of the CNN-Bayesian model as another comparison model, named VGG-Ex, tobetter compare the role of the Bayesian classiﬁer.We used data augmentation techniques on the dataset to prevent overﬁtting, and each image wasrandomly processed in brightness, saturation, hue, and contrast. After the processing is completed,each image is rotated and transformed, and each image is rotated three times (90, 180, 270). Thereare 6100 images in our ﬁnal data set. We also employed random split technique for training and testingmodel to prevent overﬁtting. During each training and test round, 4880 images randomly selectedfrom the image-label datasets were used as training data, and the remaining 1220 images were usedas test data. The SegNet, DeepLab, VGG-Ex, and CNN-Bayesian model were trained with the sameimage dataset. This was done ﬁve times. Table 3shows the total number of samples of each categoryused in each training and test round.Remote Sens. 2019,11, 619 11 of 21Table 3. Total number of samples of each category used in each training and test round.Category Number of Total Samples (Million)Winter wheat 1572Agricultural buildings 6Woodland 568Developed land 1199Roads 51Water bodies 57Farmland 1521Bare ﬁelds 13324.2. Results and EvaluationTable 4shows the confusion matrices for the segmentation results of the four models. Each rowof the confusion matrix represents the proportion taken by the actual category, and each columnrepresents the proportion taken by the predicted category. Our approach achieved better classiﬁcationresults. The proportion of “winter wheat” wrongly categorized as “non-winter wheat” was, onaverage, 0.033, and the proportion of “non-winter wheat” wrongly classiﬁed as “winter wheat” was,on average, 0.021.Table 4. Confusion matrix of the winter wheat classiﬁcation.Approach Predicted Winter Wheat Non-Winter WheatCNN-BayesianWinter wheat 0.669 0.021Non-winter wheat 0.033 0.277VGG-ExWinter wheat 0.631 0.059Non-winter wheat 0.049 0.261SegNetWinter wheat 0.574 0.116Non-winter wheat 0.093 0.217DeepLabWinter wheat 0.605 0.085Non-winter wheat 0.063 0.247In this paper, we used four popular criteria, named accuracy, precision, recall and Kappacoefﬁcient to evaluate the performance of the proposed model 45]. Table 5shows the values ofevaluation criteria of the four models.Table 5. Comparison of the four models’ performance.Index CNN-Bayesian VGG-Ex SegNet DeepLabAccuracy 0.946 0.892 0.791 0.852Precision 0.932 0.878 0.766 0.837Recall 0.941 0.872 0.756 0.825Kappa 0.879 0.778 0.616 0.712To further compare the classiﬁcation accuracy of planting area edges, we further subdividedthe categories into “inner” and “edge” labels. If only winter wheat category pixels are used in theconvolution process to extract the pixel features, it is classiﬁed as inner; otherwise it is classiﬁed asedge. Table 6show the confusion matrices for the segmentation results of the four models.Remote Sens. 2019,11, 619 12 of 21Table 6. Confusion matrix for winter wheat inner/edge classiﬁcation.Approach PredictedWinter WheatInnerWinter WheatEdgeNon-WinterWheatCNN-BayesianWinter wheat inner 0.542 0.001Winter wheat edge 0.127 0.02Non-winter wheat 0.006 0.027 0.277VGG-ExWinter wheat inner 0.539 0.012Winter wheat edge 0.092 0.047Non-winter wheat 0.008 0.041 0.261SegNetWinter wheat inner 0.532 0.035Winter wheat edge 0.042 0.081Non-winter wheat 0.033 0.06 0.217DeepLabWinter wheat inner 0.538 0.026Winter wheat edge 0.067 0.059Non-winter wheat 0.015 0.048 0.247As can be seen from Table 4, the accuracy of inner category of four models’ results were similar,but the CNN-Bayesian model was more accurate with regard to the edge category. The accuracy ofCNN-Bayesian model in edge recognition is three times higher than that of SegNet, two times higherthan that of DeepLab. By comparing the accuracy of winter inner edge of CNN-Bayesian and that ofVGG-Ex, it can be found that the ability of CNN-Bayesian to recognize winter wheat edge is improvedby nearly 30% due to the use of Bayesian classiﬁer.Figure 5shows ten images and corresponding results randomly selected from the tested images,each containing 1204 ⇥1024 pixels. The CNN-Bayesian model misclassiﬁed only small number ofpixels at the corner of the winter wheat planting area. In the DeepLab results and VGG-Ex results, themisclassiﬁed pixels were mainly distributed at the junction of winter wheat and non-winter wheatareas, including edge and corner locations, but the number of misclassiﬁed pixels in the VGG-Exmodel results is less than that of the DeepLab. The SegNet results had the most errors, which werescattered throughout the image; most misclassiﬁed pixels were located on the edges and corners, withsome also occurring in the planting area.Remote Sens. 2019,11, 619 13 of 21Remote Sens. 2019, 11, FOR PEER REVIEW 13 of 22 Figure 5. Comparison of segmentation results for Gaofen satellite imagery: a) original images,(b) ground truth, c) results of CNN-Bayesian, d) results of VGG-Ex, e) results of SegNet, and f)results of DeepLab.Remote Sens. 2019,11, 619 14 of 215. DiscussionsThis paper proposed novel per-pixel classiﬁcation approach to extract winter wheat spatialdistribution from GF-2 imagery. This approach can extract winter wheat with ﬁne ﬁeld edge by usingtwo strategies, including CNN structure to extract features and two-level classiﬁer to determine thepixel’s category accurately. The contributions of these two strategies are discussed as follows.5.1. The Effectiveness of Feature ExtractorTo distinguish winter wheat from other categories, popular deep learning algorithm CNN wasapplied to explore the features. The trained feature extractor of proposed CNN-Bayesian model hasstrong feature extraction ability, which can make the distances between feature vectors extractedfrom pixels of the same category, but with different spectral information, close, and make thedistances between feature vectors extracted from pixels from different category, but with close spectralinformation, far away.Since the CNN-Bayesian model and the VGG-Ex model use the same feature extractor, weselected the most different set of semantic features from the last layer of the CNN-Bayesian, SegNet,and DeepLab models for comparative analysis, Figure 6show the statistical results, respectively. Thedegree of confusion in the CNN-Bayesian model results is smaller than that in the other two modelsbecause its network structure and data organization mode are better, and the improved poolingalgorithm used in feature extractor has larger receptive ﬁeld, and has greater advantage in featureaggregation than the classical pooling algorithm. The CNN-Bayesian model feature extractor can keepthe size of the feature image of the last layer unchanged without using deconvolution. Furthermore, itcan eliminate location errors of the feature value that may be caused by the deconvolution operationand ensure one-to-one correspondence between the feature value and the pixel, thus reducing thedegree of confusion between the features of winter wheat edge and non-winter wheat areas. Comparedwith the comparison model, the CNN-Bayesian model better suits the data features of high-resolutionremote sensing images.As can been seen for the statistical result of SegNet, although the feature values of winter wheatinner pixels and winter wheat edge pixels are scattered, the feature values of winter wheat inner pixelsare basically not overlapped with the feature values of other categories. However, the overlap betweenthe feature values of winter wheat edge pixels and other categories is large, which is the reason thatthe accuracy of winter wheat inner higher than that of winter wheat edge.The feature values of some winter wheat edge pixels were confused with those of non-winterwheat pixels in all three cases, but those of winter wheat inner pixels were never confused with thoseof non-winter wheat pixels. This shows that pixel position has great impact on the feature extractionresults, mainly for two main reasons: First, ﬁeld edge pixel information is different from inner pixelinformation, because edge areas often contain both winter wheat and bare ﬁelds or other land usetypes, and the proportion of winter wheat varies greatly, whereas inner areas contain only winterwheat. Second, pixel blocks centered on pixels at the edge of winter wheat ﬁelds, usually containmore non-winter wheat than wheat pixels (Figure 7). Thus, when extracting the feature values of theseedge pixels, approximately 50% of the pixels involved in the convolution operation are pixels of othercategories, whereas the ratio for corners is 75% or higher.Remote Sens. 2019,11, 619 15 of 21Remote Sens. 2019, 11, FOR PEER REVIEW 15 of 22 Figure 6. Statistical results of the (a) CNN-Bayesian, (b) SegNet, and (c) DeepLab. Figure 6. Statistical results of the a) CNN-Bayesian, b) SegNet, and c) DeepLab.Remote Sens. 2019,11, 619 16 of 21Remote Sens. 2019, 11, FOR PEER REVIEW 16 of 22 As can been seen for the statistical result of SegNet, although the feature values of winter wheat inner pixels and winter wheat edge pixels are scattered, the feature values of winter wheat inner pixels are basically not overlapped with the feature values of other categories. However, the overlap between the feature values of winter wheat edge pixels and other categories is large, which is the reason that the accuracy of winter wheat inner higher than that of winter wheat edge. The feature values of some winter wheat edge pixels were confused with those of non-winter wheat pixels in all three cases, but those of winter wheat inner pixels were never confused with those of non-winter wheat pixels. This shows that pixel position has great impact on the feature extraction results, mainly for two main reasons: First, field edge pixel information is different from inner pixel information, because edge areas often contain both winter wheat and bare fields or other land use types, and the proportion of winter wheat varies greatly, whereas inner areas contain only winter wheat. Second, pixel blocks centered on pixels at the edge of winter wheat fields, usually contain more non-winter wheat than wheat pixels (Figure 7). Thus, when extracting the feature values of these edge pixels, approximately 50% of the pixels involved in the convolution operation are pixels of other categories, whereas the ratio for corners is 75% or higher. Figure 7. Examples of the effect of pixel position on the extracted features; pixel boxes (red) centered on corner or edge areas contain 50% or more non-winter wheat pixels. 5.2. The Effectiveness of Classifier Both the CNN-Bayesian model and the comparison model use the category-probability vector as the basis for determining the category of the pixels. The main advantage of the CNN-Bayesian model is that it takes into account the deep meaning of the difference between elements of category-probability vector, and use hierarchical strategy to determine the category of the pixels. The category of pixels with high confidence were directly determined, and the category of pixels with low confidence were determined combining prior knowledge. VGG-Ex, SegNet and DeepLab only use the maximum probability value as the basis to determine the category of the pixels. Therefore, the strategy adopted by the CNN-Bayesian model helps to improve the accuracy of the results, and the results are shown and compared in Figure 5, Table and Table 4. Figure 7. Examples of the effect of pixel position on the extracted features; pixel boxes (red) centeredon corner or edge areas contain 50% or more non-winter wheat pixels.5.2. The Effectiveness of ClassiﬁerBoth the CNN-Bayesian model and the comparison model use the category-probability vector asthe basis for determining the category of the pixels. The main advantage of the CNN-Bayesian model isthat it takes into account the deep meaning of the difference between elements of category-probabilityvector, and use hierarchical strategy to determine the category of the pixels. The category of pixelswith high conﬁdence were directly determined, and the category of pixels with low conﬁdence weredetermined combining prior knowledge. VGG-Ex, SegNet and DeepLab only use the maximumprobability value as the basis to determine the category of the pixels. Therefore, the strategy adoptedby the CNN-Bayesian model helps to improve the accuracy of the results, and the results are shownand compared in Figure 5, Tables 3and4.We select the number of pixels in each conﬁdence level of the CNN-Bayesian, VGG-Ex, SegNet,and DeepLab models for comparative analysis (Figure 8). The pixel ratio of the SegNet and DeepLabmodels is higher than that of the CNN-Bayesian model and VGG-Ex at lower conﬁdence level. Thisshows that the feature composition of the CNN-Bayesian model is more reasonable, because it usescolor and texture features in addition to the high-level semantic features used by all three models.As the conﬁdence increases, the classiﬁcation errors of the four models decrease and the degreeof reduction increases (Figure 9). This is because the conﬁdence value directly reﬂects the degree towhich the pixel characteristics match the overall category characteristics and, thus, the likelihood thatthe classiﬁcation result is correct. Therefore, it is reasonable to choose the conﬁdence value as the indexof the conﬁdence that given pixel will be classiﬁed into certain category.Overall, these results show that the CNN-Bayesian model is more capable than the comparisonmodels, reﬂecting its advantageous use of two-level classiﬁer structure. Since the second-levelclassiﬁer makes full use of the conﬁdence and planting structure information, the number ofmisclassiﬁed pixels is effectively reduced.Remote Sens. 2019,11, 619 17 of 21As can be seen from Figures 8and9, for the CNN-Bayesian model, the number of pixels withconﬁdence lower than 0.23 is small, but the proportion of misclassiﬁcation is very large. This is thereason we choose 0.23 as conﬁdence threshold described in Section 3.3.Remote Sens. 2019, 11, FOR PEER REVIEW 17 of 22 We select the number of pixels in each confidence level of the CNN-Bayesian, VGG-Ex, SegNet, and DeepLab models for comparative analysis (Figure 8). The pixel ratio of the SegNet and DeepLab models is higher than that of the CNN-Bayesian model and VGG-Ex at lower confidence level. This shows that the feature composition of the CNN-Bayesian model is more reasonable, because it uses color and texture features in addition to the high-level semantic features used by all three models. Figure 8. Distribution of confidence values for the four models. As the confidence increases, the classification errors of the four models decrease and the degree of reduction increases (Figure 9). This is because the confidence value directly reflects the degree to which the pixel characteristics match the overall category characteristics and, thus, the likelihood that the classification result is correct. Therefore, it is reasonable to choose the confidence value as the index of the confidence that given pixel will be classified into certain category. Figure 9. Distribution of misclassified pixels for all four models. Overall, these results show that the CNN-Bayesian model is more capable than the comparison models, reflecting its advantageous use of two-level classifier structure. Since the second-level classifier makes full use of the confidence and planting structure information, the number of misclassified pixels is effectively reduced. As can be seen from Figures and 9, for the CNN-Bayesian model, the number of pixels with confidence lower than 0.23 is small, but the proportion of misclassification is very large. This is the reason we choose 0.23 as confidence threshold described in Section 3.3. 5.3. Comparison to Other Similar Works Figure 8. Distribution of conﬁdence values for the four models.Remote Sens. 2019, 11, FOR PEER REVIEW 17 of 22 We select the number of pixels in each confidence level of the CNN-Bayesian, VGG-Ex, SegNet, and DeepLab models for comparative analysis (Figure 8). The pixel ratio of the SegNet and DeepLab models is higher than that of the CNN-Bayesian model and VGG-Ex at lower confidence level. This shows that the feature composition of the CNN-Bayesian model is more reasonable, because it uses color and texture features in addition to the high-level semantic features used by all three models. Figure 8. Distribution of confidence values for the four models. As the confidence increases, the classification errors of the four models decrease and the degree of reduction increases (Figure 9). This is because the confidence value directly reflects the degree to which the pixel characteristics match the overall category characteristics and, thus, the likelihood that the classification result is correct. Therefore, it is reasonable to choose the confidence value as the index of the confidence that given pixel will be classified into certain category. Figure 9. Distribution of misclassified pixels for all four models. Overall, these results show that the CNN-Bayesian model is more capable than the comparison models, reflecting its advantageous use of two-level classifier structure. Since the second-level classifier makes full use of the confidence and planting structure information, the number of misclassified pixels is effectively reduced. As can be seen from Figures and 9, for the CNN-Bayesian model, the number of pixels with confidence lower than 0.23 is small, but the proportion of misclassification is very large. This is the reason we choose 0.23 as confidence threshold described in Section 3.3. 5.3. Comparison to Other Similar Works Figure 9. Distribution of misclassiﬁed pixels for all four models.5.3. Comparison to Other Similar WorksAt present, there are some methods focus on improving the classiﬁcation accuracy of edgeregions 43–45,67]. These methods describe the association between inputs from the semantic level, sothat the relationship between prediction labels of adjacent pixels can be described, and the predictionresults are not only related to the features of the predicted pixels. Also relevant, and affected by theresults of previous predictions, our method can describe the statistical of inputs. The prediction resultis determined by the features of the pixel itself and the regional statistical features, which is more inline with the characteristics of remote sensing data.6. ConclusionsUsing satellite remote sensing has become mainstream approach for extracting winter wheatspatial distribution, but ﬁeld edge results are usually rough, resulting in lowered overall accuracy.In this paper, we proposed new approach for extracting spatial distribution information for winterwheat, which signiﬁcantly improves the accuracy of edge extraction results. The main contributions ofthis paper are as follows: (1) Our feature extractor is designed to meet the characteristics of remoteRemote Sens. 2019,11, 619 18 of 21sensing image data, avoiding extra calculations and errors caused by using deconvolution in the featureextraction process. The feature extractor can fully explore the deep and spatial semantic featuresof the remote sensing image. (2) Our classiﬁer effectively uses the conﬁdence value of the categoryprobability vector and combines the planting structure characteristics of winter wheat to reclassifypixels with low conﬁdence value, thus effectively reducing classiﬁcation errors for edge pixels. As weoptimized the method of extracting and using remote sensing image features and rationally used color,texture, semantic, and statistical features to obtain high-precision spatial distribution data of winterwheat. The spatial distribution data of winter wheat in Shandong Province in 2017 and 2018 obtainedby the proposed approach has been used by the Meteorological Bureau of Shandong Province.The number of categories that can be extracted by the proposed CNN-Bayesian model isdetermined by the number of categories of samples in the training dataset. When the model isused to extract other land use types or applied to another area, only new training dataset is neededto retrain the model. The successfully trained model can then be used to extract high-precision spatialdistribution data of land use from high-resolution remote sensing images.The main disadvantage of our approach is that it requires more pre-pixel label ﬁles. Futureresearch should test the use of semi-supervised classiﬁcation to reduce the dependence on pre-pixellabel ﬁles.Author Contributions: Conceptualization: C.Z., F.L., and S.G.; methodology: C.Z.; software: C.Z. and F.L.;validation: S.G., Y.H., and D.S.; formal analysis: C.Z. and H.Z.; investigation: K.F.; resources: Y.H.; data curation:Y.Z.; writing—original draft preparation: C.Z.; writing—review and editing: F.L. and S.G.; visualization: K.F.;supervision: C.Z.; project administration: C.Z. and F.L.; funding acquisition: C.Z., F.L., and S.G.Funding: This research was funded by the National Key and Program of China, grant number2017YFA0603004; the Science Foundation of Shandong, grant numbers ZR2017MD018 and ZR2016DP04; theNational Science Foundation of China, grant number 41471299; the Open Research Project of the Key Laboratoryfor Meteorological Disaster Monitoring, Early Warning and Risk Management of Characteristic Agriculture inArid Regions, grant numbers CAMF-201701 and CAMF-201803; and the Key Project of Shandong ProvincialMeteorological Bureau, grant number 2017sdqxz03.Conﬂicts of Interest: The authors declare no conﬂict of interest.References1. Websit of Food and Agriculture Organization of the United Nations. Available online: http://www.fao.org/faostat/zh/#data/QC (accessed on August 2018).2. Announcement of the National Statistics Bureau on Grain Output in 2017. Available online: http://www.gov.cn/xinwen/2017-12/08/content_5245284.htm (accessed on December 2017).3. Zhang, J.; Feng, L.; Yao, F. Improved maize cultivated area estimation over large scale combiningMODIS–EVI time series data and crop phenological information. ISPRS J. Photogramm. Remote Sens. 2014,94,102–113. CrossRef ]4. Chen, X.-Y.; Lin, Y.; Zhang, M.; Yu, L.; Li, H.-C.; Bai, Y.-Q. Assessment of the cropland classiﬁcations in fourglobal land cover datasets: case study of Shaanxi Province, China. J. Integnit. Agric. 2017,16, 298–311.[CrossRef ]5. Ma, L.; Gu, X.; Xu, X.; Huang, W.; Jia, J.J. Remote sensing measurement of corn planting area based onﬁeld-data. Trans. Chin. Soc. Agric. Eng. 2009,25, 147–151. (In Chinese) CrossRef ]6. McCullough, I.M.; Loftin, C.S.; Sader, S.A. High-frequency remote monitoring of large lakes with MODIS500 imagery. Remote Sens. Environ. 2012,124, 234–241. CrossRef ]7. Hao, H.E.; Zhu, X.F.; Pan, Y.Z.; Zhu, W.Q.; Zhang, J.S.; Jia, B. Study on scale issues in measurement of winterwheat plant area by remote sensing. J. Remote Sens. 2008,12, 168–175. (In Chinese)8. Wang, L.; Jia, L.; Yao, B.; Ji, F.; Yang, F. Area change monitoring of winter wheat based on relationshipanalysis of GF-1 NDVI among different years. Trans. Chin. Soc. Agric. Eng. 2018,34, 184–191. (In Chinese)[CrossRef ]9. Wang, D.; Fang, S.; Yang, Z.; Wang, L.; Tang, W.; Li, Y.; Tong, C. regional mapping method for oilseed rapebased on HSV transformation and spectral features. ISPRS Int. J. Geo-Informat. 2018,7, 224. CrossRef ]Remote Sens. 2019,11, 619 19 of 2110. Georgi, C.; Spengler, D.; Itzerott, S.; Kleinschmit, B. Automatic delineation algorithm for site-speciﬁcmanagement zones based on satellite remote sensingdata. Precis. Agric. 2018,19, 684–707. CrossRef ]11. Wang, L.; Guo, Y.; He, J.; Wang, L.; Zhang, X.; Liu, T. Classiﬁcation method by fusion of decision tree andSVM based on Sentinel-2A image. Trans. Chin. Soc. Agric. Mach. 2018,49, 146–153. (In Chinese) CrossRef ]12. Qian, X.; Li, J.; Cheng, G.; Yao, X.; Zhao, S.; Chen, Y.; Jiang, L. Evaluation of the effect of feature extractionstrategy on the performance of high-resolution remote sensing image scene classiﬁcation. J. Remote Sens.2018,22, 758–776. (In Chinese) CrossRef ]13. Wang, L.; Xu, S.; Li, Q.; Xue, H.; Wu, J. Extraction of winter wheat planted area in Jiangsu province usingdecision tree and mixed-pixel methods. Trans. Chin. Soc. Agric. Eng. 2016,32, 182–187. (In Chinese)[CrossRef ]14. Guo, Y.S.; Liu, Q.S.; Liu, G.H.; Huang, C. Extraction of main crops in Yellow River Delta based on MODISNDVI time series. J. Nat. Res. 2017,32, 1808–1818. (In Chinese) CrossRef ]15. Xu, Q.; Yang, G.; Long, H.; Wang, C.; Li, X.; Huang, D. Crop information identiﬁcation based on MODISNDVI time-series data. Trans. Chin. Soc. Agric. Eng. 2014,30, 134–144. (In Chinese) CrossRef ]16. Hao, W.; Mei, X.; Cai, X.; Du, J.; Liu, Q. Crop planting extraction based on multi-temporal remote sensingdata in Northeast China. Trans. Chin. Soc. Agric. Eng. 2011,27, 201–207. (In Chinese) CrossRef ]17. Feng, M.; Yang, W.; Zhang, D.; Cao, L.; Wang, H.; Wang, Q. Monitoring planting area and growth situationof irrigation-land and dry-land winter wheat based on TM and MODIS data. Trans. Chin. Soc. Agric. Eng.2009,25, 103–109. (In Chinese)18. Sha, Z.; Zhang, J.; Yun, B.; Yao, F. Extracting winter wheat area in Huanghuaihai Plain using MODIS-EVI dataand phenology difference avoiding threshold. Trans. Chin. Soc. Agric. Eng. 2018,34, 150–158. (In Chinese)[CrossRef ]19. Yang, P.; Yang, G. Feature extraction using dual-tree complex wavelet transform and gray level co-occurrencematrix. Neurocomputing 2016,197, 212–220. CrossRef ]20. Reis, S.; Tasdemir, K. Identiﬁcation of hazelnut ﬁelds using spectral and Gabor textural features. ISPRS J.Photogramm. Remote Sens. 2011,66, 652–661. CrossRef ]21. Naseera, M.T.; Asima, S. Detection of cretaceous incised-valley shale for resource play, Miano gas ﬁeld, SWPakistan: Spectral decomposition using continuous wavelet transform. Asian. Earth. Sci. 2017,147, 358–377.[CrossRef ]22. Liu, Y.; Bian, L.; Meng, Y.; Wang, H.; Zhang, S.; Yang, Y.; Shao, X.; Wang, Bo. Discrepancy measures forselecting optimal combination of parameter values in object-based image analysis. ISPRS J. Photogramm.Remote Sens. 2012,68, 144–156. CrossRef ]23. Blaschke, T.; Hay, G.J.; Kelly, M.; Lang, S.; Hofmann, P.; Addin, E.; Feitosa, R.; Meer, F.; Werff, H.; Coillie, F.;et al. Geographic Object-Based Image Analysis—Towards new paradigm. ISPRS J. Photogramm. RemoteSens. 2013,87, 180–191. CrossRef ]24. Wu, M.; Yang, L.; Yu, B.; Wang, Y.; Zhao, X.; Zheng, N.; Wang, C. Mapping crops acreages based on remotesensing and sampling investigation by multivariate probability proportional to size. Trans. Chin. Soc. Agric.Eng. 2014,30, 146–152. (In Chinese) CrossRef ]25. You, W.; Zhi, Z.; Wang, F.; Wu, Q.; Guo, L. Area extraction of winter wheat at county scale based on modiﬁedmultivariate texture and GF-1 satellite images. Trans. Chin. Soc. Agric. Eng. 2016,32, 131–139. (In Chinese)[CrossRef ]26. Liu, D.; Han, L.; Han, X. High spatial resolution remote sensing image classiﬁcation based on deep learning.Acta Opt. Sin. 2016,36, 0428001. (In Chinese) CrossRef ]27. Li, D.; Zhang, L.; Xia, G. Automatic analysis and mining of remote sensing big data. J. Surv. Mapp. 2014,43,1211–1216. (In Chinese) CrossRef ]28. Mas, J.F.; Flores, J.J. The application of artiﬁcial neural networks to the analysis of remotely sensed data.Int. J. Remote Sens. 2007,29, 617–663. CrossRef ]29. Paciﬁci, F.; Chini, M.; Emery, W.J. neural network approach using multi-scale textural metrics from veryhigh-resolution panchromatic imagery for urban land-use classiﬁcation. Remote Sens. Environ. 2009,113,1276–1292. CrossRef ]30. Liu, C.; Hong, L.; Chen, J.; Chu, S.; Deng, M. Fusion of pixel-based and multi-scale region-based featuresfor the classiﬁcation of high-resolution remote sensing image. J. Remote Sens. 2015,5, 228–239. (In Chinese)[CrossRef ]Remote Sens. 2019,11, 619 20 of 2131. Huang, X.; Zhang, L. An SVM ensemble approach combining spectral, structural, and semantic featuresfor the classiﬁcation of high-resolution remotely sensed imagery. IEEE Trans. Geosci. Remote Sens. 2013,51,257–272. CrossRef ]32. Mountrakis, G.; Im, J.; Ogole, C. Support vector machines in remote sensing: review. ISPRS J. Photogramm.Remote Sens. 2011,66, 247–259. CrossRef ]33. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet classiﬁcation with deep convolutional neural networks.Commun. ACM 2017,60, 84–90. CrossRef ]34. Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A.Going deeper with convolutions. In Proceedings of the 2015 IEEE Conference on Computer Vision andPattern Recognition (CVPR), Boston, MA, USA, 7–12 June 2015. CrossRef ]35. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv2014, arXiv:1409.1556.36. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 26 June–1 July 2016;pp. 770–778. CrossRef ]37. Chang, L.; Deng, X.M.; Zhou, M.Q.; Wu, Z.K.; Yuan, Y.; Yang, S.; Wang, H. Convolutional neural networks inimage understanding. Acta Autom. Sin. 2016,42, 1300–1312. (In Chinese) CrossRef ]38. Fischer, W.; Moudgalya, S.S.; Cohn, J.D.; Nguyen, N.T.T.; Kenyon, G.T. Sparse coding of pathology slidescompared to transfer learning with deep neural networks. BMC Bioinform. 2018,19, 489. CrossRef ]39. Long, J.; Shelhamer, E.; Darrell, T. Fully convolutional networks for semantic segmentation. IEEE Trans.Pattern Anal. Mach. Intell. 2014,39, 640–651.40. Badrinarayanan, V.; Kendall, A.; Cipolla, R. SegNet: deep convolutional encoder-decoder architecture forimage segmentation. arXiv 2015; arXiv:1505.07293. CrossRef ]41. Ronneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional networks for biomedical image segmentation.arXiv 2015; arXiv:1505.04597.42. Chen, L.; Papandreou, G.; Kokkinos, I.; Murphy, K.; Yuille, A.L. DeepLab: Semantic image segmentationwith deep convolutional nets, Atrous convolution, and fully connected CRFs. IEEE Trans. Patt. Anal. Mach.Intell. 2018,40, 834–848. CrossRef ]43. Visin, F.; Romero, A.; Cho, K.; Matteucci, M.; Courville, A. ReSeg: recurrent neural network-based modelfor semantic segmentation. arXiv 2016; arXiv:1511.07053.44. Zhang, L.; Wang, L.; Zhang, X.; Shen, P.; Bennamoun, M.; Zhu, G.; Shah, S.A.A.; Song, J. Semantic scenecompletion with dense CRF from single depth image. Neurocomputing 2018,318, 182–195. CrossRef ]45. Fu, G.; Liu, C.; Zhou, R.; Sun, T.; Zhang, Q. Classiﬁcation for high resolution remote sensing imagery using afully convolutional network. Remote Sens. 2017,9, 498. CrossRef ]46. Fu, K.; Lu, W.; Diao, W.; Yan, M.; Sun, H.; Zhang, Y.; Sun, X. WSF-NET: Weakly supervised feature-fusionnetwork for binary segmentation in remote sensing image. Remote Sens. 2018,10, 1970. CrossRef ]47. Castelluccio, M.; Poggi, G.; Sansone, C.; Verdoliva, L. Land use classiﬁcation in remote sensing images byconvolutional neural networks. arXiv 2015; arXiv:1508.00092.48. Nogueira, K.; Penatti, O.A.B.; Dos Santos, J.A. Towards better exploiting convolutional neural networks forremote sensing scene classiﬁcation. Pattern Recognit. 2017,61, 539–556. CrossRef ]49. Lin, H.; Shi, Z.; Zou, Z. Maritime semantic labeling of optical remote sensing images with multi-scale fullyconvolutional network. Remote Sens. 2017,9, 480. CrossRef ]50. Sharma, A.; Liu, X.; Yang, X.; Shi, D. patch-based convolutional neural network for remote sensing imageclassiﬁcation. Neural Netw. 2017,95, 19–28. CrossRef ]51. Gaetano, R.; Ienco, D.; Ose, K.; Cresson, R. two-branch CNN architecture for land cover classiﬁcation ofPAN and MS imagery. Remote Sens. 2018,10, 1746. CrossRef ]52. Duan, L.; Xiong, X.; Liu, Q.; Yang, W.; Huang, C. Field rice panicle segmentation based on deep fullconvolutional neural network. Trans. Chin. Soc. Agric. Eng. 2018,34, 202–209. (In Chinese) CrossRef ]53. Jiang, T.; Liu, X.; Wu, L. Method for mapping rice ﬁelds in complex landscape areas based on pre-trainedconvolutional neural network from HJ-1 A/B data. ISPRS Int. J. Geo-Inf. 2018,7, 418. CrossRef ]54. Hasan, M.M.; Chopin1, J.P.; Laga, H.; Miklavcic, S.J. Detection and analysis of wheat spikes usingConvolutional Neural Networks. Plant Methods 2018,14, 100. CrossRef ]Remote Sens. 2019,11, 619 21 of 2155. Rzanny, M.; Seeland, M.; Wäldchen, J.; Mäder, P. Acquiring and preprocessing leaf images for automatedplant identiﬁcation: Understanding the tradeoff between effort and information gain. Plant Methods 2017,13,97. [CrossRef ]56. Jiao, J.; Fan, Z.; Liang, Z. Remote sensing estimation of rape planting area based on improved AlexNetmodel. Comp. Meas. Cont. 2018,26, 186–189. (In Chinese) CrossRef ]57. Huang, H.; Deng, J.; Lan, Y.; Yang, A.; Deng, X.; Zhang, L. fully convolutional network for weed mappingof unmanned aerial vehicle (UAV) imagery. PLoS ONE 2018,13, e0196302. CrossRef ]58. Huang, H.; Deng, J.; Lan, Y.; Yang, A.; Deng, X.; Wen, S.; Zhang, H.; Zhang, Y. Accurate weed mapping andprescription map generation based on fully convolutional networks using UAV imagery. Sensors 2018,18,3299. CrossRef ]59. Huang, H.; Lan, Y.; Deng, J.; Yang, A.; Deng, X.; Zhang, L.; Wen, S. semantic labeling approach for accurateweed mapping of high resolution UAV imagery. Sensors 2018,18, 2113. CrossRef ]60. Ha, J.G.; Moon, H.; Kwak, J.T.; Hassan, S.I.; Dang, M.; Lee, O.N.; Park, H.Y. Deep convolutional neuralnetwork for classifying Fusarium wilt of radish from unmanned aerial vehicles. Appl. Remote Sens. 2017,11,042621. CrossRef ]61. Long, M.; Ou, Y.; Liu, H.; Fu, Q. Image recognition of Camellia oleifera diseases based on convolutional neuralnetwork transfer learning. Trans. Chin. Soc. Agric. Eng. 2018,34, 194–201. CrossRef ]62. Liu, T.; Feng, Q.; Yang, S. Detecting grape diseases based on convolutional neural network. J. Northeast.Agric. Univ. 2018,49, 78–83. CrossRef ]63. Wang, Q.; Gao, J.Y.; Yuan, Y. Embedding Structured Contour and Location Prior in Siamesed FullyConvolutional Networks for Road Detection. IEEE Trans. Intell. Transp. 2018,19, 230–241. CrossRef ]64. Ji, S.; Zhang, C.; Xu, A.; Shi, Y.; Duan, Y. 3D convolutional neural networks for crop classiﬁcation withmulti-temporal remote sensing images. Remote Sens. 2018,10, 75. CrossRef ]65. Ndikumana, E.; Ho Tong Minh, D.; Baghdadi, N.; Courault, D.; Hossard, L. Deep recurrent neural networkfor agricultural classiﬁcation using multitemporal SAR Sentinel-1 for Camargue, France. Remote Sens. 2018,10, 1217. CrossRef ]66. Namin, S.T.; Esmaeilzadeh, M.; Najaﬁ, M.; Brown, T.B.; Borevitz, J.O. Deep phenotyping: Deep learning fortemporal phenotype/genotype classiﬁcation. Plant Methods 2018,14, 66. CrossRef ]67. Maggiori, E.; Charpiat, G.; Tarabalka, Y.; Alliez, P. Recurrent Neural Networks to Correct Satellite ImageClassiﬁcation Maps. arXiv 2017; arXiv:1608.03440. CrossRef ]68. Peng, X.; Feng, J.S.; Xiao, S.J.; Yau, W.Y.; Zhou, J.T.; Yang, S.F. Structured AutoEncoders for SubspaceClustering. IEEE Image Process 2018,27, 5076–5086. CrossRef ][PubMed ]69. Wang, Q.; Meng, Z.T.; Li, X.L. Locality Adaptive Discriminant Analysis for Spectral-Spatial Classiﬁcation ofHyperspectral Images. IEEE Geosci. Remote Sens. 2017,14, 2077–2081. CrossRef ]70. Huang, Z.; Zhu, H.; Zhou, J.T.; Peng, X. Multiple Marginal Fisher Analysis. IEEE Trans. Ind. Electron. 2018.[CrossRef ]71. Jung, M.C.; Park, J.; Kim, S. Spatial Relationships between Urban Structures and Air Pollution in Korea.Sustainability 2019,11, 476. CrossRef ]72. Chen, M.; Sun, Z.; Davis, J.M.; Liu, Y.; Corr, C.A.; Gao, W. Improving the mean and uncertainty of ultravioletmulti-ﬁlter rotating shadowband radiometer in situ calibration factors: Utilizing Gaussian process regressionwith new method to estimate dynamic input uncertainty. Atmos. Meas. Tech. 2019,12, 935–953. CrossRef ]73. Website of Zhangqiu County People’s Government. Available online: http://www.jnzq.gov.cn/col/col22490/index.html (accessed on 21 October 2018).74. Calibration Parameters for Part of Chinese Satellite Images. Available online: http://www.cresda.com/CN/Downloads/dbcs/index.shtml (accessed on 29 May 2018).©2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open accessarticle distributed under the terms and conditions of the Creative Commons Attribution(CC BY) license http://creativecommons.org/licenses/by/4.0/ ).