DeepSpectra: An end-to-end deep learning approach for quantitativespectral analysisXiaolei Zhanga,b,1, Tao Lina,b,1, Jinfan Xua,b, Xuan Luoa,b, Yibin Yinga,b,c,*aCollege of Biosystems Engineering and Food Science, Zhejiang University, Hangzhou, Zhejiang, 310058, ChinabKey Laboratory of On Site Processing Equipment for Agricultural Products, Ministry of Agriculture and Rural Affairs, ChinacFaculty of Agricultural and Food Science, Zhejiang &F University, Hangzhou, Zhejiang, 311300, Chinahighlights graphical abstract/C15DeepSpectra with the Inceptionmodule is developed for quantitativespectral analysis./C15DeepSpectra outperforms other CNNapproaches on raw spectra analysis./C15Preprocessing strategies have littlepositive impact on DeepSpectramodel performance./C15DeepSpectra on raw data comparableto the best calibration approach onpreprocessing./C15The model repeatability and accuracyimproved with increased samplesizes.article infoArticle history:Received 25 September 2018Received in revised form24 November 2018Accepted January 2019Available online January 2019Keywords:ChemometricsInceptionConvolutional neural networkModel accuracyRepeatabilityabstractLearning patterns from spectra is critical for the development of chemometric analysis of spectroscopicdata. Conventional two-stage calibration approaches consist of data preprocessing and modeling anal-ysis. Misuse of preprocessing may introduce artifacts or remove useful patterns and result in worsemodel performance. An end-to-end deep learning approach incorporated Inception module, namedDeepSpectra, is presented to learn patterns from raw data to improve the model performance. Deep-Spectra model is compared to three CNN models on the raw data, and 16 preprocessing approaches areincluded to evaluate the preprocessing impact by testing four open accessed visible and near infraredspectroscopic datasets (corn, tablets, wheat, and soil). DeepSpectra model outperforms the other threeconvolutional neural network models on four datasets and obtains better results on raw data than inpreprocessed data for most scenarios. The model is compared with linear partial least square (PLS) andnonlinear arti /uniFB01cial neural network (ANN) methods and support vector machine (SVR) on raw and pre-processed data. The results show that DeepSpectra approach provides improved results than conven-tional linear and nonlinear calibration approaches in most scenarios. The increased training samples canimprove the model repeatability and accuracy.©2019 Elsevier B.V. All rights reserved.1. IntroductionThe development of chemometrics boosts the spectroscopicbased analysis in many different /uniFB01elds, including agricultural*Corresponding author. College of Biosystems Engineering and Food Science,Zhejiang University, Hangzhou, Zhejiang, 310058, China.E-mail address: yingyb@zju.edu.cn (Y. Ying).1X.Z. and T.L. contributed equally to this work.Contents lists available at ScienceDirectAnalytica Chimica Actajournal homepage: www.elsevier.com /locate/acahttps://doi.org/10.1016/j.aca.2019.01.0020003-2670/ ©2019 Elsevier B.V. All rights reserved.Analytica Chimica Acta 1058 (2019) 48 e57products 1], pharmaceuticals 2], petrochemical 3], and soil 4].Linear and nonlinear calibration approaches are developed toestablish the relationships between the spectra to the concentra-tion. Given the experimental changes and variability of sampleorigins, there exist various signals and artifacts from samples andenvironment. The uncertainty and variations from spectral signaland artifacts, however, lead to critical challenge for modelgeneralization and robustness. Practical limitations to the deploy-ment of such techniques appear when existing calibration modelsare to be applied to measurements recorded by new samples and indifferent environmental conditions.Preprocessing is usually required to enhance calibration modelaccuracy by reducing un-modeled variations such as instrumentaland experimental artifacts 5,6]. typical preprocessing approachfor near infrared spectral analysis includes four steps: baselinecorrection, multiplicative scatter correction, noise removal, andscaling 7]. The choice of suitable preprocessing or combinationof preprocessing methods may strongly affect the analysis perfor-mance and require signi /uniFB01cant computational resources by the trial-and-error approach. Misuse of preprocessing techniques, however,can decrease the model performance. Different datasets mayrequire various optimal preprocessing approaches, possibly due tothe differences of artifacts among various datasets 7,8].Increasing the complexity of the model is another approach toimprove model accuracy. Nonlinear calibration methods (e.g. Arti-/uniFB01cial Neural Network (ANN)) can capture both linear and nonlinearfeatures of the spectra. It is, however, often criticized for thecomplex structure of the model, the risk of over /uniFB01tting, and arequirement for large sample sizes 9]. Most existing nonlinearcalibration analyses do not work on raw spectral data, oftenrequiring principal component analysis (PCA) to reduce thedimension of spectra data matrix 9e11]. The reduction of datadimension, however, would possibly change the original patterns ofspectra signal and lose useful information, not necessary forlearning patterns from the data. Improving the model accuracy androbustness, therefore, requires an integrated approach to removevarious artifacts and distill the information related to productcharacteristics from the spectroscopic data.Recent developments in machine learning have demonstratedthat data-driven deep learning approaches can discover intricatestructures in high-dimensional data, reducing the need from priorknowledge and human effort in feature engineering 12]. deeplearning architecture is multilayer stack composed of simple butnon-linear modules which transform the representation at onelevel (starting with the raw input) into representation at higher,slightly more abstract level. With the composition of enough suchtransformations, very complex functions can be learned. In partic-ular, Convolutional Neural Networks (CNN) are popular deeplearning approach that has been well applied in two- and three-dimensional data analysis, including image recognition 13,14],video analysis 15], drug discovery 16], and playing Go 17,18]. TheCNN approach takes advantage of local sparse connections to studylocal patterns from the raw data and reduce the risk of over /uniFB01ttingby adopting the weight sharing. The Inception module developed inGooLeNet network achieves new state of art in deep learning byincreasing both the width and depth of neural network 13]. Thedetails of DeepSpectra model are described in the section of Ma-terials and Methods. To the best of our knowledge, some studieshave developed deep learning based approaches for vibrationalspectral analysis 19e22]. There, however, has few studies consid-ering the increase of model width to capture multiple local spectrafeatures hidden in the raw spectra for better understanding of therelationship between spectra and sample concentration.In this paper, we develop spectral analytical approach withoutthe requirement of data preprocessing for quantitative spectralanalysis. The approach is named DeepSpectra for deep convolu-tional neural network based analysis. The DeepSpectra approach istested by four datasets and its model accuracy is compared withthree CNN models on the raw data. To evaluate the preprocessingimpact on DeepSpectra, 16 preprocessing strategies are quanti /uniFB01edin this study. We compare DeepSpectra model with three popularconventional calibration methods, including partial least square(PLS), arti /uniFB01cial neural network (ANN), and support vector regres-sion (SVR), on the raw and preprocessed data. Furthermore, theimpact of sample size on the repeatability and accuracy of Deep-Spectra approach is evaluated. The results demonstrate thatDeepSpectra approach improves quantitative analysis of spectro-scopic data without the need for data preprocessing.2. Materials and methods2.1. DeepSpectra model architectureDeepSpectra model developed in this study is based on con-volutional neural network (CNN). CNN has the characteristics oflocal connections, weights sharing, pooling, and deeper layers 12].Different neurons in each layer are locally connected to the neuronsin the preceding layer by the /uniFB01lter with the shared weights. The‘deep ’here means on the level on both the depth and width of thenetwork Fig. ). The input of the model is one-dimensional rawspectroscopic data and the output is the object character to beestimated. The model has three convolutional layers (labeled asConv1, Conv2, and Conv3), /uniFB02atten layer (labeled as Flatten), onefully connected layer (labeled as F1), and one output layer. Becauseof the local connection and shared weights, CNN has fewer train-able parameters than fully connected neural networks.One unique feature of DeepSpectra model is its adoption ofInception structure 13] in layer Conv2 and Conv3 using four par-allel different sizes of convolution modules as well as the poolingand /C21 convolutions. To the best of our knowledge, the use ofInception module for one-dimensional spectral analysis has notbeen investigated yet. The increase of the depth of the network canpossibly extract both low and high levels of features from rawspectra, without the need of principal component analysis fordimension reduction. The increase of the width of the network bystacking of parallel /C21,m/C21, and n/C21 convolutions in layerConv3 improves the adaptability of the network to the differentscales of the local spectra features. DeepSpectra learns patternsfrom limited variables through concatenated outputs by combina-tions of these /uniFB01lter sizes. The Inception module allows DeepSpectrato have large width and depth while keeping the computationalcomplexity constant 13]. The /C21 convolutions (green module)and pooling (gray module) in layer Conv2 reduce the number ofweights of the network by decreasing the number and length offeature maps, respectively. 1 /C21 convolution in Conv3 after thepooling operation is also used for feature map decreasing.An example of three /uniFB01lter sizes of ‘1/C21’,‘3/C21’, and ‘5/C21’inthe second layer is shown in Fig. . In the /uniFB01rst layer, all the /uniFB01ltershave size of three and stride of three. Consequently, one neuronin the Conv2 layer covers receptive /uniFB01eld of three, nine, and 15original spectral variables, respectively. Once the neuron is acti-vated, the pattern from the associated receptive /uniFB01eld is learned as afeature.Layer Conv1 is convolutional layer with eight /uniFB01lters of thesame /uniFB01lter size. After the /uniFB01rst convolutional layer, one-dimensionalspectra are transformed to eight feature maps with smaller size.The /uniFB01lter size and stride vary for different datasets, which areoptimized according to input spectra features and the trainingsample size Table ).Layer Flatten is to concatenate the parallel outputs of layerX. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 49Conv3 and convert them into one-dimensional single vector. It isthen fed into the fully connected layer F1. The Flatten layer has noparameters to be trained. The dropout function is adopted to pre-vent model over /uniFB01tting and improve computing performance byrandomly killing off bunch of neurons 23].Layer F1 is dense layer fully connected to layer Flatten. Thenumber of neurons in layer F1 is smaller than that in layer Flatten.The output layer is fully connected to layer F1 with the dropoutfunction incorporated. Layer F2 has one neuron to output theobjective to be estimated.For the objective function, we use mean squared error (MSE)and L2 norm regularization to minimize the sum of squares loss andprevent from over /uniFB01tting (Eq. (1)).Loss ¼1NXNn¼1½ðyn/C0bynÞ2/C138þlkwk2(1)Where ynandbynare measured values and predicted values, Nis thenumber of samples in the training set, wis the weight matrix, and lis the regularization coef /uniFB01cient.We use the leaky recti /uniFB01ed linear function 24] as the activationfunction for the convolutional layers and fully connected layer inthe model. batch normalization (BN) is used after layer Flattenand F1 to speed up training process and improve accuracy 25]. Weuse backpropagation together with Adam 26] optimizer to trainthe model and /uniFB01nd local minimum of the objective function. Anearly stopping strategy is used to avoid over /uniFB01tting. To avoidgradient vanishing or exploding, we use variance scaling for weightinitialization 27].2.2. Design of experiments2.2.1. Comparison of DeepSpectra with other CNN modelsTo evaluate the model performance of DeepSpectra, we compareFig. 2. The changes of the receptive spectra region by two convolutional layers withdifferent /uniFB01lter sizes. An example of two convolutional layers with the /uniFB01rst layer has a/uniFB01lter size of three and stride of three, and the second layer has three /uniFB01lter sizes of one,three, and /uniFB01ve. green rectangle in the input layer represents the input spectralvariables, rectangle in the Conv1 and Conv2 layers represents neuron. One neuronin the Conv2 layer covers receptive /uniFB01eld of three, nine, and 15 original spectralvariables, respectively. (For interpretation of the references to colour in this /uniFB01gurelegend, the reader is referred to the Web version of this article.)Table 1Hyperparameters used in DeepSpectra model of four spectra datasets.Hyperparameter Corn Tablet Wheat SoilKernel size 7 7 9Kernel size 3 3 5Kernel size 5 5 7Stride 3 3 5Stride 2 2 3Hidden number 16 32 32 64Batch size 32 128 128 512Dropout rate 0.2 0.4 0.1 0.5Regularization coef /uniFB01cient 0.001 0.001 0.01 0.01Learning rate 0.01 0.01 0.01 0.01Learning rate decay 0.001 0.001 0.001 0.001Fig. 1. The structure of DeepSpectra model. It consists of three convolutional layers (Conv1, Conv2, and Conv3), one /uniFB02atten layer, one fully connected layer (F1), and one output layer.The blue module represents general convolution, the green module represents /C21 convolution, and the gray module represents max pooling. The /uniFB01rst /C21 convolution modulein layer Conv3 is directly connected to layer Conv1, which is called short connection. The last /C21 convolution module in layer Conv3 and the two /C21 convolution modules inlayer Conv2 are used for channel number reduction to reduce the computation complexity. Small rectangles in each layer represent neurons, and large ectangles represent featuremaps in Conv1, Conv2 and Conv3. (For interpretation of the references to colour in this /uniFB01gure legend, the reader is referred to the Web version of this article.)X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 50it with other three convolutional neural networks (CNN) basedmodels Fig. ). The major difference of these three models lies inmodel depth. The /uniFB01rst CNN model (Model 1) is the shallowest onewith only one convolutional layer (Conv1), one fully connectedlayer (F1), and one output layer. Layer Conv1 has eight /uniFB01lters of thesame /uniFB01lter size.The second CNN model (Model 2) including two convolutionallayers (Conv1 and Conv2), one fully connected layer (F1), and oneoutput layer. Layer Conv1 includes eight /uniFB01lters of the same /uniFB01ltersize. The second convolutional layer (Conv2) includes 12 /uniFB01lters ofthree different sizes to learn different features of data by usingvarious /uniFB01lter sizes within one convolutional layer. Multiple /uniFB01ltersizes can learn multiple features with different receptive /uniFB01elds.The third CNN model (Model 3) is deeper than other models,which consists of three convolutional and two pooling layers(Conv1, Pooling1, Conv2, Pooling2, Conv3), followed by fullyconnected layer (F1) and an output layer.The hyperparameters that include the /uniFB01lter sizes and strides inthe convolutional layers and the hidden numbers in the /uniFB02attenlayer are listed in Table A1 The activation and objective functions ofthese three CNN models are the same as that for DeepSpectra.2.2.2. Comparison of DeepSpectra with conventional calibrationapproachesTo compare DeepSpectra model with conventional calibrationmethods, we consider three popular linear and nonlinearFig. 3. Architectures of three CNN models. The blue module represents general convolution, the green module represents /C21 convolution, and the gray module represents maxpooling. (For interpretation of the references to colour in this /uniFB01gure legend, the reader is referred to the Web version of this article.)X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 51calibration approaches, partial least square (PLS), principalcomponent analysis arti /uniFB01cial neural network (PCA-ANN), andsupport vector regression (SVR) in this study. The /uniFB02owchart of theexperimental design is shown in Fig. . To understand the pre-processing effect on the DeepSpectra and other calibration ap-proaches, 16 preprocessing strategies Table A2 are consideredbased on previous study 28], which includes combination ofbaseline correction (asymmetric least squares, AsLS), multiplicativescatter correction (standard normal variate, SNV), smoothing (S-G/uniFB01lter), and scaling (Mean centering and Pareto scaling).Parameters of all the models have been optimized toward animproved model performance. For the PLS approach, 5-fold crossvalidation is used to optimize the number of latent variables (LVs)from to 40. For the PCA-ANN approach, the input spectral vari-ables are reduced to lower dimensions through PCA before feedinginto typical arti /uniFB01cial neural network with one hidden layer. Weoptimize the number of principal components (PCs) from range of2e40 by 5-fold cross validation. For the SVR approach, we use radialbasis function (RBF) as kernel function. We optimize the penaltyparameter from list of 0.01, 0.1, 1, 10, 100, 1000, and the kernelcoef/uniFB01cient of 0.001 and 0.0001.All the models are implemented on the Python platform usingKeras and Scikit-learn library. The process is performed on Linuxworkstation (Ubuntu 14.04 LTS) with 128 GB of RAM, and an NvidiaGeforce GTX1080Ti graphics card with 11 GB of RAM.2.3. Model evaluationThe model performance is evaluated by root mean squared errorof prediction (RMSEP) and R2, which are shown in Eqs. (2) and (3) .RMSEP ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPNn¼1½ðyn/C0bynÞ2/C138Ns(2)R2¼PNn¼1ðbyn/C0yÞ2PNn¼1ðyn/C0yÞ2(3)Where ynandbynare measured and predicted values, respectively. yis the average of values, and Nis the number of samples in thetraining set.The coef /uniFB01cient of variation (CV) is used to evaluate the repeat-ability of model performance among different datasets. (Eq. (4)).CV¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPNj¼1ðRMSEP j/C0RMSEP avgÞ2NrRMSEP avg(4)Where the Nis the repeated times, which is 25 in this study. TheRSMEP jis the j-th root mean squared error of prediction. TheRMSEP avgis the average root mean squared error of prediction forFig. 4. Flowchart for the experimental design. DeepSpectra model as well as the PLS, PCA-ANN, and SVR models are implemented on raw and preprocessed data. The PC and LVnumbers are optimized through to 40. Abbreviations: AsLS: asymmetric least squares; SNV: standard normal variate; SG: Savitzky-Golay; MC: mean ce ntering; PC: principalcomponent; LV: latent variable.X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 5225 repetitions.2.4. Dataset descriptionFour datasets have been investigated in this study Table ). Allof samples in each dataset are collected from either widegeographical origins, long time horizons, various production con-ditions, or different spectrometers. The raw spectra of four datasetsare displayed in Figure A1 .The/uniFB01rst dataset consists of 80 samples of corn measured on 3different NIR spectrometers (M5, MP5, and MP6). We randomlyselect 27, 27, and 26 spectra samples from M5, MP5, and MP6respectively, and divide them in training (60 samples) and test set(20 samples). The spectra cover range from 1100 to 2498 nm at2 nm intervals (700 channels). The protein content of the samples isto be predicted. The dataset is available at: www.eigenvector.com/Data/Corn/ .The second dataset consists of 238 samples of pharmaceuticaltablets, which produced from laboratory (89 samples), an inter-mediate scale tablet press (72), and an industrial scale press (67)[29]. The tablets produced in laboratory and intermediate scalepress make up training set, while those produced in industrial scalepress are used for testing. The spectra are measured in trans-mittance mode with an ABB Bomem FT-NIR model MB-160. Allspectra range from 952 to 1310 nm with 372 variables. The param-eter to be predicted is active ingredient content (AC). The dataset isfrom 2012 International Diffuse Re /uniFB02ectance Conference (IDRC)Shootout competition: https://cnirs.clubexpress.com/content.aspx?page_id ¼22&club_id ¼409746 &module_id ¼148861.The third dataset iswheat spectroscopic datawith different classesof samples collected from eight crop years (1998 e2005) 30]. 775wheat samples grown from seven crop years (1998 and 2000 e2005)are used asthetrainingsetwhile 107samples growninyear1999 isforthetestset.Thewheatspectrahavearangefrom400to2498 nmwitharesolution of nm, scanned on NIRSystems 6500 instrument. Theparameter to be predicted is protein content (PC). The dataset is from2008 IDRC Shootout competition: https://cnirs.clubexpress.com/content.aspx?page_id ¼22&club_id ¼409746 &module_id ¼148863 .The fourth dataset is soil spectroscopic data collected from theU.S., Africa, Asia, South America and Europe 31]. It includes 2502soil samples in the training set and 1291 in the test set, recorded indiffuse re /uniFB02ectance mode with FieldSpec Pro-FR spectrometer. Thespectra cover range from 350 to 2500 nm with resolution of1 nm. The parameter to be predicted is soil organic carbon (SOC).An extreme value of SOC ¼536.8 kg/C01is removed according to aprevious study 31]. The dataset is from 2006 IDRC Shootoutcompetition: https://cnirs.clubexpress.com/content.aspx? page_id¼22&club_id ¼409746 &module_ id ¼148866 .In the base-case analysis, we compare the performance ofDeepSpectra with three CNN models on the raw data. Statisticalevaluation is provided by t-test to compare the performance (R2)between DeepSpectra and other three models 32]. F-test is usedfor checking the equal variances assumption before conducting t-test. To evaluate the preprocessing impact on DeepSpectra, weapply 16 preprocessing strategies for four datasets. The PLS, PCA-ANN, and SVR models on the raw and preprocessed datasets areused as the benchmark. The soil dataset is used for sensitivityanalysis of sample size by randomly selecting 10% e100% of thesamples with resolution of 10% from the original training set,respectively. The same test set is used for performance comparison.3. Results and discussion3.1. The comparisons between DeepSpectra and three CNN modelsThe end-to-end learning approach by DeepSpectra outperformsthe other three convolutional neural network models on all fourdatasets in terms of lower RMSEP and higher R2(Table ). For cornprotein analysis, DeepSpectra model obtains mean RMSEP of 0.12and mean R2of 0.91, which is much better than other CNN modelsand previous study 20]. The second best performance is fromModel which has mean RMSEP of 0.18, or 50% of RMSEP in-crease from the DeepSpectra model. For the dataset of pharma-ceutical tablets, DeepSpectra model provides mean RMSEP ofactive ingredient content (AC) of 0.35 and mean R2of 0.88, whichoutperforms the other CNN models and the best result from theShootout competition (RMSEP ¼0.41) 29]. The Models and 2provide low RMSEP of 0.38, or 10% increase from the result ofDeepSpectra. For wheat protein analysis, DeepSpectra model gen-erates lowest mean RMSEP of 0.20, followed by the Models and1 with mean RMSEP of 0.25 and 0.27, respectively. previousstudy only achieves best RMSEP of 0.37 33]. For soil organiccarbon analysis, the DeepSpectra model provides lowest RMSEPof 8.88 among four calibration approaches, which is slightly betterthan Model (RMSEP ¼8.91) and previous study using boostedregression trees model on preprocessed data of /uniFB01rst derivative(RMSEP ¼9.0) 31].DeepSpectra model provides signi /uniFB01cantly higher R2than allother three CNN models with 95% con /uniFB01dence interval for mostdatasets, except one scenario where it is not signi /uniFB01cantly betterthan Model on the soil dataset Table A3 ). It also provides aconsistent improved performance compared to previous studies[22,29,31,32]. The advantages of DeepSpectra model is due to itsunique neural network structure in layers Conv2 and Conv3. Theparallel convolutions with different /uniFB01lter sizes in the third con-volutional layer (Conv3) allow the model to capture both local andhigh abstracted features in one layer. The /C21 convolutions andpooling in the second convolutional layer (Conv2) reduce thenumber of variables to alleviate computational challenges. Theseunique characteristics of network structure help to captureimportant features from the inputs and prevent possible over-/uniFB01tting. Model with three /uniFB01lter sizes also performs relatively well,only second to DeepSpectra, which demonstrates that parallelcombinations of convolutional operations with varied /uniFB01lter sizeshave promising structure to learn patterns from raw spectra withthe increase of model width.Model with deep neural network of three convolutional andtwo pooling layers obtains relatively poor performance on bothtraining and test set, especially on the corn dataset where anRMSEC of 0.44 and RMSEP of 0.35 are provided. We also notice thatModel obtains closer performance to Models and on thedatasets with larger sample size. It shows that increased sampleTable 2Description of the datasets.Datasets total samples training samples test samples features Min Max Mean SDCorn protein content (%) 80 60 20 700 7.65 9.71 7.65 0.50Tablet AC (%) 228 161 67 372 4.61 9.79 7.49 1.26Wheat protein content (%) 882 775 107 1050 9.70 17.00 13.54 1.56Soil organic content (g kg/C01) 3793 2502 1291 2151 241.60 11.97 20.87X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 53size can improve the modeling /uniFB01tting performance by deep neuralnetwork. The advantage of deep neural network, however, is notre/uniFB02ected on the small dataset. With the increase of sample size,Model demonstrates improved performance for better repre-senting the structure of spectral data.The stack of pooling layers could lead to information loss, whichresults in the poor performance by Model 3, especially on the corndataset with small number of spectral features. Model cannot learnenough effective patterns from the little information to /uniFB01t data well.It is trade-off between over /uniFB01tting and under /uniFB01tting for the design ofthe model structure. Compared to Model 3, DeepSpectra applies /C21 convolutions and pooling within one layer in the Inception module,which reduces the information loss by the pooling operation onlyand thus retains the information in the model analysis. Models and2 do not have pooling layers so they can capture more features whilebearing the risk of over /uniFB01tting. The impact of information loss frompooling operations could be alleviated with the increase in thenumber of spectral features. The corn dataset has only 700 featureswith resolution of nm, while soil dataset has 2151 features with aresolution of nm. The soil dataset, therefore, contains more infor-mation from the spectra. Model is less sensitive to information lossresulted from pooling on soil dataset and provides higher accuracy.The tablet dataset has /uniFB01ner spectra resolution and larger sample sizecompared to the corn dataset, which result in relatively goodmodeling performance for Model 3.DeepSpectra, with similar depth and pooling operations as Model3, has an Inception structure to extract features from the differentscales simultaneously 13]. The Inception structure reduces infor-mation loss from pooling and prevents over /uniFB01tting by weight reduc-tion, which leads to better matching to the four spectral datasets.The resultsre /uniFB02ect the advantage of Inception structure with increasedwidth and depth over conventional deep neural networks in one-dimensional spectral analysis, especially on the dataset with fewsample number and few spectral features such as the corn dataset.DeepSpectra model, however, requires longer computing timethan other three CNN models for each dataset Fig. ). The trainingprocess of the DeepSpectra model costs up to 34.26 min for soildatasets in this study, followed by Model 2, Model 3, and Model 1with 14.74, 13.94, and 12.09 min, respectively. The training time isTable 3Predictive results obtained by DeepSpectra as well as three CNN models on raw data for four datasets. The mean and standard deviation of RMSEP from 25 ru ns are presented.The last column presents the RMSEPs from previous studies.Datasets Model 1(Mean ±Std) Model 2(Mean ±Std) Model 3(Mean ±Std) DeepSpectra(Mean ±Std) Previous study(Range)Corn protein content (%)RMSEP 0.22 ±0.04 0.18 ±0.05 0.35 ±0.01 0.12 ±0.02 eR20.72 ±0.09 0.80 ±0.10 0.26 ±0.06 0.91 ±0.04 0.74e0.82 22]Tablet AC (%)RMSEP 0.38 ±0.05 0.38 ±0.05 0.48 ±0.06 0.35 ±0.05 0.41e0.48 29]R20.85 ±0.05 0.85 ±0.04 0.77 ±0.06 0.88 ±0.03 0.84e0.86 29]Wheat protein content (%)RMSEP 0.27 ±0.07 0.25 ±0.06 0.49 ±0.15 0.20 ±0.01 0.37e0.70 33]R20.97 ±0.02 0.97 ±0.01 0.89 ±0.07 0.98 ±0.002 0.85e0.95 33]Soil organic carbon (g kg/C01)RMSEP 9.27 ±0.18 8.91 ±0.17 9.34 ±0.19 8.88 ±0.22 9.00 31]R20.82 ±0.01 0.83 ±0.01 0.82 ±0.01 0.84 ±0.01 0.55e0.82 31]Fig. 5. Training time of four deep learning models on four datasets.Table 4RMSEPs obtained by DeepSpectra on the 16 kinds of preprocessed data and raw data.Experiment Baseline Scatter Smoothing Scaling Corn (%) Tablet (%) Wheat (%) Soil (g kg/C01)1 AsLS SNV yes Pareto 0.21 ±0.01 0.42 ±0.02 0.90 ±0.02 13.83 ±0.252 AsLS SNV yes MC 0.22 ±0.01 0.41 ±0.03 0.90 ±0.02 13.92 ±0.213 AsLS SNV none Pareto 0.23 ±0.02 0.42 ±0.03 0.90 ±0.02 13.95 ±0.264 AsLS SNV none MC 0.22 ±0.02 0.41 ±0.02 0.90 ±0.02 13.85 ±0.255 AsLS none yes Pareto 0.26 ±0.01 0.93 ±0.07 0.76 ±0.02 12.95 ±0.206 AsLS none yes MC 0.27 ±0.02 0.90 ±0.08 0.77 ±0.03 12.86 ±0.237 AsLS none none Pareto 0.27 ±0.01 0.94 ±0.06 0.77 ±0.02 12.92 ±0.238 AsLS none none MC 0.27 ±0.01 0.91 ±0.11 0.76 ±0.02 12.92 ±0.299 none SNV yes Pareto 0.16 ±0.01 0.36 ±0.03 0.20 ±0.01 9.39 ±0.1910 none SNV yes MC 0.15 ±0.02 0.36 ±0.03 0.20 ±0.01 9.36 ±0.2111 none SNV none Pareto 0.15 ±0.01 0.37 ±0.02 0.20 ±0.01 9.36 ±0.2612 none SNV none MC 0.15 ±0.01 0.37 ±0.02 0.20 ±0.01 9.42 ±0.2313 none none yes Pareto 0.14 ±0.04 0.36 ±0.08 0.20 ±0.01 8.94 ±0.2214 none none yes MC 0.12 ±0.01 0.38 ±.311 0.23 ±0.13 9.05 ±0.2215 none none none Pareto 0.12 ±0.02 0.38 ±0.06 0.20 ±0.01 8.94 ±0.1616 none none none MC 0.13 ±0.02 0.36 ±0.04 0.20 ±0.02 9.01 ±0.2417 none none none none 0.12 ±0.02 0.35 ±0.05 0.20 ±0.01 8.88 ±0.22X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 54related to sample size, input features, batch size, and others. Soildataset requires most computing time to calibrate for all fourmodels, given its large sample size (2502) and input features (2151).It is interesting to /uniFB01nd that the tablet dataset needs least amount oftraining time though its sample size is larger than corn dataset. Thisis because the tablet dataset has only 372 input features while thecorn dataset has 700 features. DeepSpectra model requires veryshort time on testing with few seconds for all four datasets.3.2. Impact of preprocessing on DeepSpectra modelDeepSpectra model on raw data obtains better results than inpreprocessed data for most scenarios Table ). For tablet and soildatasets, DeepSpectra model based on raw spectra outperforms themodel using all 16 preprocessing strategies. For the tablet dataset,DeepSpectra model obtains lowest mean RMSEP of 0.35 on theraw data, compared to mean RMSEP in range of 0.36 e0.94 onthe preprocessed data. For the soil dataset, the performance on theraw spectra is at mean RMSEP of 8.88, improved from range ofRMSEP from 8.94 to 13.95 using the preprocessed data. For the corndataset, DeepSpectra model provides mean RMSEP of 0.12 with astandard deviation of 0.20 by raw data, which is comparable to thebest results by two preprocessing approaches (smoothing þMCscaling and Pareto scaling only). For the wheat dataset, DeepSpectramodel on seven preprocessing strategies achieve mean RMSEP of0.20, which is the same as the accuracy achieved using the raw data.Fig. 6. Results of DeepSpectra, PLS, PCA-ANN, and SVR on the preprocessed data and raw data. RMSEP: root mean square error of prediction. The RMSEP values are resented inTable A4 .X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 55The results demonstrate that the DeepSpectra model can learnpatterns from raw data and achieve the best quantitative analysisresults without any preprocessing.3.3. Comparison of DeepSpectra with conventional calibrationapproachesDeepSpectra model on raw spectra data outperforms the con-ventional linear (PLS) and nonlinear (PCA-ANN and SVR) ap-proaches using raw and preprocessed data in most scenarios onfour datasets Fig. ,Table A4 ). Different datasets, however, wouldhave their preferred conventional calibration methods Fig. ). ThePLS method provides lowest RMSEP for wheat and corn datasets,whereas the PCA-ANN and SVR approaches are best suited for soilorganic carbon and tablet analysis, respectively.DeepSpectra model provides an improved model performance,especially for the dataset with large sample size such as soilsamples. The mean RMSEP can be reduced to 8.88, which is muchlower than the second best performance of 11.67 based on the PCA-ANN approach using the preprocessed data. DeepSpectra also im-proves the active ingredient content analysis of tablets to meanRMSEP of 0.35, comparing to the best result by traditional ap-proaches at 0.42 based on the SVR method after data preprocessing(strategies and 11).For protein content analysis of corn and wheat samples, Deep-Spectra model on raw data provides comparable performance tothe best preprocessing strategy by the PLS model. This correspondsto previous study suggesting linear relation of the spectra to theconcentration for protein analysis 9]. DeepSpectra model achievesthe lowest REMSEP at 0.12 and 0.20 for corn and wheat, respectively,which compares to the best performance of PLS approach at 0.12 and0.21. DeepSpectra model has no obvious advantage over PLS modelfor corn dataset with small sample size. The nonlinear PCA-ANN andSVR approaches, however, provide much poor results on both rawand preprocessed data for both datasets. Considering all four data-sets, the results demonstrate that DeepSpectra model not only per-forms improved results on capturing nonlinear patterns but alsoprovides comparable performance for samples with linear patterns.The appropriate selection of preprocessing strategy doesimprove the performance by the traditional calibration approaches(Fig. ). Different calibration models, however, have their preferredpreprocessing methods for each dataset. For example, the pre-processing approaches e8 boost the PLS model for tablets anal-ysis, but they unfortunately worsen the performance for PCA-ANNand SVR analysis.3.4. Impact of sample size change on the model performanceDeepSpectra model provides varied result on the same datasetwith different runs as result of stochastic optimization approach.The mean and standard deviation of RMSEP from 25 runs for eachdataset are provided in Table . Although the hyperparameters are/uniFB01xed, the trained weights would vary by different model runs dueto some random factors such as randomly dropout some weightsand stochastic optimization. The repeatability of DeepSpectraapproach is improved with increased training samples, in terms of alower coef /uniFB01cient of variation Fig. ). The coef /uniFB01cient of variation ofRMSEP for soil dataset with 2502 training samples is only 2.37%while corn dataset with 60 training samples generates coef /uniFB01cientof variation of 16.67%, followed by 11.43% and 5% with tablet andwheat dataset, respectively Fig. ).The accuracy of DeepSpectra is improved with an increasedsample size of training dataset. The RMSEP by DeepSpectraapproach reduces from 11.43 to 8.88 with the increasing number ofsamples in the training set of the soil dataset Fig. ). This indicatesthat the increased sample number would improve DeepSpectramodel to learn patterns from various data sources. The magnitudeof improvement, however, is not signi /uniFB01cant after using 80% of theoriginal training dataset. The coef /uniFB01cient of variation of RMSEP is11.18% with 10% samples (250 samples) used in the training set forsoil data, which is much higher than 2.54% with 20% (500) samplesused. DeepSpectra model shows relatively high and stablerepeatability with coef /uniFB01cient of variation is around 2%, with morethan 750 samples used in the training set.4. Conclusion and future workDeepSpectra model is developed to learn patterns from rawspectra without the need for data preprocessing and dimensionalreduction. This study provides detailed introduction of deeplearning approach for quantitative spectral analysis. The modelincludes three convolutional layers incorporated with Inceptionmodule to learn patterns from various spectra variables. Using fourpublic spectral datasets, the results show that DeepSpectraFig. 7. Coef/uniFB01cient of variation by DeepSpectra model on the raw data for four datasets.Fig. 8. DeepSpectra model performance of 25 repeated runs with sample size from10% to 100% randomly selected from original training set. mean RMSEP and 90%con/uniFB01dence interval are presented for each scenario.X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 56approach outperforms other CNN models on the raw data. Theincluding of preprocessing methods does not improve DeepSpectramodel performance for most scenarios. DeepSpectra model on rawspectra data outperforms the conventional linear (PLS) andnonlinear (PCA-ANN and SVR) approaches using raw and pre-processed data in most scenarios. DeepSpectra model provides alow RMSEP on small sample size dataset, but it shows poor stabilitywith high coef /uniFB01cient of variation. The increased sample numberscan improve the DeepSpectra model repeatability and accuracy.Model repeatability and interpretation capability are critical forthe further development of deep learning based spectral analysis. Itwill be critical to understand the minimum and recommendedrequirement of sample size for deep learning based spectral anal-ysis from model accuracy and repeatability perspective in futurestudies. It will also be interesting to understand and visualize theidenti /uniFB01cation of critical spectral features related to componentanalysis. The improved understanding of mechanism of deeplearning might be helpful for global analysis of component con-centration among various products, such as protein content ofgrains and soluble solids content of fruits.Declaration of interestsThe authors declare that they have no known competing/uniFB01nancial interests or personal relationships that could haveappeared to in /uniFB02uence the work reported in this paper.AcknowledgementsThis work was partially funded by the Thousand Young TalentsProgram of China and Zhejiang University. The authors would alsolike to acknowledge Chiyu Wu and anonymous reviewers for theirhelpful comments and suggestions, which substantially improvedthe quality of the paper.Appendix A. Supplementary dataSupplementary data to this article can be found online athttps://doi.org/10.1016/j.aca.2019.01.002 .References[1] G. Binetti, L. Del Coco, R. Ragone, S. Zelasco, E. Perri, C. Montemurro,R. Valentini, D. Naso, F.P. Fanizzi, F.P. Schena, Cultivar classi /uniFB01cation of Apulianolive oils: use of arti /uniFB01cial neural networks for comparing NMR, NIR andmerceological data, Food Chem. 219 (2017) 131 e138, https://doi.org/10.1016/j.foodchem.2016.09.041 .[2] E.M. Hetrick, Z. Shi, L.E. Barnes, A.W. Garrett, R.G. Rupard, T.T. Kramer,T.M. Cooper, D.P. Myers, B.C. Castle, Development of near infraredspectroscopy-based process monitoring methodology for pharmaceuticalcontinuous manufacturing using an of /uniFB02ine calibration approach, Anal. Chem.89 (2017) 9175 e9183, https://doi.org/10.1021/acs.analchem.7b01907 .[3] A. Palou, A. Mir /C19o, M. Blanco, R. Larraz, J.F. /C19omez, T. Martínez, J.M. Gonz /C19alez,M. Alcal /C18a, Calibration sets selection strategy for the construction of robust PLSmodels for prediction of biodiesel/diesel blends physico-chemical propertiesusing NIR spectroscopy, Spectrochim. Acta Part Mol. Biomol. Spectrosc. 180(2017) 119 e126, https://doi.org/10.1016/j.saa.2017.03.008 .[4] A. Gredilla, S. Fdez-Ortiz de Vallejuelo, N. Elejoste, A. de Diego, J.M. Madariaga,Non-destructive Spectroscopy combined with chemometrics as tool forGreen Chemical Analysis of environmental samples: review, TrAC TrendsAnal. Chem. 76 (2016) 30 e39,https://doi.org/10.1016/j.trac.2015.11.011 .[5] Y. Bi, K. Yuan, W. Xiao, J. Wu, C. Shi, J. Xia, G. Chu, G. Zhang, G. Zhou, localpre-processing method for near-infrared spectra, combined with spectralsegmentation and standard normal variate transformation, Anal. Chim. Acta909 (2016) 30 e40,https://doi.org/10.1016/j.aca.2016.01.010 .[6] J. Gerretzen, E. Szyma /C19nska, J. Bart, A.N. Davies, H.J. van Manen, E.R. van denHeuvel, J.J. Jansen, L.M.C. Buydens, Boosting model performance and inter-pretation by entangling preprocessing selection and variable selection, Anal.Chim. Acta 938 (2016) 44 e52,https://doi.org/10.1016/j.aca.2016.08.022 .[7] J. Engel, J. Gerretzen, E. Szyma /C19nska, J.J. Jansen, G. Downey, L. Blanchet,L.M.C. Buydens, Breaking with trends in pre-processing? TrAC Trends Anal.Chem. 50 (2013) 96 e106, https://doi.org/10.1016/j.trac.2013.04.015 .[8] Å. Rinnan, F. van den Berg, S.B. Engelsen, Review of the most common pre-processing techniques for near-infrared spectra, TrAC Trends Anal. Chem. 28(2009) 1201 e1222, https://doi.org/10.1016/j.trac.2009.07.007 .[9] W. Ni, L. Nørgaard, M. Mørup, Non-linear calibration models for near infraredspectroscopy, Anal. Chim. Acta 813 (2014) e14,https://doi.org/10.1016/j.aca.2013.12.002 .[10] M. Sun, D. Zhang, L. Liu, Z. Wang, How to predict the sugariness and hardnessof melons: near-infrared hyperspectral imaging method, Food Chem. 218(2017) 413 e421, https://doi.org/10.1016/j.foodchem.2016.09.023 .[11] F. Allegrini, A.C. Olivieri, Sensitivity, prediction uncertainty, and detectionlimit for arti /uniFB01cial neural network calibrations, Anal. Chem. 88 (2016)7807 e7812, https://doi.org/10.1021/acs.analchem.6b01857 .[12] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (2015) 436 e444,https://doi.org/10.1038/nature14539 .[13] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in: 2015 IEEEConf. Comput. Vis. Pattern Recognit, IEEE, 2015, pp. e9,https://doi.org/10.1109/CVPR.2015.7298594 .[14] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in:2016 IEEE Conf. Comput. Vis. Pattern Recognit, IEEE, 2016, pp. 770 e778,https://doi.org/10.1109/CVPR.2016.90 .[15] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-Fei, Large-scale video classi /uniFB01cation with convolutional neural networks, in: 2014 IEEEConf. Comput. Vis. Pattern Recognit, IEEE, 2014, pp. 1725 e1732, https://doi.org/10.1109/CVPR.2014.223 .[16] M.H.S. Segler, M. Preuss, M.P. Waller, Planning chemical syntheses with deepneural networks and symbolic AI, Nature 555 (2018) 604 e610, https://doi.org/10.1038/nature25978 .[17] D. Silver, A. Huang, C.J. Maddison, A. Guez, L. Sifre, G. van den Driessche,J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman,D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach,K. Kavukcuoglu, T. Graepel, D. Hassabis, Mastering the game of Go with deepneural networks and tree search, Nature 529 (2016) 484 e489, https://doi.org/10.1038/nature16961 .[18] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez,T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui, L. Sifre, G. vanden Driessche, T. Graepel, D. Hassabis, Mastering the game of Go withouthuman knowledge, Nature 550 (2017) 354 e359, https://doi.org/10.1038/nature24270 .[19] J. Acquarelli, T. van Laarhoven, J. Gerretzen, T.N. Tran, L.M.C. Buydens,E. Marchiori, Convolutional neural networks for vibrational spectroscopic dataanalysis, Anal. Chim. Acta 954 (2017) 22 e31, https://doi.org/10.1016/j.aca.2016.12.010 .[20] E.J. Bjerrum, M. Glahder, T. Skov, Data Augmentation of Spectral Data forConvolutional Neural Network (CNN) Based Deep Chemometrics, 2017,pp. e10.http://arxiv.org/abs/1710.01927 .[21] S. Malek, F. Melgani, Y. Bazi, One-dimensional convolutional neural networksfor spectroscopic signal regression, J. Chemom. 32 (2018), https://doi.org/10.1002/cem.2977 e2977.[22] C. Yuanyuan, W. Zhibin, Quantitative analysis modeling of infrared spectros-copy based on ensemble convolutional neural networks, Chemometr. Intell.Lab. Syst. 181 (2018) e10,https://doi.org/10.1016/j.chemolab.2018.08.001 .[23] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,Dropout: simple way to prevent neural networks from over /uniFB01tting, J. Mach.Learn. Res. 15 (2014) 1929 e1958, https://doi.org/10.1214/12-AOS1000 .[24] A.L. Maas, A.Y. Hannun, A.Y. Ng, Recti /uniFB01er nonlinearities improve neuralnetwork acoustic models, Proc. 30 Th Int. Conf. Mach. Learn. 28 (2013) .[25] S. Ioffe, C. Szegedy, Batch normalization: accelerating deep network trainingby reducing internal covariate shift, in: Int. Conf. Int. Conf. Mach. Learn, 2015,pp. 448 e456, https://doi.org/10.1007/s13398-014-0173-7.2 .[26] D.P. Kingma, J. Ba, Adam: Method for Stochastic Optimization, vols. e15,2014. http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503 .[27] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into recti /uniFB01ers: surpassing human-level performance on ImageNet classi /uniFB01cation, in: 2015 IEEE Int. Conf. Comput.Vis, IEEE, 2015, pp. 1026 e1034, https://doi.org/10.1109/ICCV.2015.123 .[28] J. Gerretzen, E. Szyma /C19nska, J.J. Jansen, J. Bart, H.-J. van Manen, E.R. van denHeuvel, L.M.C. Buydens, Simple and effective way for data preprocessing se-lection based on design of experiments, Anal. Chem. 87 (2015) 12096 e12103,https://doi.org/10.1021/acs.analchem.5b02832 .[29] B. Igne, P. Berzaghi, D. Bu, P. Dardenne, P. Tillmann, M. Westerhaus, Summaryof the 2012 IDRC software shoot-out, NIR News 23 (2012) 13 e15,https://doi.org/10.1255/nirn.1331 .[30] C. Brenner, R. Pierce, D. Funk, “The good, the bad, and the ugly ”: 2008 IDRCsoftware Shootout, NIR News 20 (2009) 12 e15, https://doi.org/10.1255/nirn.1137 .[31] D.J. Brown, K.D. Shepherd, M.G. Walsh, M. Dewayne Mays, T.G. Reinsch, Globalsoil characterization with VNIR diffuse re /uniFB02ectance spectroscopy, Geoderma132 (2006) 273 e290, https://doi.org/10.1016/j.geoderma.2005.04.025 .[32] H. Motulsky, Intuitive Biostatistics: Nonmathematical Guide to StatisticalThinking, fourth ed., Oxford University Press, 2014 .[33] X. Bian, P. Diwu, C. Zhang, L. Lin, G. Chen, X. Tan, Y. Guo, B. Cheng, Robustboosting neural networks with random weights for multivariate calibration ofcomplex samples, Anal. Chim. Acta 1009 (2018) 20 e26, https://doi.org/10.1016/j.aca.2018.01.013 .X. Zhang et al. Analytica Chimica Acta 1058 (2019) 48 e57 57