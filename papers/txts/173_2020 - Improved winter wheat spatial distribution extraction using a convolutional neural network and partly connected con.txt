remote sensing ArticleImproved Winter Wheat Spatial DistributionExtraction Using Convolutional Neural Networkand Partly Connected Conditional Random FieldShouyi Wang1,â€ , Zhigang Xu2,â€ , Chengming Zhang1,*,â€ , Yuanyuan Wang1,3, Shuai Gao4,Hao Yin1and Ziyun Zhang11College of Information Science and Engineering, Shandong Agricultural University, 61 Daizong Road,Taian 271000, Shandong, China; 2017110610@sdau.edu.cn (S.W.); wyy@sdau.edu.cn (Y.W.);2018110569@sdau.edu.cn (H.Y.); 2018110571@sdau.edu.cn (Z.Z.)2School of Computer Science, Hubei University of Technology, 28 Nanli Road, Wuhan 430068, Hubei, China;20161071@hbut.edu.cn3Shandong Technology and Engineering Center for Digital Agriculture, 61 Daizong Road,Taian 271000, Shandong, China4Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Dengzhuangnan Road,Beijing 100094, China; gaoshuai@radi.ac.cn*Correspondence: chming@sdau.edu.cn; Tel.:+86-139-5382-3659â€ These authors are co-ï¬rst authors as they contributed equally to this work.Received: February 2020; Accepted: 28 February 2020; Published: March 2020/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045/gid00001/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046Abstract:Improving the accuracy of edge pixel classiï¬cation is crucial for extracting the winterwheat spatial distribution from remote sensing imagery using convolutional neural networks (CNNs).In this study, we proposed an approach using partly connected conditional random ï¬eld model(PCCRF) to reï¬ne the classiï¬cation results of Reï¬neNet, named Reï¬neNet-PCCRF. First, we usedan improved Reï¬neNet model to initially segment remote sensing images, followed by obtainingthe category probability vectors for each pixel and initial pixel-by-pixel classiï¬cation result. Second,using manual labels as references, we performed statistical analysis on the results to select pixelsthat required optimization. Third, based on prior knowledge, we redeï¬ned the pairwise potentialenergy, used linear model to connect diâ†µerent levels of potential energies, and used only pixel pairsassociated with the selected pixels to build the PCCRF. The trained PCCRF was then used to reï¬nethe initial pixel-by-pixel classiï¬cation result. We used 37 Gaofen-2 images obtained from 2018 to 2019of representative Chinese winter wheat region (Taiâ€™an City, China) to create the dataset, employedSegNet and Reï¬neNet as the standard CNNs, and fully connected conditional random ï¬eld as thereï¬nement methods to conduct comparison experiments. The Reï¬neNet-PCCRFâ€™s accuracy (94.51%),precision (92.39%), recall (90.98%), and F1-Score (91.68%) were clearly superior than the methodsused for comparison. The results also show that the Reï¬neNet-PCCRF improved the accuracy oflarge-scale winter wheat extraction results using remote sensing imagery.Keywords:convolutional neural network; partly connected conditional random ï¬eld; remote sensingimagery; image segmentation; reï¬ned edge; prior knowledge; winter wheat; Gaofen-2 image;Taiâ€™an, China1. IntroductionThe crop spatial distribution includes the shape, location, and area of each piece of crop plantingarea. The accurate measurement of crop spatial distributions is of great signiï¬cance for scientiï¬cresearch, food security, estimates of grain production, and agricultural management and policy [1â€“3].Remote Sens.2020,12, 821; doi:10.3390/rs12050821www.mdpi.com/journal/remotesensingRemote Sens. 2020,12, 821 of 25Whether the edges are ï¬ne is key indicator of the crop spatial distribution data quality; to achievethis, research related to obtaining large-scale and high-quality crop spatial distribution has attractedwidespread attention 4,5].Ground surveys can be used to obtain accurate crop spatial distributions. However, this methodis highly labor-intensive and time-consuming, thereby making it di cult to obtain large-scale data 6].The data obtained via ground surveys are mainly used to verify the data obtained using othertechnologies 7].As remote sensing technologies can rapidly obtain up-to-date, large-scale, ï¬nely detailed groundimages, remote sensing imagery has become the main source of data used to generate accurate cropspatial distributions 8â€“10]. Image segmentation technology can produce pixel-by-pixel classiï¬cationresults; thus, it is widely used in extracting crop spatial distributions 11,12]. Furthermore, both thespeciï¬c pixel feature extraction method and classiï¬er have decisive impact on the accuracy of theclassiï¬cation results 13,14].As pixel features form the basis for high-quality image segmentation, previous studies havedeveloped various feature extraction methods to obtain â†µective pixel features 15,16]. Previously,spectral features were used in remote sensing image segmentation, of which the normalized di â†µerencevegetation index (NDVI) was the frequently used feature when extracting vegetation 17]. The spectralfeature extraction method is based on statistical and analytical technologies. By performing series ofmathematical operations on the channel value of each pixel, the result obtained is used as the value ofthe pixel feature 18].In low-spatial-resolution images, such as from Moderate Resolution Imaging Spectroradiometer(MODIS) and Enhanced Thematic Mapper /Thematic Mapper (ETM /TM), pixels inside winter wheatand other crop ï¬elds have good consistency and low change rates, which can better distinguish cropï¬elds from other land-use types 19,20]. However, at the edge of the crop planting area, the featurevalue extracted from the mixed pixels has weak discrimination ability, resulting in more pixels beingmisclassiï¬ed 21,22]. In addition, di â†µerences in crop growth within the planting area adversely â†µectthe spectral feature extraction, thereby resulting in mis-segmented pixels that form the so-called "saltand pepper" phenomenon 23,24].As the spectral features only express the characteristic information of the pixels themselves,the â†µect is usually not ideal when applied to higher-spatial-resolution images 23]. There is moredetailed information in higher-spatial-resolution remote sensing images, and the spatial correlationbetween pixels is signiï¬cantly enhanced, but the spectral characteristics cannot express this correlationinformation, and therefore, in such cases, spectral features are ine â†µective 25,26]. To better expressthe spatial correlation information between pixels, previous studies have proposed series of texturefeature extraction methods, such as the wavelet transform 27,28], Gabor ï¬lter 29,30], and gray levelco-occurrence matrix (GLCM) 31]. Combining spectral and textural features enables the extraction ofhigher-quality crop spatial distributions from low- and medium-resolution imagery 32].In addition to the spectral and texture features, previous studies have developed series of methods,including neural networks 33,34], support vector machines 35,36], random forests 37â€“39], and decisiontrees 40,41], to obtain features with improved distinguishing abilities for high-spatial-resolution remotesensing images. These methods generally use the channel values of pixels as the input, as well ascomplex mathematical operations to obtain improved distinguishing features. As these methods donot consider or barely consider the spatial correlation between pixels, the distinguishing ability of theextracted features is not ideal for several types of new higher-spatial-resolution remote sensing images.With the success of convolutional neural networks (CNNs) in camera image processing, researchersbegan to successfully use these networks for feature extraction from remote sensing images andhave achieved good results 42â€“45]. The convolution operation can accurately express the spatialrelationship between pixels and extract deep information from the pixels (when the convolution kernelis set appropriately), combining the advantages of previous feature extraction methods 14,46â€“48].Classic CNNs, such as the Fully Convolutional Network (FCN) 49], SegNet 50], DeepLab 51], andRemote Sens. 2020,12, 821 of 25Reï¬neNet 52], form the basis for the rapidly developing ï¬eld of remote sensing image segmentation.Although the use of CNNs can signiï¬cantly improve the accuracy of remote sensing image segmentation,errors remain common near object edges owing to the inherent characteristics of the convolutionoperation 49â€“51,53]. Thus, convolution must be combined with other post-processing techniques toimprove the accuracy of the results 51,54,55].Reï¬neNet and most other classic CNNs typically use two-dimensional (2-D) convolution methodsto extract feature values. Two-dimensional convolution methods are unsuitable for processing imageswith small channels, such as optical remote sensing images or camera images 56]. To preserve thespectral and spatial features when processing hyperspectral remote sensing images, previous studieshave used three-dimensional (3-D) convolution methods to extract spectralâ€“spatial features 56,57].As the 3-D convolution method can fully use the abundant spectral and spatial information ofhyperspectral imagery, this convolution method has achieved remarkable success in the classiï¬cationof hyperspectral images.Conditional random ï¬eld (CRF) is commonly used post-processing technique for camera imagesegmentation 55,58]. As CRFs have the ability to capture both local and long-range dependencies withinan image, they signiï¬cantly improve CNN segmentation results 59]. The existing CRFs, such as the fullyconnected CRF modeling processes, are complicated and require large number of calculations 60].To complete the calculations, previous studies have used approximate calculations 60,61], reduction ofthe number of samples involved in modeling 62,63], and introduced conditional independence 64â€“66].However, in doing so, the performance of the CRFs gets reduced 67]. To combine CNN and CRFs,and achieve end-to-end training, several studies 67â€“69] have converted the CRF into an iterativecalculation, while others 64] have converted the CRF into convolution operation.The existing CRF mode uses only the channel value and position of the pixel, which emphasizes thesmoothness of the image data 70]. As the spatial resolution of remote sensing image is signiï¬cantlylower than that of camera image, the color change at the boundary of the object is not as apparentas in the camera image. When CRF is applied to remote sensing image segmentation, new featuresshould be used in the modeling process. In the existing CRF modeling, the CNN is used only as aunary potential function, and any other information provided by the CNN is not used. In addition, it isunreasonable to use the equal weight method to connect the unary potential function and the pairwisepotential function, which needs to be improved.As winter wheat is an important food crop, previous studies have proposed numerous methods toextract the spatial distribution information of winter wheat from remote sensing images. When usinglow- and medium-resolution images as data sources, NDVI and other vegetation indices are typicallyused as the main features 71]. When higher-resolution remote sensing images are used as data sources,regression methods 72], support vector machines 73,74], random forests 75], linear discriminantanalysis 76], and CNNs 77,78] are the more commonly used methods. There is signiï¬cant numberof mis-segmented pixels at the edges of winter wheat planting areas, which are common problems thatthese methods must overcome. Although the edge accuracy of the winter wheat planting area canbe improved with the use of CRF 78], improving the computational ciency of CRFs is still animportant issue that requires an urgent solution.In this study, we proposed partly connected conditional random ï¬eld (PCCRF) model topost-process the Reï¬neNet extraction results, referred to as Reï¬neNet-PCCRF, to eventually achievethe goal of obtaining the high-quality winter wheat spatial distribution. The main contributions of thispaper are as follows:â€¢ The statistical analysis technology is used to analyze the segmentation results of Reï¬neNet, andprior knowledge is applied to PCCRF modeling.â€¢ Based on prior knowledge, we modiï¬ed the fully connected conditional random ï¬eld (FCCRF)to build the PCCRF. We reï¬ned the deï¬nition of pairwise potential energy, employing linearmodel to connect the unary potential energy and pairwise potential energy. Compared to theequal weight connection model used in the FCCRF, the new fusion model used in the PCCRFRemote Sens. 2020,12, 821 of 25can better reï¬‚ect the di â†µerent roles of information generated from larger receptive ï¬eld andinformation generated from smaller receptive ï¬eld.â€¢ We only used pixel pairs associated with the selected pixels in the PCCRF, which can â†µectivelyreduce the amount of data required for computing models and improve the computationaleciency of the PCCRF.â€¢ Beneï¬ting from the ability to describe the spatial correlation between pixel categories of CRF,Reï¬neNet-PCCRF can not only improve the classiï¬cation accuracy of edge pixels in the winterwheat planting area, but it also has high computing ciency.2. Study Area and Dataset2.1. Study AreaTaiâ€™an City covers an area of 7761 km2within the Shandong Province of China (116200to 117590E, 35380to 36280N), including 3665 km2of farmland. This region is an important crop productionarea (Figure 1). The area is temperate, continental, semi-humid, monsoon climate zone with fourdistinct seasons and su cient light and heat to allow for crop growth. The average annual temperatureis 12.9C, the average annual sunshine is 2627.1 h, and the average annual rainfall is 697 mm. The maincrops include winter wheat (grown from October through June of the following year) and corn (grownfrom April to November).Remote Sens. 2020, 12, 821 of 26 Based on prior knowledge, we modified the fully connected conditional random field (FCCRF) to build the PCCRF. We refined the definition of pairwise potential energy, employing linear model to connect the unary potential energy and pairwise potential energy. Compared to the equal weight connection model used in the FCCRF, the new fusion model used in the PCCRF can better reflect the different roles of information generated from larger receptive field and information generated from smaller receptive field. We only used pixel pairs associated with the selected pixels in the PCCRF, which can effectively reduce the amount of data required for computing models and improve the computational efficiency of the PCCRF. Benefiting from the ability to describe the spatial correlation between pixel categories of CRF, RefineNet-PCCRF can not only improve the classification accuracy of edge pixels in the winter wheat planting area, but it also has high computing efficiency. 2. Study Area and Dataset 2.1. Study Area Taiâ€™an City covers an area of 7761 km2 within the Shandong Province of China (116Â°20â€² to 117Â°59â€² E, 35Â°38â€² to 36Â°28â€² N), including 3665 km2 of farmland. This region is an important crop production area (Figure 1). The area is temperate, continental, semi-humid, monsoon climate zone with four distinct seasons and sufficient light and heat to allow for crop growth. The average annual temperature is 12.9 Â°C, the average annual sunshine is 2627.1 h, and the average annual rainfall is 697 mm. The main crops include winter wheat (grown from October through June of the following year) and corn (grown from April to November). Figure 1. Geographical location and crop distribution for Tai'an City, Shandong Province, China. 2.2. Remote Sensing and Pre-Processing We collected 37 Gaofen-2 (GF-2) remote sensing images from November 2018 to April 2019 covering the entire study area. Each GF-2 image consisted of multispectral and panchromatic image. The former was composed of four spectral bands (blue, green, red, and near-infrared), where the spatial resolution of each multispectral image was m, whereas that of the panchromatic image was m. Environment for Visualizing Images (ENVI) software Version 5.5 (developed by Harris Geospatial Solutions, Broomfield, Colorado, United States of America) is remote sensing image processing software that integrates numerous mainstream image processing tools and therefore improves the efficiency of image processing and utilization. ENVI can especially use an interactive data language to develop image processing programs according to our requirements, which can Figure 1. Geographical location and crop distribution for Taiâ€™an City, Shandong Province, China.2.2. Remote Sensing and Pre-ProcessingWe collected 37 Gaofen-2 (GF-2) remote sensing images from November 2018 to April 2019covering the entire study area. Each GF-2 image consisted of multispectral and panchromatic image.The former was composed of four spectral bands (blue, green, red, and near-infrared), where the spatialresolution of each multispectral image was m, whereas that of the panchromatic image was m.Environment for Visualizing Images (ENVI) software Version 5.5 (developed by Harris GeospatialSolutions, Broomï¬eld, Colorado, United States of America) is remote sensing image processingsoftware that integrates numerous mainstream image processing tools and therefore improvesthe ciency of image processing and utilization. ENVI can especially use an interactive datalanguage to develop image processing programs according to our requirements, which can furtherimprove our work ciency. We used ENVI which copyright purchased by Shandong ProvincalClimate Center to preprocess the imagery through three steps: atmospheric correction used the Fastline-of-sight atmospheric analysis of spectral hypercubes (FLAASH) module, orthorectiï¬cation usedthe Rational Polynomial Coe cient (RPC) module, and data fusion used the Nearest NeighborRemote Sens. 2020,12, 821 of 25Diâ†µusion (NNDi â†µuse) Pan Sharpening module. We developed batch program using an interactivedata language (IDL) to improve the degree of automation during pre-processing.After pre-processing, each image contained four channels (red, blue, green, and near-infrared)with spatial resolution of m.The main land-use types used for image capture were winter wheat, mountain land, water, urbanresidential area, agricultural building, woodland, farm land, roads, and rural residential area, amongothers. As winter wheat was the main crop in the pre-processed images, we used it as the extractiontarget in this study to test the â†µectiveness of the proposed method.2.3. Create Imageâ€“Label Pair DatasetLarger image blocks are advantageous for model training. Considering the hardware used inour research, we cut each pre-processed image into equal-sized image blocks (1000 â‡¥1000 pixels).A total of 920 cloudless image blocks were selected for manual labeling with numbers assigned tothe following categories: (1) winter wheat, (2) mountain land, (3) water, (4) urban residential area,(5) agricultural building, (6) woodland, (7) farm land, (8) roads, (9) rural residential area, and (10)others. While selecting the pixel blocks, we used the following principle: each pixel block shouldcontain at least three land-use types, where the area proportion of each land-use type in the selectedimages was similar to that in the pre-processed images.We created label ï¬le for each image block, comprising single-channel image ï¬le in which thenumber of rows and columns was identical to the corresponding image. We used visual interpretationto assign category number to each pixel and saved it in the corresponding location in the label ï¬le.After labeling, the image block and its corresponding label ï¬le formed an imageâ€“label pair (Figure 2).Remote Sens. 2020, 12, 821 of 26 further improve our work efficiency. We used ENVI which copyright purchased by Shandong Provincal Climate Center to preprocess the imagery through three steps: atmospheric correction used the Fast line-of-sight atmospheric analysis of spectral hypercubes (FLAASH) module, orthorectification used the Rational Polynomial Coefficient (RPC) module, and data fusion used the Nearest Neighbor Diffusion (NNDiffuse) Pan Sharpening module. We developed batch program using an interactive data language (IDL) to improve the degree of automation during pre-processing. After pre-processing, each image contained four channels (red, blue, green, and near-infrared) with spatial resolution of m. The main land-use types used for image capture were winter wheat, mountain land, water, urban residential area, agricultural building, woodland, farm land, roads, and rural residential area, among others. As winter wheat was the main crop in the pre-processed images, we used it as the extraction target in this study to test the effectiveness of the proposed method. 2.3. Create Imageâ€“Label Pair Dataset Larger image blocks are advantageous for model training. Considering the hardware used in our research, we cut each pre-processed image into equal-sized image blocks (1000 1000 pixels). total of 920 cloudless image blocks were selected for manual labeling with numbers assigned to the following categories: (1) winter wheat, (2) mountain land, (3) water, (4) urban residential area, (5) agricultural building, (6) woodland, (7) farm land, (8) roads, (9) rural residential area, and (10) others. While selecting the pixel blocks, we used the following principle: each pixel block should contain at least three land-use types, where the area proportion of each land-use type in the selected images was similar to that in the pre-processed images. We created label file for each image block, comprising single-channel image file in which the number of rows and columns was identical to h c r e p n i g m g . e s d i u l interpretation to assign category number to each pixel and saved it in the corresponding location in the label file. After labeling, the image block and its corresponding label file formed an imageâ€“label pair (Figure 2). Figure 2. An example of an imageâ€“label pair used for testing the proposed method: (a) image block, (b) manually-labeled image corresponding to (a), (c) image block, and (d) manually-labeled image corresponding to (c). Figure 2. An example of an imageâ€“label pair used for testing the proposed method: a) image block,(b) manually-labeled image corresponding to a), (c) image block, and d) manually-labeled imagecorresponding to c).3. MethodologyWe ï¬rst modiï¬ed the original Reï¬neNet model as an initial segmentation model (Section 3.1), andthen performed statistical analysis on the initial segmentation results to obtain the prior knowledge(Section 3.2). Based on the obtained knowledge, we constructed the PCCRF model (Section 3.3) andRemote Sens. 2020,12, 821 of 25trained the model (Section 3.4). The trained model was then used to reï¬ne the initial segmentationresults of the CNNs to generate the ï¬nal results. We designed set of comparative experimentsto evaluate the performance of the proposed method (Section 3.5). Figure 3summarizes the entireï¬‚owchart of the proposed approach.Remote Sens. 2020, 12, 821 of 26 3. Methodology We first modified the original RefineNet model as an initial segmentation model (Section 3.1), and then performed statistical analysis on the initial segmentation results to obtain the prior knowledge (Section 3.2). Based on the obtained knowledge, we o s r c e t e C R m d l (Section 3.3) and trained the model (Section 3.4). The trained model was then used to refine the initial segmentation results of the CNNs to generate the final results. We designed set of comparative experiments to evaluate the performance of the proposed method (Section 3.5). Figure summarizes the entire flowchart of the proposed approach. Figure 3. Flowchart of the proposed approach. CL: Confidence level. 3.1. Improved RefineNet Model We selected RefineNet as our initial segmentation model. Unlike the FCN, SegNet, DeepLab, and other models, this model uses multi-path structure that fuses low-level detailed semantic features with high-level rough semantic features, thereby effectively improving the distinguishability of the pixel features. We modified the classic RefineNet model to initially segment remote sensing images; Figure shows the structure of the improved RefineNet model. Figure 3. Flowchart of the proposed approach. CL: Conï¬dence level.3.1. Improved Reï¬neNet ModelWe selected Reï¬neNet as our initial segmentation model. Unlike the FCN, SegNet, DeepLab, andother models, this model uses multi-path structure that fuses low-level detailed semantic featureswith high-level rough semantic features, thereby â†µectively improving the distinguishability of thepixel features. We modiï¬ed the classic Reï¬neNet model to initially segment remote sensing images;Figure 4shows the structure of the improved Reï¬neNet model.Improvements to the Reï¬neNet model were as follows.First, we replaced the equal weight fusion model used in the classic model with linear fusionmodel to fuse detailed low-level semantic features and high-level rough semantic features. The fusionmethod is as follows:s=aâ‡¥f+bâ‡¥g, (1)where sdenotes the fused features, frepresents the detailed low-level semantic feature values generatedby the convolution block, gdenotes the up-sampling feature of the high-level rough semantic features,andaandbare the coe cients of the fusion model. The speciï¬c values of aandbmust be determinedvia model training.Second, we modiï¬ed the classiï¬er of Reï¬neNet, i.e., Softmax, to simultaneously output theprediction category label and category probability vector, P, for each pixel.The probability value of pixel was assigned as the ith category label pi, which was calculatedas follows:pi=eriPm1erj, (2)where mis the number of categories, and riandrjrepresent the output of the Reï¬neNet encoder, i.e.,the product of the pixelâ€™s feature vector and ith feature function, respectively. Based on the deï¬nitionofpi,Pcan be deï¬ned as follows:P=(p1,p2,...,pm). (3)We used the stochastic gradient descent algorithm 79] to train the improved Reï¬neNet model,and used the trained model to segment image blocks to obtain initial segmentation results, includingthe prediction label image and category probability vectors for each pixel.Remote Sens. 2020,12, 821 of 25Remote Sens. 2020, 12, 821 of 26 Figure 4. Structure of the improved RefineNet model used in this study. CB: Convolution Block; FF: Fuse Function; UL: Upsampling Layer Improvements to the RefineNet model were as follows. First, we replaced the equal weight fusion model used in the classic model with linear fusion model to fuse detailed low-level semantic features and high-level rough semantic features. The fusion method is as follows: ð‘ =ð‘ŽÃ— ð‘“+ð‘Ã— ð‘”, (1) where d n t s h f s d e t r s f represents the detailed low-level semantic feature values generated by the convolution block, denotes the up-sampling feature of the high-level rough semantic features, and and are the coefficients of the fusion model. The specific values of and must be determined via model training. Second, we modified the classifier of RefineNet, . . S f m x t s m ultaneously output the prediction category label and category probability vector, P, for each pixel. The probability value of pixel was assigned as the ith category label pi, which was calculated as follows: ð‘=à³âˆ‘à³à³•à°­, (2) where is the number of categories, and ri and rj represent the output of the RefineNet encoder, i.e., the product of the pixelâ€™s feature vector and ith feature function, respectively. Based on the definition of pi, can be defined as follows: ð‘ƒ=( ð‘à¬µ,ð‘à¬¶,â€¦,ð‘). (3) We used the stochastic gradient descent algorithm [79] to train the improved RefineNet model, and used the trained model to segment image blocks to obtain initial segmentation results, including the prediction label image and category probability vectors for each pixel. 3.2. Statistical Analysis of the Initial Segmentation Results Figure 4. Structure of the improved Reï¬neNet model used in this study. CB: Convolution Block; FF:Fuse Function; UL: Upsampling Layer.3.2. Statistical Analysis of the Initial Segmentation ResultsIn previous study 21], we proposed the conï¬dence level, CL, as an indicator to evaluate thecredibility degree of the predicted category label of the pixel using the CNN:CL=pmaxpmax0, (4)where pmaxrepresents the maximum value of Pandpmaxâ€™represents the maximum value of Pwithpmaxexcluded.We used Cgate to represent the conï¬dence level threshold. The predicted category label of thepixel was considered credible if CL>Cgate and not if otherwise. After Cgate was determined, the pixelsetI={1, 2, ...,m} was divided into two subsets, as follows:PC={i},CLof pixel iCgate of pixel i, (5)PIC={i},CLof pixel i<Cgate of pixel i. (6)As the classiï¬cation results of the pixels in the PCwere credible, we only needed to post-processthe classiï¬cation results of the pixels in the PIC.The value of Cgate had signiï¬cant impact on the overall accuracy. When Cgate was high, thenumber of pixels that required post-processing was large, such that there was signiï¬cant improvementin the overall classiï¬cation accuracy. When the value of Cgate was low, the number of pixels thatrequired post-processing was small, but improvements to the overall classiï¬cation accuracy were notalways apparent.The following steps were used in our study to determine the value of Cgate First, we useda TIFF ï¬le to store the CL while we predicted the category label, category probability vector, andmanual-labeled category.Remote Sens. 2020,12, 821 of 25Second, the pixels were divided into two sets based on the artiï¬cially-labeled category andpredicted category using the following rules:PR={i}, mannul category label of pixel i=Predicted category label of i, (7)PW={i}, mannul category label of pixel i,Predicted catagory label of i. (8)Third, histogram was produced for PRandPWusing the CLas the x-axis and the number ofpixels corresponding to certain CLvalue as the y-axis. Figure 5provides an example of histogram,which was used to determine the value of Cgate. In general, the principle is that when CL is greaterthan Cgate the number of misclassiï¬ed pixels should be as small as possible.Remote Sens. 2020, 12, 821 of 26 In previous study [21], we proposed the confidence level, CL, as an indicator to evaluate the credibility degree of the predicted category label of the pixel using the CNN: ð¶ð¿ ð‘à¯«âˆ’ð‘à¯«á‡², (4) where pmax represents the maximum value of and pmaxâ€™ represents the maximum value of with pmax excluded. We used Cgate to represent the confidence level threshold. The predicted category label of the pixel was considered credible if CL Cgate, and not if otherwise. After Cgate was determined, the pixel set = {1, 2, â€¦, m} was divided into two subsets, as follows: ð‘ƒð¶ ={ð‘–},ð¶ð¿ of pixel ð‘–ð¶ð‘”ð‘Žð‘¡ð‘’ of pixel ð‘–, (5) ð‘ƒð¼ð¶ {ð‘–},ð¶ð¿ of pixel àµ ð¶ð‘”ð‘Žð‘¡ð‘’ of pixel ð‘–. (6) As the classification results of the pixels in the PC were credible, we only needed to post-process the classification results of the pixels in the PIC. The value of Cgate had significant impact on the overall accuracy. When Cgate was high, the number of pixels that required post-processing was large, such that there was significant improvement in the overall classification accuracy. When the value of Cgate was low, the number of pixels that required post-processing was small, u i p o e e t t t e v r l c a s f c t o accuracy were not always apparent. The following steps were used in our study to determine the value of Cgate. First, we used TIFF file to store the CL while we predicted the category label, category probability vector, and manual-labeled category. Second, the pixels were divided into two sets based on the artificially-labeled category and predicted category using the following rules: ð‘ƒð‘… ={ð‘–},m n u c t g ry label of pixel = Predicted category label of ð‘–, (7) ð‘ƒð‘Š ={ð‘–}, mannul category label of pixel àµ Predicted catagory label of ð‘–. (8) Third, histogram was produced for PR and PW using the CL as the x-axis and the number of pixels corresponding to certain CL value as the y-axis. Figure provides an example of histogram, which was used to determine the value of Cgate. In general, the principle is that when CL is greater than Cgate, the number of misclassified pixels should be as small as possible. Figure 5. An example of histogram. Figure 5. An example of histogram.3.3. The PCCRF Model3.3.1. Description of the Modeling SchemeAccording to the obtained prior knowledge, in the classiï¬cation results generated by the CNN, theresults for the pixels located inside the object are credible, but the credibility of the pixels located at theedge of the object is low. Furthermore, only low-credibility classiï¬cation results require post-processing.Based on previous studies 51â€“53,58,59], approximately 80% of the pixel-by-pixel classiï¬cationresults generated by CNN models are credible. Therefore, only approximately 20% of the pixelclassiï¬cation results require post-processing. This strategy can signiï¬cantly reduce the number ofcalculations, thereby improving the ciency and performance of the model. This is in reference toour use of term â€œpartly connected.â€Based on the abovementioned analysis, we consider the following case: on given image, whenthe category labels of certain pixels have been determined by the CNN, how the category labels of theremaining pixels are to be determined needs to be clariï¬ed (Figure 6).Remote Sens. 2020,12, 821 of 25Remote Sens. 2020, 12, 821 of 26 3.3. The PCCRF Model 3.3.1. Description of the Modeling Scheme According to the obtained prior knowledge, in the classification results generated by the CNN, the results for the pixels located inside the object are credible, but the credibility of the pixels located at the edge of the object is low. Furthermore, only low-credibility classification results require post-processing. Based on previous studies [51â€“53,58,59], approximately 80% of the pixel-by-pixel classification results generated by CNN models are credible. Therefore, only approximately 20% of the pixel classification results require post-processing. This strategy can significantly reduce the number of calculations, thereby improving the efficiency and performance of the model. This is in reference to our use of term â€œpartly connected.â€ Based on the abovementioned analysis, we consider the following case: on given image, when the category labels of certain pixels have been determined by the CNN, how the category labels of the remaining pixels are to be determined needs to be clarified (Figure 6). Figure 6. description of the modeling progress for partly connected conditional random field. We can observe that the main difference between the PCCRF and FCCRF is that the former can take full advantage of the fact that certain pixels have already been assigned certain category labels. In the PCCRF, we used the category probability vectors generated by the CNN to build unary potential energy similar to the FCCRF by using the relationship between pixel pairs to build pairwise potential energy. Considering that there are numerous mixed pixels on the remote sensing image, we must select appropriate features to form feature vector for the pixels (Section 3.3.2), and then use these vectors to define the pairwise potential energy (Section 3.3.3). Based on this, we can provide the definition of PCCRF (Section 3.3.4). 3.3.2. Features Selection Based on prior knowledge, the inner and edge pixels of the winter wheat planting areas are extremely similar in terms of color and texture. Considering that the near-infrared band (NIR) can better distinguish between crops and non-crops, we selected the red, blue, green, and NIR bands, along with the NDVI, contrast (CON), uniformity (UNI), inverse difference (INV), and entropy (ENT), to construct the feature vectors for the pixels. The NDVI was calculated following the methods reported in Ma et al. [17]: ð‘ð·ð‘‰ð¼ =à¯‡à¯‚à¯‹à¬¿à¯‹à¯—à¯‡à¯‚à¯‹à¬¾à¯‹à¯—. (9) Figure 6. description of the modeling progress for partly connected conditional random ï¬eld.We can observe that the main di â†µerence between the PCCRF and FCCRF is that the former cantake full advantage of the fact that certain pixels have already been assigned certain category labels.In the PCCRF, we used the category probability vectors generated by the CNN to build unarypotential energy similar to the FCCRF by using the relationship between pixel pairs to build pairwisepotential energy. Considering that there are numerous mixed pixels on the remote sensing image, wemust select appropriate features to form feature vector for the pixels (Section 3.3.2), and then usethese vectors to deï¬ne the pairwise potential energy (Section 3.3.3). Based on this, we can provide thedeï¬nition of PCCRF (Section 3.3.4).3.3.2. Features SelectionBased on prior knowledge, the inner and edge pixels of the winter wheat planting areas areextremely similar in terms of color and texture. Considering that the near-infrared band (NIR) canbetter distinguish between crops and non-crops, we selected the red, blue, green, and NIR bands, alongwith the NDVI, contrast CON ), uniformity UNI), inverse di â†µerence INV), and entropy ENT ), toconstruct the feature vectors for the pixels. The NDVI was calculated following the methods reportedin Ma et al. 17]:NDVI =NIR RedNIR+Red. (9)Here, CON ,UNI,INV, and ENT were extracted using the methods proposed by Yang and Yang 27],based on the GLCM:CON =Xq1n=0n2â‡¢Xqi=1Xqj=1g(i,j)whereij=n, (10)UNI =Xqi=1Xqj=1(g(i,j))2, (11)INV =Xqi=1Xqj=1g(i,j)1+(ij)2, (12)ENT =Xqi=1Xqj=1(g(i,j)logg(i,j) (13)where qis the gray level and g(i,j) is an element of the GLCM.The feature vector fof each pixel comprises nine elements, structured as follows:f=(red,green ,blue,NIR,NDVI ,UNI,CON ,ENT ,INV). (14)Remote Sens. 2020,12, 821 10 of 253.3.3. Deï¬nition of the Pairwise Potential EnergyBased on the Gaussian kernel function, we deï¬ne the potential energy of pixel pair, âŒ§â‡£xi,xjâŒ˜, as:âŒ§â‡£xi,xjâŒ˜=Âµâ‡£xi,xjâŒ˜0BBBBB@!(1)exp0BBBBB@fifj22âœ“2â†µIiIj22âœ“21CCCCCA+!(1)exp0BBBB@fifj22âœ“21CCCCA1CCCCCA, (15)where iandjeach represent single pixel of image I,xiis the predicted category label of pixel iby theCNN, xjrepresents the predicted category label of pixel jby the CNN, xiandxjare elements of categorylabel set L={l1,l2,...,ln},fi,fjrepresent the feature vector of the pixel, as discussed in Section 3.3.2,IiIjis the Manhattan distance between iandj,fifjis the Euclidean distance between iandj, and Âµ(xi,xj) is the label comparison function. When xiandxjare identical, the value was set to 0;otherwise, it is set to 1. Here, !(1),!(2),âœ“â†µ,âœ“, and âœ“are determined through training the PCCRF.Based on the deï¬nition of âŒ§ij, we can deï¬ne the sum of the pairwise potential energy of xi,âŒ§(xi), as:âŒ§(xi)=Xj2IâŒ§â‡£xi,xjâŒ˜. (16)The total pairwise potential energy associated with iis deï¬ned as follows:âŒ§(i)=Xxi2LâŒ§(xi). (17)Considering that the unary potential energy is an element of the category probability vector, thevalue range is [0, 1], and therefore, we used âŒ§(i)to normalize âŒ§(xi):nâŒ§(xi)=âŒ§(xi)âŒ§(i). (18)We used nâŒ§(xi)to build the PCCRF.3.3.4. Deï¬nition of PCCRFAs discussed in Section 3.2,Iis the set of pixels and PCandPICare the subsets of I. As theclassiï¬cation results of the pixels in PCwere credible, we only needed to optimize the classiï¬cationresults of the pixels in PIC. Based on the above-mentioned analysis, we only used such pixel pairs tobuild the PCCRF, where at least one pixel in the pixel-pair belonged to PIC.Letibe pixel in PICandjbe pixel in I. Therefore, x={x1,x2,...,xm} represents label setassignment of PIC. Then, âœ“represents the model parameter set of !(1),!(2),âœ“â†µ,âœ“, and âœ“. We deï¬nethe Gibbs energy of xas follows:E(x|PIC,âœ“)=Xi2PIC(@'(xi)+(1@)nâŒ§(xi)), (19)where '(xi)represents the unary potential energy of xi,'(xi)is an element of the category probabilityvector of pixel igenerated by the CNN, @is the weight value for the unary potential energy, and (1@)is the weight value for the pairwise potential energy. Here, @is determined while training the PCCRF.Based on the above analysis, we deï¬ne the PCCRF as follows:P(X=x|PIC,âœ“)=E(x|PIC,âœ“)Py2XE(yPIC,âœ“), (20)where Xrepresents the set of all possible label set assignments of the PIC and yrepresents label setassignment of the PIC.By minimizing the above CRF energy, E(x), we can assign an optimal set of labels to the PIC.Remote Sens. 2020,12, 821 11 of 25In the PCCRF, '(xi)provides the information from large receptive ï¬eld to predict the categorylabel for pixel, while nâŒ§(xi)provides additional information from small receptive ï¬eld to optimizethe category label.The PCCRF takes full advantage of prior information. When the predicted category of the pixelusing the CNN is credible, the category label can be determined using only the information from alarge receptive ï¬eld. Otherwise, it uses additional information to optimize the category label.3.4. PCCRF TrainingWe deï¬ned the objective function of the PCCRF based on the cross-entropy of the samplesas follows:H(p,q)=Xtq=1qilog(pi), (21)where pis the predicted category probability distribution (CPD) output by the PCCRF, qis the actualCPD, tis the number of category labels, and iis the index of an element in the CPD. Based on this, theloss function of the PCCRF model was deï¬ned as follows:Loss =1TotalXtsXti=1qilog(pi), (22)where Total is the number of samples used in the training stage. We then used the stochastic gradientdescent to train the model via the following steps:1. Pretrained the Reï¬neNet;2. Constructed the PCCRF training dataset using the training prediction results generated by thetrained Reï¬neNet;3. Performed statistical analysis on the training dataset and determined the value of Cgate ;4. Initialized the parameters of the PCCRF model; and5. Calculated the parameters of the PCCRF using the method proposed in Zheng et al. 55].3.5. Experimental SetupWe conducted comparison experiments based on the Reï¬neNet (which combines low-level andhigh-level features) and SegNet (which only uses high-level semantic features) using three levelsof conï¬guration for each experiment: the original model, classic CRF post-processing, and PCCRFpost-processing (Table 1).Table 1. Model conï¬gurations used for the comparative experiments.Number Name Description1 SegNet Extraction using only SegNet2 SegNet-CRF Classic CRF post-processing of SegNet results3 SegNet-PCCRF PCCRF post-processing of SegNet results4 Reï¬neNet Extraction using only Reï¬neNet5 Reï¬neNet-CRF Classic CRF post-processing of Reï¬neNet results6 Reï¬neNet-PCCRF PCCRF post-processing of Reï¬neNet results (method proposed here)We applied data augmentation techniques on the training dataset, such as horizontal ï¬‚ip, coloradjustment, and vertical ï¬‚ip steps. The color adjustment factors included brightness, hue, saturation,and contrast. Each image in the training dataset was processed 10 times. All images created using thedata augmentation techniques were only used for training the CNNs.We used cross-validation techniques in the comparative experiments. Each CNN model wastrained over ï¬ve rounds. In each round, 200 images were selected as test images and the other imageswere used as training images to guarantee that each image was used at least once as test image.Remote Sens. 2020,12, 821 12 of 25Table 2lists the hyper-parameter setup used to train the proposed Reï¬neNet-PPCRF. In thecomparison experiments, the hyper-parameters were also applied to the comparison model.Table 2. The hyper-parameter setup.Hyper-Parameter ValueMini-batch size 32Learning rate 0.00001Momentum 0.9Epochs 30,0004. Results and EvaluationFigure 7presents 10 randomly selected image blocks and their corresponding results using the sixcomparison methods.Remote Sens. 2020, 12, 821 13 of 26 Figure 7. Comparison of the segmentation results for 10 randomly selected image blocks: (a) original images, (b) manually-labeled images corresponding to (a), (c) SegNet, (d) SegNet-CRF, (e) SegNet-PCCRF, (f) RefineNet, (g) RefineNet-CRF, and (h) RefineNet-PCCRF. CRF: Conditional random field, PCCRF: Partly connected conditional random field. Although there were certain misclassified pixels in the inner regions of the winter wheat planting area in the SegNet results, the overall classification accuracy of each comparison method in the inner regions of the winter wheat planting area was satisfactory. The difference between the result of the six comparison modes at the edge was observable. In the SegNet results, the edges of the winter wheat fields were rough, and therefore, the RefineNet results were superior to those of the SegNet, thereby demonstrating the importance of using fused features over high-level features. Both the CRF and PCCRF post-processing methods produced superior results, thus demonstrating the importance of post-processing procedures. The SegNet-PCCRF was superior to SegNet-CRF, while the RefineNet-PCCRF was superior to the RefineNet-CRF; this demonstrated that the PCCRF was more suitable as post-processing method. Comparing the SegNet-PCCRF and RefineNet-CRF, the performance of the RefineNet-CRF was superior, thereby confirming that the initial segmentation method was also an extremely significant factor in determining the final result. Figure 7. Comparison of the segmentation results for 10 randomly selected image blocks:(a) original images, b) manually-labeled images corresponding to a), (c) SegNet, d) SegNet-CRF,(e) SegNet-PCCRF, f) Reï¬neNet, g) Reï¬neNet-CRF, and h) Reï¬neNet-PCCRF. CRF: Conditionalrandom ï¬eld, PCCRF: Partly connected conditional random ï¬eld.Remote Sens. 2020,12, 821 13 of 25Although there were certain misclassiï¬ed pixels in the inner regions of the winter wheat plantingarea in the SegNet results, the overall classiï¬cation accuracy of each comparison method in theinner regions of the winter wheat planting area was satisfactory. The di â†µerence between the resultof the six comparison modes at the edge was observable. In the SegNet results, the edges of thewinter wheat ï¬elds were rough, and therefore, the Reï¬neNet results were superior to those of theSegNet, thereby demonstrating the importance of using fused features over high-level features. Boththe CRF and PCCRF post-processing methods produced superior results, thus demonstrating theimportance of post-processing procedures. The SegNet-PCCRF was superior to SegNet-CRF, whilethe Reï¬neNet-PCCRF was superior to the Reï¬neNet-CRF; this demonstrated that the PCCRF wasmore suitable as post-processing method. Comparing the SegNet-PCCRF and Reï¬neNet-CRF, theperformance of the Reï¬neNet-CRF was superior, thereby conï¬rming that the initial segmentationmethod was also an extremely signiï¬cant factor in determining the ï¬nal result.We used four popular criteria, named accuracy, precision, recall, and F1-score 80] to evaluate theperformance of the proposed model. They were calculated using the confusion matrix.Accuracy is the ratio of the number of correctly classiï¬ed samples to the total number of samples,calculated as:Accuracy =Pmi=1ciiPmi=1Pmj=1cij, (23)where ciidenotes the number of correctly classiï¬ed samples, and cijis the number of samples of class imisidentiï¬ed as class j. Precision denotes the average proportion of pixels correctly classiï¬ed into oneclass from the total retrieved pixels, calculated as:Precision =12Xicii/Xjcij. (24)Recall represents the average proportion of pixels that are correctly classiï¬ed in relation to theactual total pixels of given class, calculated as:Recall =12Xicii/Xicij. (25)F1-score represents the harmonic mean of precision and recall, calculated as:F1=2Â·Precision â‡¥RecallPrecision +Recall. (26)We evaluated the results using the accuracy, precision, recall, and F1-score. The Reï¬neNet-PCCRFscored highest among all models using all metrics (Table 3).Table 3. Comparison of the six results.Index SegNet SegNet-CRF SegNet-PCCRF Reï¬neNet Reï¬neNet-CRF Reï¬neNet-PCCRFAccuracy 79.01% 81.31% 83.86% 86.79% 94.01% 94.51%Precision 76.50% 78.94% 80.68% 85.45% 91.71% 92.39%Recall 73.61% 76.24% 80.40% 79.54% 89.16% 90.98%F1-score 75.03% 77.57% 80.54% 82.39% 90.42% 91.68%The confusion matrices for all categories (Figure 8) and The confusion matrices for winter wheatand others (Figure 9) for each models demonstrating that the Reï¬neNet-PCCRF achieved the bestsegmentation results.Remote Sens. 2020,12, 821 14 of 25Remote Sens. 2020, 12, 821 15 of 26 Figure 8. Cont.Remote Sens. 2020,12, 821 15 of 25Remote Sens. 2020, 12, 821 16 of 26 Figure 8. Cont.Remote Sens. 2020,12, 821 16 of 25Remote Sens. 2020, 12, 821 17 of 26 Figure 8. Confusion matrices of different models using the GaoFen-2 (GF-2) image datasets: (a) SegNet, (b) SegNet-CRF, (c) SegNet-PCCRF, (d) RefineNet, (e) RefineNet-CRF, and (f) RefineNet-PCCRF. Figure 8. Confusion matrices of di â†µerent models using the GaoFen-2 (GF-2) image datasets: a) SegNet,(b) SegNet-CRF, c) SegNet-PCCRF, d) Reï¬neNet, e) Reï¬neNet-CRF, and f) Reï¬neNet-PCCRF.Remote Sens. 2020,12, 821 17 of 25Remote Sens. 2020, 12, 821 18 of 26 Figure 9. Confusion matrices of the different models using the GF-2 image datasets: (a) SegNet, (b) SegNet-CRF, (c) SegNet-PCCRF, (d) RefineNet, (e) RefineNet-CRF, and (f) RefineNet-PCCRF. In the confusion matrices of the six models, there was nearly no confusion between the winter wheat and urban areas. This could be attributed to the difference in the characteristics of the two land-use types. However, the confusion between winter wheat and farmland was serious. This was because most winter wheat regions that were misclassified as farmlands had poor growing conditions. In these areas, their characteristics were similar to those of farmlands in winter, which led to greater probability of misclassification. There was also certain degree of confusion in the winter wheat and woodland areas. This was because certain trees were still green in winter, similar to the Figure 9. Confusion matrices of the di â†µerent models using the GF-2 image datasets: a) SegNet,(b) SegNet-CRF, c) SegNet-PCCRF, d) Reï¬neNet, e) Reï¬neNet-CRF, and f) Reï¬neNet-PCCRF.In the confusion matrices of the six models, there was nearly no confusion between the winterwheat and urban areas. This could be attributed to the di â†µerence in the characteristics of the twoland-use types. However, the confusion between winter wheat and farmland was serious. This wasbecause most winter wheat regions that were misclassiï¬ed as farmlands had poor growing conditions.In these areas, their characteristics were similar to those of farmlands in winter, which led to greaterRemote Sens. 2020,12, 821 18 of 25probability of misclassiï¬cation. There was also certain degree of confusion in the winter wheat andwoodland areas. This was because certain trees were still green in winter, similar to the characteristicsin the regions of winter wheat. However, in this case, due to the use of both texture and high-levelsemantic information, the degree of confusion was signiï¬cantly lower than that of farmland. This alsoexplained the advantage of post-processing from another aspect, as it led to the introduction of newinformation, which could â†µectively improve the accuracy of the classiï¬cation results.Table 4lists the average time required for each method to complete the testing of single image.The proposed Reï¬neNET-PPCRF method required approximately 3% more time but improved theaccuracy by 5%â€“8%. The time consumed by the CRF was higher than that using the proposed PCCRFmethod because the CRF had to calculate the distances between all pixelâ€“pixel pairs for single image,while the proposed PCCRF method calculated the distances for only small number of pixelâ€“pixelpairs. The number of pixelâ€“pixel pairs calculated in the SegNet-PCCRF was only approximately 30%of that of the SegNet-CRF. The number of pixelâ€“pixel pairs calculated in the Reï¬neNet-PCCRF is onlyapproximately 20% of that in the Reï¬neNet-CRF.Table 4. Statistical comparison of model performance.Index SegNet SegNet-CRF SegNet-PCCRF Reï¬neNet Reï¬neNet-CRF Reï¬neNet-PCCRFTime (ms) 301 383 315 293 403 3135. Discussion5.1. PCCRF NecessityThe CNN models typically use multiple convolutional layers to obtain high-level semantic features,which then assign the features to each pixel in the receptive ï¬eld through deconvolution operation.When the operation is performed at the edges of the object, since there may be two or more types ofpixels in the sensory ï¬eld, this can cause di â†µerences in the feature values of edges and inner pixels,resulting in higher classiï¬cation error at object edges (Figure 7).The structural characteristics of the convolutional neural network indicate that there willbe inevitable misclassiï¬cation of pixels at the edges. This problem can only be improved usingpost-processing methods or improving the structure of the convolutional neural network.At present, numerous post-processing methods have been proposed, but most of these methodsfail to make full use of the results provided by convolutional neural networks. The PCCRF proposedin this study comprehensively uses the advantages of the CRF and prior knowledge provided by theCNN, which is more â†µective post-processing method.5.2. Comparison between PCCRF and FCCRFPCCRF has three clear advantages over FCCRF. First, it has clearer model structure. In PCCRF, acategory probability vector is used to express the calculation result, and each component representsthe probability that the pixel to be processed is classiï¬ed into certain category. The class probabilityvector of pixel is divided into two levels for calculation: (1) pixel-level class probability vectorthat represents the class probability distribution calculated on the basis of the characteristics of thepixel itself and (2) class-level class probability vector that represents class probability distributioncalculated on the basis of the class of pixels around the pixel to be classiï¬ed. The scale factor expressesthe fusion of two types of information in which the two messages involved in the fusion have thesame meaning. In contrast, in FCCRF, each component of the ï¬rst level vector is class featurevalue calculated on the basis of the characteristics of the pixel itself, whereas each component of thesecond-level vector is category feature value calculated on the basis of the category information ofthe pixel to be processed and the surrounding pixels. The two feature values with di â†µerent propertiesare added together to produce the class feature value of the pixel. The meaning of the eigenvaluesobtained using this processing method is not clear enough.Remote Sens. 2020,12, 821 19 of 25Second, FCCRF does not introduce any prior knowledge, and all pixel-pairs need to be calculated,which leads to overcalculation. Hence, there is need to solve model parameters through ï¬ndingapproximate values. In contrast, PCCRF introduces prior knowledge and only processes pixels withlow classiï¬cation reliability, â†µectively reducing the number of calculations and directly solving themodel through methods such as the stochastic gradient descent algorithm.Third, PCCRF uses color, texture, and low-level semantics to form feature vectors, which is morein line with the characteristics of remote sensing data. FCCRF obtains good results using only colorfeatures because the camera image resolution is usually very high and the detailed information is veryrich. The color of the pixels often di â†µers greatly where two objects are adjacent. However, in remotesensing imagery, large number of mixed pixels means that the di â†µerences in the pixel color of twoobjects are often much smaller, and hence, the additional information used by PCCRF improves itsclassiï¬cation performance.5.3. Cgate â†µectGiven the overall importance of the Cgate parameter in the Reï¬neNet-PCCRF, we held otherparameters steady and calculated the relationships among the Cgate accuracy (Figure 10), andconsumed time (Figure 11).Remote Sens. 2020, 12, 821 20 of 26 Second, FCCRF does not introduce any prior knowledge, and all pixel-pairs need to be calculated, which leads to overcalculation. Hence, there is need to solve model parameters through finding approximate values. In contrast, PCCRF introduces prior knowledge and only processes pixels with low classification reliability, effectively reducing the number of calculations and directly solving the model through methods such as the stochastic gradient descent algorithm. Third, PCCRF uses color, texture, and low-level semantics to form feature vectors, which is more in line with the characteristics of remote sensing data. FCCRF obtains good results using only color features because the camera image resolution is usually very high and the detailed information is very rich. The color of the pixels often differs greatly where two objects are adjacent. However, in remote sensing imagery, large number of mixed pixels means that the differences in the pixel color of two objects are often much smaller, and hence, the additional information used by PCCRF improves its classification performance. 5.3. Cgate Effect Given the overall importance of the Cgate parameter in the RefineNet-PCCRF, we held other parameters steady and calculated the relationships among the Cgate, c u a y F g r 1 ) a d consumed time (Figure 11). Figure 10. The relationship between the average segmentation accuracy and Cgate. Figure 10. The relationship between the average segmentation accuracy and Cgate .Remote Sens. 2020,12, 821 20 of 25Remote Sens. 2020, 12, 821 21 of 26 Figure 11. Relationship between average consumed time and Cgate. Higher Cgate values improved the accuracy because pixels were filtered with higher level of confidence. Post-processing resulted in the reclassification of the initially misclassified pixels, thus improving the accuracy of the overall result. Therefore, when selecting the Cgate value, we must consider the classification ability of the initial segmentation model. In addition, selecting model with stronger classification ability for preliminary segmentation can significantly improve the performance of the results obtained from the PCCRF model. Higher Cgate values also increased the consumed time; this indicated that further reduction in the number of pixels involved in modeling, i.e., using more prior knowledge, is the key to further improving the calculation efficiency of both the PCCRF and classic CRF models. 5.4. Comparison between PP-CNN and RefineNet-PPCRF To obtain high-quality spatial distribution information of winter wheat, we used an improved Euclidean distance to establish PP-CNN as post-processing method [81]. According to the improved Euclidean distance of the feature vector between pixel being classified and the determined winter wheat pixel, it can be determined whether the pixel being classified is displaying winter wheat. Unlike the PP-CNN, the proposed PCCRF was established on the basis of the CRF. Due to the advantage of the CRF using global distribution characteristics, the PP-CRF can more accurately determine the category label of the edge of the winter wheat planting area. In general, the PP-CNN can be used in cases where the feature differences are stable between the mixed pixels on the edge of the winter wheat planting area and the inner pixels of the same area. When the difference is unbalanced, the distance threshold bias is large, which increases the probability of pixel classification errors during post-processing. The PCCRF fully considers the spatial correlation between pixel categories, hence yielding strong global balance ability. Therefore, this method can better handle situations where the edge pixels are significantly different from the inner pixels, thereby effectively reducing the impact of large differences in crop growth. 6. Conclusions CNNs can significantly improve the overall accuracy of remote sensing image segmentation results. However, in the segmentation results, there are certain misclassified pixels in the adjacent land-use types. This study used the advantages f h C F o e t a c n e c i e h s a i l correlation between pixel categories, introduced v r e y f r o k o l d e a d r p s d Figure 11. Relationship between average consumed time and Cgate .Higher Cgate values improved the accuracy because pixels were ï¬ltered with higher level ofconï¬dence. Post-processing resulted in the reclassiï¬cation of the initially misclassiï¬ed pixels, thusimproving the accuracy of the overall result. Therefore, when selecting the Cgate value, we mustconsider the classiï¬cation ability of the initial segmentation model. In addition, selecting model witha stronger classiï¬cation ability for preliminary segmentation can signiï¬cantly improve the performanceof the results obtained from the PCCRF model. Higher Cgate values also increased the consumed time;this indicated that further reduction in the number of pixels involved in modeling, i.e., using moreprior knowledge, is the key to further improving the calculation ciency of both the PCCRF andclassic CRF models.5.4. Comparison between PP-CNN and Reï¬neNet-PPCRFTo obtain high-quality spatial distribution information of winter wheat, we used an improvedEuclidean distance to establish PP-CNN as post-processing method 81]. According to the improvedEuclidean distance of the feature vector between pixel being classiï¬ed and the determined winterwheat pixel, it can be determined whether the pixel being classiï¬ed is displaying winter wheat. Unlikethe PP-CNN, the proposed PCCRF was established on the basis of the CRF. Due to the advantage of theCRF using global distribution characteristics, the PP-CRF can more accurately determine the categorylabel of the edge of the winter wheat planting area.In general, the PP-CNN can be used in cases where the feature di â†µerences are stable betweenthe mixed pixels on the edge of the winter wheat planting area and the inner pixels of the same area.When the di â†µerence is unbalanced, the distance threshold bias is large, which increases the probabilityof pixel classiï¬cation errors during post-processing. The PCCRF fully considers the spatial correlationbetween pixel categories, hence yielding strong global balance ability. Therefore, this method canbetter handle situations where the edge pixels are signiï¬cantly di â†µerent from the inner pixels, therebyeâ†µectively reducing the impact of large di â†µerences in crop growth.6. ConclusionsCNNs can signiï¬cantly improve the overall accuracy of remote sensing image segmentationresults. However, in the segmentation results, there are certain misclassiï¬ed pixels in the adjacentland-use types. This study used the advantages of the CRF model that can describe the spatialRemote Sens. 2020,12, 821 21 of 25correlation between pixel categories, introduced variety of prior knowledge, and proposed PCCRFmodel. The proposed PCCRF model can be used to post-process the results of the CNN to better solvethe problem of rough edges in the results extracted using only the CNN.The main contributions of this study are as follows: (1) Pre-processing (such as statistical analysis ofthe CNN segmentation results) allows for the use of post-processing and modeling of prior knowledge,such that only those pixels with lower conï¬dence are processed, thus signiï¬cantly reducing calculationtime. As the Reï¬neNet has high segmentation accuracy, this post-processing only requires the useof 20% of all the pixels. (2) According to the characteristics of the winter wheat planting area on theremote sensing image, the PCCRF uses original channel values, texture features, and low-level semanticfeatures to compose the feature vector and construct the pairwise potential energy. This feature vectorbetter matches the characteristics of the remote sensing imagery. At the same time, after normalizingthe pairwise potential energy, the data range is identical to that of the unary potential energy. Thisaspect is more reasonable than that of the FCCRF. (3) The PCCRF uses linear model to fuse the unaryenergy and pairwise energy such that the parameters of the linear mode are determined while trainingthe PCCRF. This strategy is more reasonable than the ï¬xed weight value strategy adopted by theFCCRF. Due to the ability to describe the globe spatial correlation between pixel categories of the CRF,the Reï¬neNet-PCCRF can ciently improve the classiï¬cation accuracy of edge pixels in winterwheat planting area.As the prior knowledge required by the PCCRF can only be obtained via statistical analysis ofthe CNN segmentation results, the PCCRF and CNN must be used separately to generate improvedextraction results, which is the major limitation of our method. In future studies, we intend to usehyperparameters and other means to express prior knowledge, convert the PCCRF into convolutionoperations, and construct complete end-to-end training model.Author Contributions: Conceptualization: C.Z. and Z.X.; methodology: C.Z. and Z.X.; software: S.W., H.Y., andZ.Z.; validation: Y.W.; formal analysis: C.Z., Z.X., and S.W.; investigation: H.Y., and Z.Z.; resources, S.G.; datacuration, S.G. and Y.W.; writingâ€”original draft preparation, C.Z., Z.X., and S.W; writingâ€”review and editing: C.Z.;visualization: S.G.; supervision: C.Z.; project administration: C.Z. and S.G; funding acquisition: C.Z. All authorshave read and agreed to the published version of the manuscript.Funding: This research was funded by the National Key R&D Program of China, grant numbers 2017YFA0603004and 2017YFD0301004; the Science Foundation of Shandong, grant number ZR2017MD018; the Key Researchand Development Program of Ningxia, grant number 2019BEH03008; the Open Research Project of the KeyLaboratory for Meteorological Disaster Monitoring, Early Warning and Risk Management of CharacteristicAgriculture in Arid Regions, grant numbers CAMF-201701 and CAMF-201803; and the arid meteorological scienceresearch fund project by the Key Open Laboratory of Arid Climate Change and Disaster Reduction of ChinaMeteorological Administration (CMA), grant number IAM201801. The Article Processing Charges (APC) wasfunded by ZR2017MD018.Acknowledgments: The authors would like to thank Zhongshan Mu and Tianyu Zhao for data provision andparticipating in ï¬eld investigation.Conï¬‚icts of Interest: The authors declare no conï¬‚ict of interest.References1. Chen, X.-Y.; Lin, Y.; Zhang, M.; Yu, L.; Li, H.-C.; Bai, Y.-Q. Assessment of the cropland classiï¬cations in fourglobal land cover datasets: case study of Shaanxi Province, China. J. Integr. Agric. 2017,16, 298â€“311.[CrossRef ]2. Ma, L.; Gu, X.; Xu, X.; Huang, W.; Jia, J.J. Remote sensing measurement of corn planting area based onï¬eld-data. Trans. Chin. Soc. Agric. Eng. 2009,25, 147â€“151. (In Chinese) CrossRef ]3. Nabil, M.; Zhang, M.; Bofana, J.; Wu, B.; Stein, A.; Dong, T.; Zeng, H.; Shang, J. Assessing factors impactingthe spatial discrepancy of remote sensing based cropland products: case study in Africa. Int. J. Appl.Earth Obs. 2020,85, 102010. CrossRef ]4. Song, Q.; Hu, Q.; Zhou, Q.; Hovis, C.; Xiang, M.; Tang, H.; Wu, W. In-Season Crop Mapping with GF-1 /WFVData by Combining Object-Based Image Analysis and Random Forest. Remote Sens. 2017,9, 1184. CrossRef ]Remote Sens. 2020,12, 821 22 of 255. Atzberger, C.; Rembold, F. Mapping the Spatial Distribution of Winter Crops at Sub-Pixel Level UsingAVHRR NDVI Time Series and Neural Nets. Remote Sens. 2013,5, 1335â€“1354. CrossRef ]6. Zhang, J.; Feng, L.; Yao, F. Improved maize cultivated area estimation over large scale combiningMODISâ€“EVI time series data and crop phenological information. ISPRS J. Photogramm. Rem. Sens. 2014,94,102â€“113. CrossRef ]7. Jiang, T.; Liu, X.; Wu, L. Method for mapping rice ï¬elds in complex landscape areas based on pre-trainedconvolutional neural network from HJ-1 /B data. ISPRS Int. J. Geo-Inf. 2018,7, 418. CrossRef ]8. Wang, L.; Jia, L.; Yao, B.; Ji, F.; Yang, F. Area change monitoring of winter wheat based on relationship analysisof GF-1 NDVI among di â†µerent years. Trans. Chin. Soc. Agric. Eng. 2018,34, 184â€“191. (In Chinese) CrossRef ]9. Wang, D.; Fang, S.; Yang, Z.; Wang, L.; Tang, W.; Li, Y.; Tong, C. regional mapping method for oilseed rapebased on HSV transformation and spectral features. ISPRS Int. J. Geo-Inf. 2018,7, 224. CrossRef ]10. Georgi, C.; Spengler, D.; Itzerott, S.; Kleinschmit, B. Automatic delineation algorithm for site-speciï¬cmanagement zones based on satellite remote sensingdata. Precis. Agric. 2018,19, 684â€“707. CrossRef ]11. Mhangara, P.; Odindi, J. Potential of texture-based classiï¬cation in urban landscapes using multispectralaerial photos. S. Afr. J. Sci. 2013,109, 1â€“8. CrossRef ]12. Wang, F.; Kerekes, J.P.; Xu, Z.Y.; Wang, Y.D. Residential roof condition assessment system using deep learning.J. Appl. Remote Sens. 2018,12, 016040. CrossRef ]13. Du, S.; Du, S.; Liu, B.; Zhang, X. Context-Enabled Extraction of Large-Scale Urban Functional Zones fromVery-High-Resolution Images: Multiscale Segmentation Approach. Remote Sens. 2019,11, 1902. CrossRef ]14. Kavzoglu, T.; Erdemir, M.Y.; Tonbul, H. Classiï¬cation of semiurban landscapes from very high-resolutionsatellite images using regionalized multiscale segmentation approach. J. Appl. Rem. Sens. 2017,11, 035016.[CrossRef ]15. Pan, X.; Zhao, J. central-point-enhanced convolutional neural network for high-resolution remote-sensingimage classiï¬cation. Int. J. Remote Sens. 2017,38, 6554â€“6581. CrossRef ]16. Ji, S.; Zhang, C.; Xu, A.; Shi, Y.; Duan, Y. 3D Convolutional Neural Networks for crop classiï¬cation withmulti-temporal remote sensing images. Remote Sens. 2018,10, 75. CrossRef ]17. Ma, Y.; Fang, S.H.; Peng, Y.; Gong, Y.; Wang, D. Remote estimation of biomass in winter oilseed rape Brassicanapus L.) using canopy hyperspectral data at di â†µerent growth stages. Appl. Sci. 2019,9, 545. CrossRef ]18. Rembold, F.; Maselli, F. Estimating inter-annual crop area variation using multi-resolution satellite sensorimages. Int. J. Remote Sens. 2010,25, 2641â€“2647. CrossRef ]19. Pan, Y.Z.; Li, L.; Zhang, J.S.; Liang, S.L.; Hou, D. Crop area estimation based on MODIS-EVI time seriesaccording to distinct characteristics of key phenology phases: case study of winter wheat area estimationin small-scale area. J. Remote Sens. 2011,15, 578â€“594. (In Chinese)20. Xu, Q.; Yang, G.; Long, H.; Wang, C.; Li, X.; Huang, D. Crop information identiï¬cation based on MODISNDVI time-series data. Trans. Chin. Soc. Agric. Eng. 2014,30, 134â€“144. (In Chinese) CrossRef ]21. Zhang, C.; Han, Y.; Li, F.; Gao, S.; Song, D.; Zhao, H.; Fan, K.; Zhang, Y. new CNN-Bayesian modelfor extracting improved winter wheat spatial distribution from GF-2 imagery. Remote Sens. 2019,11, 619.[CrossRef ]22. Zhu, C.M.; Luo, J.C.; Shen, Z.F.; Chen, X. Winter wheat planting area extraction using multi-temporal remotesensing data based on ï¬led parcel characteristic. Trans. CSAE 2011,27, 94â€“99. (In Chinese)23. Becker-Reshef, I.; Vermote, E.; Lindeman, M.; Justice, C. generalized regression-based model for forecastingwinter wheat yields in Kansas and Ukraine using MODIS data. Remote Sens. Environ. 2010,114, 1312â€“1323.[CrossRef ]24. Jha, A.; Nain, A.S.; Ranjan, R. Wheat acreage estimation using remote sensing in tarai region of Uttarakhand.Vegetos 2013,26, 105â€“111. CrossRef ]25. Zhong, Y.; Lin, X.; Zhang, L. support vector conditional random ï¬elds classiï¬er with Mahalanobisdistance boundary constraint for high spatial resolution remote sensing imagery. IEEE J-STARS 2014,7,1314â€“1330. CrossRef ]26. Fu, T.; Ma, L.; Li, M.; Johnson, B.A. Using Convolutional Neural Network to identify irregular segmentationobjects from very high-resolution remote sensing imagery. J. Appl. Remote Sens. 2018,12, 025010. CrossRef ]27. Yang, P.; Yang, G. Feature extraction using dual-tree complex wavelet transform and gray level co-occurrencematrix. Neurocomputing 2016,197, 212â€“220. CrossRef ]Remote Sens. 2020,12, 821 23 of 2528. Bruce, L.M.; Mathur, A.; Byrd, J.D., Jr. Denoising and Wavelet-Based Feature Extraction of MODISMulti-Temporal Vegetation Signatures. Gisci. Remote Sens. 2006,43, 170â€“180. CrossRef ]29. Li, D.; Yang, F.; Wang, X. Crop region extraction of remote sensing images based on fuzzy ARTMAP andadaptive boost. J. Intell. Fuzzy Syst. 2015,29, 2787â€“2794. CrossRef ]30. Jain, A.K.; Ratha, N.K.; Lakshmanan, S. Object detection using gabor ï¬lters. Pattern Recogn. 1997,30, 295â€“309.[CrossRef ]31. Moya, L.; Zakeri, H.; Yamazaki, F.; Liu, W.; Mas, E.; Koshimura, S. 3D gray level co-occurrence matrixand its application to identifying collapsed buildings. ISPRS J. Photogramm. Remote Sens. 2019,149, 14â€“28.[CrossRef ]32. Li, D.; Yang, F.; Wang, X. Study on Ensemble Crop Information Extraction of Remote Sensing Images Basedon SVM and BPNN. J. Indian Soc. Remote Sens. 2016,45, 229â€“237. CrossRef ]33. Yuan, H.; Van Der Wiele, C.; Khorram, S. An automated artiï¬cial neural network system for land use /landcover classiï¬cation from Landsat TM imagery. Remote Sens. 2009,1, 243â€“265. CrossRef ]34. Atkinson, P.M.; Tatnall, A.R.L. Introduction Neural networks in remote sensing. Int. J. Remote Sens. 2010,18,699â€“709. CrossRef ]35. Zhang, F.; Ni, J.; Yin, Q.; Li, W.; Li, Z.; Liu, Y.F.; Hong, W. Nearest-regularized subspace classiï¬cation forPolSAR imagery using polarimetric feature vector and spatial information. Remote Sens. 2017,9, 1114.[CrossRef ]36. LÃ¶w, F.; Michel, U.; Dech, S.; Conrad, C. Impact of feature selection on the accuracy and spatial uncertaintyof per-ï¬eld crop classiï¬cation using Support Vector Machines. ISPRS J. Photogramm. Remote Sens. 2013,85,102â€“119. CrossRef ]37. Santos Pereira, L.F.; Barbon, S.; Valous, N.A.; Barbin, D.F. Predicting the ripening of papaya fruit with digitalimaging and random forests. Comput. Electron. Agric. 2018,145, 76â€“82. CrossRef ]38. Liu, D.; Li, J. Data Field Modeling and Spectral-Spatial Feature Fusion for Hyperspectral Data Classiï¬cation.Sensors 2016,16, 2146. CrossRef ]39. Pelletier, C.; Valero, S.; Inglada, J.; Champion, N.; Dedieu, G. Assessing the robustness of Random Forests tomap land cover with high resolution satellite image time series over large areas. Remote Sens. Environ. 2016,187, 156â€“168. CrossRef ]40. Zhang, K.W.; Hu, B.X. Individual urban tree species classiï¬cation using very high spatial resolution airbornemulti-spectral imagery using longitudinal proï¬les. Remote Sens. 2012,4, 1741â€“1757. CrossRef ]41. Sang, X.; Guo, Q.Z.; Wu, X.X.; Fu, Y.; Xie, T.Y.; He, C.W.; Zang, J.L. Intensity and stationarity analysis of landuse change based on CART algorithm. Nat. Sci. Rep. 2019,9, 12279. CrossRef ][PubMed ]42. Hasan, M.M.; Chopin, J.P.; Laga, H.; Miklavcic, S.J. Detection and analysis of wheat spikes using ConvolutionalNeural Networks. Plant Methods 2018,14, 100. CrossRef ][PubMed ]43. Zhang, M.; Lin, H.; Wang, G.; Sun, H.; Fu, J. Mapping Paddy Rice Using Convolutional Neural Network(CNN) with Landsat Datasets in the Dongting Lake Area, China. Remote Sens. 2018,10, 1840. CrossRef ]44. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv2014, arXiv:1409.1556.45. Ding, P.; Zhang, Y.; Deng, W.; Jia, P.; Kuijper, A. light and faster regional convolutional neural network forobject detection in optical remote sensing images. J. Photogramm. Remote Sens. 2018,141, 208â€“218. CrossRef ]46. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. ImageNet classiï¬cation with deep convolutional neural networks.Commun. ACM 2017,60, 84â€“90. CrossRef ]47. Ronneberger, O.; Fischer, P.; Brox, T. U-Net: Convolutional networks for biomedical image segmentation.arXiv 2015, arXiv:1505.04597.48. Li, W.; Chen, P.; Wang, B.; Xie, C. Automatic Localization and Count of Agricultural Crop Pests Based on anImproved Deep Learning Pipeline. Sci. Rep. 2019,9, 7024. CrossRef ]49. Long, J.; Shelhamer, E.; Darrell, T.; Berkeley, U.C. Fully Convolutional Networks for Semantic Segmentation.arXiv 2015, arXiv:1411.4038v2.50. Badrinarayanan, V.; Kendall, A.; Cipolla, R. SegNet: deep convolutional encoder-decoder architecture forimage segmentation. arXiv 2015, arXiv:1505.07293. CrossRef ]51. Chen, L.; Papandreou, G.; Kokkinos, I.; Murphy, K.; Yuille, A.L. DeepLab: Semantic image segmentationwith deep convolutional nets, Atrous convolution, and fully connected CRFs. IEEE Trans. Pattern Anal.Mach. Intell. 2018,40, 834â€“848. CrossRef ][PubMed ]Remote Sens. 2020,12, 821 24 of 2552. Lin, G.; Milan, A.; Shen, C.; Reid, I. Reï¬neNet: Multi-path reï¬nement networks for high-resolution semanticsegmentation. arXiv 2016, arXiv:1611.06612v3.53. Liu, S.; Ding, W.; Liu, C.; Liu, Y.; Wang, Y.; Li, H. ERN: Edge loss reinforced semantic segmentation networkfor remote sensing images. Remote Sens. 2018,10, 1339. CrossRef ]54. Zhao, W.; Fu, Y.; Wei, X.; Wang, H. An improved image semantic segmentation method based on superpixelsand conditional random ï¬elds. Appl. Sci. 2018,8, 837. CrossRef ]55. Zheng, S.; Jayasumana, S.; Romera-Paredes, B.; Vineet, V.; Su, Z.; Du, D.; Huang, C.; Torr, P.H.S. Conditionalrandom ï¬elds as recurrent neural networks. arXiv 2016, arXiv:1502.03240v3.56. Li, Y.; Zhang, H.; Shen, Q. Spectralâ€“spatial classiï¬cation of hyperspectral imagery with 3D convolutionalneural network. Remote Sens. 2017,9, 67. CrossRef ]57. Sellami, A.; Farah, M.; Farah, I.R.; Solaiman, B. Hyperspectral imagery classiï¬cation based on semi-supervised3-D deep neural network and adaptive band selection. Expert Syst. Appl. 2019,129, 246â€“259. CrossRef ]58. Zhang, P.; Li, M.; Wu, Y.; An, L.; Jia, L. Unsupervised SAR image segmentation using high-order conditionalrandom ï¬elds model based on product-of-experts. Pattern Recog. Lett. 2016,78, 48â€“55. CrossRef ]59. Zhou, L.; Fu, K.; Liu, Z.; Zhang, F.; Yin, Z.; Zheng, J. Superpixel based continuous conditional random ï¬eldneural network for semantic segmentation. Neurocomputing 2019,340, 196â€“210. CrossRef ]60. Liu, Y.; Piramanayagam, S.; Monteiro, S.T.; Saber, E. Semantic segmentation of multisensor remote sensingimagery with deep ConvNets and higher-order conditional random ï¬elds. J. Appl. Remote Sens. 2019,13.[CrossRef ]61. Panboonyuen, T.; Jitkajornwanich, K.; Lawawirojwong, S.; Srestasathiern, P.; Vateekul, P. Road Segmentationof Remotely-Sensed Images Using Deep Convolutional Neural Networks with Landscape Metrics andConditional Random Fields. Remote Sens. 2017,9, 680. CrossRef ]62. Zhang, G.; Jia, X. Simpliï¬ed conditional random ï¬elds with class boundary constraint for spectral-spatialbased remote sensing image classiï¬cation. IEEE Geosci. Remote Sens. Lett. 2012,9, 856â€“860. CrossRef ]63. Wei, L.; Yu, M.; Liang, Y.; Yuan, Z.; Huang, C.; Li, R.; Yu, Y. Precise Crop Classiï¬cation UsingSpectral-Spatial-Location Fusion Based on Conditional Random Fields for UAV-Borne HyperspectralRemote Sensing Imagery. Remote Sens. 2019,11, 2011. CrossRef ]64. Teichmann, M.T.T.; Cipolla, R. Convolutional CRFs for semantic segmentation. arXiv 2018,arXiv:1805.04777v2.65. Wei, L.; Yu, M.; Zhong, Y.; Zhao, J.; Liang, Y.; Hu, X. Spatialâ€“Spectral Fusion Based on Conditional RandomFields for the Fine Classiï¬cation of Crops in UAV-Borne Hyperspectral Remote Sensing Imagery. RemoteSens. 2019,11, 780. CrossRef ]66. Zhong, P.; Wang, R. Learning conditional random ï¬elds for classiï¬cation of hyperspectral images. IEEETrans. Image Process. 2010,19, 1890â€“1907. CrossRef ]67. He, C.; Fang, P.; Zhang, Z.; Xiong, D.; Liao, M. An End-to-End Conditional Random Fields and Skip-ConnectedGenerative Adversarial Segmentation Network for Remote Sensing Images. Remote Sens. 2019,11, 1604.[CrossRef ]68. KnÃ¶belreiter, P.; Reinbacher, C.; Shekhovtsov, A.; Pock, T. End-to-End Training of Hybrid CNN-CRF Modelsfor Stereo. arXiv 2017, arXiv:1611.10229v2.69. Wang, M.; Cheng, J.C.P. uniï¬ed convolutional neural network integrated with conditional random ï¬eldfor pipe defect segmentation. Comput. Aided Civ. Inf. 2020,35, 162â€“177. CrossRef ]70. Zhao, J.; Zhong, Y.; Zhang, L. Detail-Preserving Smoothing Classiï¬er Based on Conditional Random Fieldsfor High Spatial Resolution Remote Sensing Imagery. IEEE Trans. Geosci. Remote Sens. 2015,53, 2440â€“2452.[CrossRef ]71. Chu, L.; Liu, Q.-S.; Huang, C.; Liu, G.-H. Monitoring of winter wheat distribution and phenological phasesbased on MODIS time-series: case study in the Yellow River Delta, China. J. Integr. Agric. 2016,15,2403â€“2416. CrossRef ]72. Zhang, X.-W.; Liu, J.-F.; Qin, Z.; Qin, F. Winter wheat identiï¬cation by integrating spectral and temporalinformation derived from multi-resolution remote sensing data. J. Integr. Agric. 2019,18, 2628â€“2643.[CrossRef ]73. Hao, Z.; Zhao, H.; Zhang, C.; Wang, H.; Jiang, Y.; Yi, Z. Estimating winter wheat area based on an SVM andthe variable fuzzy set method. Remote Sens. Lett. 2019,10, 343â€“352. CrossRef ]Remote Sens. 2020,12, 821 25 of 2574. He, T.; Xie, C.; Liu, Q.; Guan, S.; Liu, G. Evaluation and Comparison of Random Forest and A-LSTMNetworks for Large-scale Winter Wheat Identiï¬cation. Remote Sens. 2019,11, 1665. CrossRef ]75. He, Y.; Wang, C.; Chen, F.; Jia, H.; Liang, D.; Yang, A. Feature Comparison and Optimization for 30-M WinterWheat Mapping Based on Landsat-8 and Sentinel-2 Data Using Random Forest Algorithm. Remote Sens.2019,11, 535. CrossRef ]76. Aneece, I.; Thenkabail, P. Accuracies Achieved in Classifying Five Leading World Crop Types and theirGrowth Stages Using Optimal Earth Observing-1 Hyperion Hyperspectral Narrowbands on Google EarthEngine. Remote Sens. 2018,10, 2027. CrossRef ]77. Teimouri, N.; Dyrmann, M.; Jorgensen, R.N. Novel Spatio-Temporal FCN-LSTM Network for RecognizingVarious Crop Types Using Multi-Temporal Radar Images. Remote Sens. 2019,11, 990. CrossRef ]78. Chen, Y.; Huang, L.; Zhu, L.; Yokoya, N.; Jia, X. Fine-Grained Classiï¬cation of Hyperspectral Imagery Basedon Deep Learning. Remote Sens. 2019,11, 2690. CrossRef ]79. Ruder, S. An overview of gradient descent optimization algorithms. arXiv 2016, arXiv:1609.04747v2.80. Wang, H.; Wang, Y.; Zhang, Q.; Xiang, S.; Pan, C. Gated convolutional neural network for semanticsegmentation in high-resolution images. Remote Sens. 2017,9, 446. CrossRef ]81. Li, F.; Zhang, C.; Zhang, W.; Xu, Z.; Wang, S.; Sun, G.; Wang, Z. Improved Winter Wheat Spatial DistributionExtraction from High-Resolution Remote Sensing Imagery Using Semantic Features and Statistical Analysis.Remote Sens. 2020,12, 538. CrossRef ]Â©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open accessarticle distributed under the terms and conditions of the Creative Commons Attribution(CC BY) license http: //creativecommons.org /licenses /by/4.0/).